{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from countries_languages import country_to_language\n",
    "import pandas as pd\n",
    "from difflib import get_close_matches\n",
    "import numpy as np\n",
    "import re\n",
    "from transliterate import translit\n",
    "from unidecode import unidecode\n",
    "import Levenshtein\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "import wikipediaapi\n",
    "\n",
    "import wikipedia\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import difflib\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import calendar\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import AssemblyHelpers\n",
    "#from AssemblyHelpers import find_money_info_from_name\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def load_csv_dataset(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "countries_codes = load_csv_dataset('/Users/finneganlaister-smith/Downloads/DEV ENVIRONMENT/data-science-jupyter-template-main/footy_project_market_value_rankings/create_dataset_1/countries_and_codes.csv')\n",
    "\n",
    "#leagues value\n",
    "leagues_value = load_csv_dataset('/Users/finneganlaister-smith/Downloads/DEV ENVIRONMENT/data-science-jupyter-template-main/footy_project_market_value_rankings/create_dataset_1/Most Updated Edited Transfermarkt Dataset.csv')\n",
    "#leagues value large \n",
    "leagues_value_large = load_csv_dataset('/Users/finneganlaister-smith/Downloads/DEV ENVIRONMENT/data-science-jupyter-template-main/footy_project_market_value_rankings/create_dataset_1/most_updated_transfermarkt_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging group 1 CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_multi_names = load_csv_dataset('CSVs of edge cases - for F/DONE/from group 1 - ad hoc fixes complete/working_multiple_names_combined.csv')\n",
    "working_multi_names = working_multi_names.drop(columns={'Unnamed: 0'})\n",
    "working_multi_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_date_multi_guys_matches = load_csv_dataset('CSVs of edge cases - for F/DONE/from group 1 - ad hoc fixes complete/match_date_multi_guys_MATCHES.csv')\n",
    "cyrillic_group_matches = load_csv_dataset('CSVs of edge cases - for F/DONE/from group 1 - ad hoc fixes complete/cyrillic_guys_multi_found_matches.csv')\n",
    "online_group_Matches = load_csv_dataset('CSVs of edge cases - for F/DONE/from group 1 - ad hoc fixes complete/Online_lookup_required_multi_names_guys.csv')\n",
    "used_lg_dataset_group = load_csv_dataset('CSVs of edge cases - for F/DONE/from group 1 - ad hoc fixes complete/Used_Large_Dataset_Multiple_Name_Dudes.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([working_multi_names, match_date_multi_guys_matches, cyrillic_group_matches, online_group_Matches, used_lg_dataset_group])\n",
    "merged_df = merged_df.reset_index()\n",
    "merged_df = merged_df.drop(columns={'index', 'Unnamed: 0'})\n",
    "\n",
    "merged_df.at[41, 'Names_Found'] = '[\\'Sohel Rana\\', \\'Mohamed Sohel Rana\\']'\n",
    "merged_df.at[60, 'Names_Found'] = '[\\'Sohel Rana\\', \\'Mohamed Sohel Rana\\']'\n",
    "merged_df.at[83, 'Names_Found'] = '[\\'Weston Mckennie\\']'\n",
    "merged_df.at[574, 'Names_Found'] = '[\\'Sohel Rana\\', \\'Mohamed Sohel Rana\\']'\n",
    "merged_df.at[583, 'Names_Found'] = '[\\'Sohel Rana\\', \\'Mohamed Sohel Rana\\']'\n",
    "merged_df.at[610, 'Names_Found'] = 'Alfred Finnbogason'\n",
    "merged_df.at[1007, 'Names_Found'] = 'Brian Rodríguez'\n",
    "merged_df.at[1012, 'Names_Found'] = 'Juan Fernando Quintero'\n",
    "merged_df.at[1024, 'Names_Found'] = 'Brian Rodríguez'\n",
    "merged_df.at[3289, 'Names_Found'] = '[\\'Sohel Rana\\', \\'Mohamed Sohel Rana\\']'\n",
    "merged_df.at[3294, 'Names_Found'] = '[\\'Sohel Rana\\', \\'Mohamed Sohel Rana\\']'\n",
    "merged_df.at[5491, 'Nationality'] = 'Cuba'\n",
    "merged_df.at[5491, 'Nationality'] = 'CU'\n",
    "merged_df.at[5491, 'Names_Found'] = 'Marcel Hernández'\n",
    "merged_df.at[5782, 'Names_Found'] = 'Nigel Charles'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle cases where mandem both / all played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_changes = 0\n",
    "for i in range(0, 5957):\n",
    "    num_identical_rows = 0\n",
    "    num_names_this = 0\n",
    "    index = i\n",
    "    row = merged_df.loc[index]\n",
    "    # Extract relevant columns\n",
    "    match = row['Match']\n",
    "    date = row['Date']\n",
    "    names_found_string = row['Names_Found']\n",
    "    if '‘' in names_found_string:\n",
    "        names_found_string = names_found_string.replace('‘', \"\\'\")\n",
    "        names_found_string = names_found_string.replace('’', \"\\'\")\n",
    "        #print(names_found_string)\n",
    "    original_jersey = row['ORIGINAL JERSEY']\n",
    "    nationality = row['Nationality']\n",
    "    if type(names_found_string) == str:\n",
    "        if names_found_string.startswith('['):\n",
    "            names_found = eval(names_found_string)\n",
    "        else:  \n",
    "            names_found = list(names_found_string)\n",
    "\n",
    "    rows_matching_this_one = merged_df[(merged_df['Match'] == match) & \n",
    "                                       (merged_df['Date'] == date) &\n",
    "                                       (merged_df['ORIGINAL JERSEY'] == original_jersey) &\n",
    "                                       (merged_df['Nationality'] == nationality) &\n",
    "                                       (merged_df['Names_Found'] == row['Names_Found'])]\n",
    "    num_identical_rows = len(rows_matching_this_one)\n",
    "    num_names_this = len(names_found)\n",
    "\n",
    "    if num_names_this != 1:\n",
    "        if num_identical_rows == num_names_this:\n",
    "            matching_indexes = list(rows_matching_this_one.index)\n",
    "            num = 0\n",
    "            number_of_changes += 1\n",
    "            for matching_index in matching_indexes:\n",
    "                print(f'index is {matching_index}, num is {num}')\n",
    "                merged_df.at[matching_index, 'Names_Found'] = names_found[num]\n",
    "                print(f'added {names_found[num]} to row {matching_index}')\n",
    "                num += 1\n",
    "                #remove the name from names_found after adding it \n",
    "            #get all the indexes of the rows matching this one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle cases where the cell has the same name more than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_all_same(names_found):\n",
    "    if not names_found:\n",
    "        return False  # If the list is empty, technically all elements are the same, but I assume this is not what you want\n",
    "    first_item = names_found[0]\n",
    "    return all(item == first_item for item in names_found)\n",
    "\n",
    "for i in range(0, 5957):\n",
    "    num_identical_rows = 0\n",
    "    num_names_this = 0\n",
    "    index = i\n",
    "    row = merged_df.loc[index]\n",
    "    # Extract relevant columns\n",
    "    match = row['Match']\n",
    "    date = row['Date']\n",
    "    names_found_string = row['Names_Found']\n",
    "    if '‘' in names_found_string:\n",
    "        names_found_string = names_found_string.replace('‘', \"\\'\")\n",
    "        names_found_string = names_found_string.replace('’', \"\\'\")\n",
    "\n",
    "    original_jersey = row['ORIGINAL JERSEY']\n",
    "    nationality = row['Nationality']\n",
    "    if type(names_found_string) == str:\n",
    "        if names_found_string.startswith('['):\n",
    "            names_found = eval(names_found_string)\n",
    "        else:  \n",
    "            names_found = list(names_found_string)\n",
    "    if len(names_found) >= 2:\n",
    "        if are_all_same(names_found):\n",
    "            print(\"All items in the list are the same.\", names_found[0])\n",
    "            merged_df.at[i, 'Names_Found'] = names_found[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('Group1_fixes_complete.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match Date Filtering, group 1 (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1_fixes_complete_csv = load_csv_dataset('Group 2 (for T) - Find their $ Info/Group1_fixes_complete.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_using_date_of_match(candidate_list_of_names_input, nationality_input, matchdate_Input, SALARY_BOOLEAN): \n",
    "\n",
    "    FILTERED_NAMES_USING_MATCH_DATE = AssemblyHelpers.multiNameMatchDateLookup(candidate_list_of_names_input, nationality_input, matchdate_Input) \n",
    "\n",
    "    #print('in fudom', type(FILTERED_NAMES_USING_MATCH_DATE))\n",
    "    #1 MATCH\n",
    "    if isinstance(FILTERED_NAMES_USING_MATCH_DATE, str) or isinstance(FILTERED_NAMES_USING_MATCH_DATE, np.str_): #YELLOW 3\n",
    "        #print('were here', FILTERED_NAMES_USING_MATCH_DATE)\n",
    "        #print('were in fudom in the right place')\n",
    "        #NAME_FOR_SELENIUM_SEARCH = FILTERED_NAMES_USING_MATCH_DATE\n",
    "        # MONEY_FOUND_ONLINE = useSeleniumToFindMoney(NAME_FOR_SELENIUM_SEARCH, nationality_input, matchdate_Input, SALARY_BOOLEAN)\n",
    "        # print('made it out of using selenium')\n",
    "        # money_thisplayer = [MONEY_FOUND_ONLINE, f'Online WAS Lookup Required - salary boolean is {SALARY_BOOLEAN}', True, False]\n",
    "        money_thisplayer = [0, 'Lookup Required', True, False]\n",
    "        return money_thisplayer, FILTERED_NAMES_USING_MATCH_DATE\n",
    "    #MULTIPLE MATCHES\n",
    "    elif(len(FILTERED_NAMES_USING_MATCH_DATE) >= 2):\n",
    "        print(FILTERED_NAMES_USING_MATCH_DATE, type(FILTERED_NAMES_USING_MATCH_DATE), type(FILTERED_NAMES_USING_MATCH_DATE) == str)\n",
    "        money_thisplayer = [0, F'FILTERING USING MATCH DATE LEFT MULTIPLE RESULTS: {FILTERED_NAMES_USING_MATCH_DATE}', True, False]\n",
    "        return money_thisplayer, FILTERED_NAMES_USING_MATCH_DATE\n",
    "    #0 MATCHES\n",
    "    elif(len(FILTERED_NAMES_USING_MATCH_DATE) == 0):\n",
    "        money_thisplayer = [0, F'FILTERING USING MATCH DATE LEFT 0 RESULTS. BEFORE FILTERING, CANDIDATE NAMES WERE {candidate_list_of_names_input}', True, False]\n",
    "        return money_thisplayer, ''\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = 0\n",
    "for i in range(5750, 5957):\n",
    "    names_found = []\n",
    "    num_identical_rows = 0\n",
    "    num_names_this = 0\n",
    "    index = i\n",
    "    row = group_1_fixes_complete_csv.loc[index]\n",
    "    # Extract relevant columns\n",
    "    match = row['Match']\n",
    "    date = row['Date']\n",
    "    names_found_string = row['Names_Found']\n",
    "    if '‘' in names_found_string:\n",
    "        names_found_string = names_found_string.replace('‘', \"\\'\")\n",
    "        names_found_string = names_found_string.replace('’', \"\\'\")\n",
    "\n",
    "    original_jersey = row['ORIGINAL JERSEY']\n",
    "    nationality = row['Nationality']\n",
    "    if type(names_found_string) == str:\n",
    "        if names_found_string.startswith('['):\n",
    "            names_found = eval(names_found_string)\n",
    "        else: \n",
    "            0==0 \n",
    "            #names_found = list(names_found_string)\n",
    "    if len(names_found) >= 2:\n",
    "        print(\"Multiple names remaining\", i, names_found, match, date, nationality)\n",
    "        filtered_names_match_date = filter_using_date_of_match(names_found, nationality, date, False)[1]\n",
    "        \n",
    "        if type(filtered_names_match_date) == str:\n",
    "            print(f'{i} -- successfully filtered to just {filtered_names_match_date}')\n",
    "            #correct_names = filtered_names_match_date\n",
    "            status = 'Success'\n",
    "            group_1_fixes_complete_csv.at[i, 'Names_Found'] = filtered_names_match_date\n",
    "        else:\n",
    "            #multi_list = filtered_names_match_date\n",
    "            print(f'{i} -- after filtering the remaining list is {filtered_names_match_date}')\n",
    "            group_1_fixes_complete_csv.at[i, 'Status'] = 'MULTIPLE'\n",
    "            group_1_fixes_complete_csv.at[i, 'Names_Found'] = filtered_names_match_date\n",
    "\n",
    "        #try doing a match date filter first of all. \n",
    "\n",
    "        #FOR STATUS == MULTIPLE\n",
    "            #check if there is another row with the same Nationality, Match, Date in the working dataset.\n",
    "                #and that one of the names in these rows is in the Names_Found\n",
    "\n",
    "                #for all such rows you find , remove that from the list of names in this row\n",
    "\n",
    "            #when ur done checking that count how many names remain\n",
    "                #if 1, leave it. \n",
    "                #if 2+, ? \n",
    "    else:\n",
    "        0==0 #add the bread info for this guy.\n",
    "#print(instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again handle stuff where mandem both played (in this dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_changes = 0\n",
    "for i in range(0, 5957):\n",
    "    num_identical_rows = 0\n",
    "    num_names_this = 0\n",
    "    index = i\n",
    "    row = group_1_fixes_complete_csv.loc[index]\n",
    "    # Extract relevant columns\n",
    "    match = row['Match']\n",
    "    date = row['Date']\n",
    "    names_found_string = row['Names_Found']\n",
    "    if '‘' in names_found_string:\n",
    "        names_found_string = names_found_string.replace('‘', \"\\'\")\n",
    "        names_found_string = names_found_string.replace('’', \"\\'\")\n",
    "        #print(names_found_string)\n",
    "    original_jersey = row['ORIGINAL JERSEY']\n",
    "    nationality = row['Nationality']\n",
    "    if type(names_found_string) == str:\n",
    "        if names_found_string.startswith('['):\n",
    "            names_found = eval(names_found_string)\n",
    "        else:  \n",
    "            names_found = list(names_found_string)\n",
    "    \n",
    "\n",
    "    if type(row['Names_Found']) == list:\n",
    "        rows_matching_this_one = group_1_fixes_complete_csv[\n",
    "            (group_1_fixes_complete_csv['Match'] == match) & \n",
    "            (group_1_fixes_complete_csv['Date'] == date) &\n",
    "            (group_1_fixes_complete_csv['Nationality'] == nationality) &\n",
    "            (group_1_fixes_complete_csv['Names_Found'].apply(lambda x: isinstance(x, list) and any(name in x for name in row['Names_Found'])))\n",
    "        ]\n",
    "    else:\n",
    "        rows_matching_this_one = group_1_fixes_complete_csv[(group_1_fixes_complete_csv['Match'] == match) & \n",
    "                                        (group_1_fixes_complete_csv['Date'] == date) &\n",
    "                                        (group_1_fixes_complete_csv['Nationality'] == nationality) &\n",
    "                                        (group_1_fixes_complete_csv['Names_Found'] == row['Names_Found'])]\n",
    "    num_identical_rows = len(rows_matching_this_one)\n",
    "    num_names_this = len(names_found)\n",
    "\n",
    "    if num_names_this != 1:\n",
    "        if num_identical_rows == num_names_this:\n",
    "            matching_indexes = list(rows_matching_this_one.index)\n",
    "            num = 0\n",
    "            number_of_changes += 1\n",
    "            for matching_index in matching_indexes:\n",
    "                print(f'index is {matching_index}, num is {num}')\n",
    "                group_1_fixes_complete_csv.at[matching_index, 'Names_Found'] = names_found[num]\n",
    "                print(f'added {names_found[num]} to row {matching_index}')\n",
    "                num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(group_1_fixes_complete_csv)):\n",
    "    if type(group_1_fixes_complete_csv.at[i, 'Names_Found']) != list:\n",
    "        if group_1_fixes_complete_csv.at[i, 'Status'] == 'MULTIPLE':\n",
    "            group_1_fixes_complete_csv.at[i, 'Status'] = 'Success'\n",
    "\n",
    "for i in range(0, len(group_1_fixes_complete_csv)):\n",
    "    if group_1_fixes_complete_csv.at[i, 'Status'] == 'Multi':\n",
    "        if type(group_1_fixes_complete_csv.at[i, 'Names_Found']) == str:\n",
    "            group_1_fixes_complete_csv.at[i, 'Status'] = 'Success'\n",
    "        else:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1_fixes_complete_csv.to_csv('Group1_fixes_complete_ii.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking rows w multiple names against the big dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1_fixes_complete_csv_ii = load_csv_dataset('Group1_fixes_complete_ii.csv')\n",
    "group_1_fixes_complete_csv_ii.at[5848, 'Names_Found'] = 'Ahmed Hamed Mahmoud'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading datasets to look for stuff in \n",
    "\n",
    "#go through these datasets where the matching worked CORRECTLY & SEE IF THERES ONE OF THE NAMES IN THE MULTI SET.\n",
    "\n",
    "#the big dataset to add stuff to \n",
    "combined_df_large = load_csv_dataset('Combined_DF_WORKING - ADD TO THIS.csv')\n",
    "#terrells new df\n",
    "terrell_new_df = load_csv_dataset('CSVs of edge cases - for T/Terrell Lookup Cases - new_df.csv')\n",
    "#Cyrillic Found Guys\n",
    "Cyrillic_Found_Guys = load_csv_dataset('Group 2 (for T) - Find their $ info/Cyrillic_Found_Guys.csv')\n",
    "#lookup_required_matches\n",
    "lookup_required_matches = load_csv_dataset('Group 2 (for T) - Find their $ info/lookup_required_matches.csv')\n",
    "#Online lookup required - 1 name found\n",
    "online_lookup_required_matches = load_csv_dataset('Group 2 (for T) - Find their $ info/Online lookup required - 1 name found.csv')\n",
    "\n",
    "\n",
    "for i in range(0, len(lookup_required_matches)):\n",
    "    if type(lookup_required_matches.at[i, 'Name Match']) == str:\n",
    "        if (lookup_required_matches.at[i, 'Name Match'].startswith('[')):\n",
    "            0==0\n",
    "        else:\n",
    "            #print(lookup_required_matches.at[i, 'Name Match'])\n",
    "            lookup_required_matches.at[i, 'Name Match'] = f\"[\\'{lookup_required_matches.at[i, 'Name Match']}\\']\"\n",
    "\n",
    "for i in range(0, len(lookup_required_matches)):\n",
    "    if lookup_required_matches.at[i, \"Name Match\"] == '[\\'Fabrice N\\'Sakala\\']':\n",
    "        lookup_required_matches.at[i, \"Name Match\"] ='[\\'Fabrice NSakala\\']'\n",
    "    if lookup_required_matches.at[i, \"Name Match\"] == '[\\'Jordan N\\'Kololo\\']':\n",
    "        lookup_required_matches.at[i, \"Name Match\"] ='[\\'Jordan NKololo\\']'\n",
    "\n",
    "#check if there's an overlapping set of multiple dudes in other datasets as well \n",
    "    #Terrells new DF (already loaded)\n",
    "    #big dataset (already loaded)\n",
    "    #cyrillic found guys (already loaded)\n",
    "    #online lookup - 1 name found (already loaded)\n",
    "    #lookup required matches (already loaded)\n",
    "\n",
    "    #other group\n",
    "    #match date zero guys - Name(s) Found\n",
    "match_date_zero_guys = load_csv_dataset('CSVs of edge cases - for F/\"other\" group/match_date_zero_guys.csv')\n",
    "            #if the same list or if ur list is IN their list \n",
    "    #large dataset said 0\n",
    "large_dataset_said_0 = load_csv_dataset('CSVs of edge cases - for F/\"other\" group/large_dataset_SAID_0.csv')\n",
    "    #terrell's wrong DF \n",
    "terrell_wrong_df = load_csv_dataset('CSVs of edge cases - for F/\"other\" group/Terrell Lookup Cases - wrong_df.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv files for third round of looking shit up\n",
    "odd_name_0_cases = load_csv_dataset('CSVs of edge cases - for F/Group 3 - Nobody Found/odd_name_found_0_cases.csv')\n",
    "cyrillic_guys_0_found = load_csv_dataset('CSVs of edge cases - for F/Group 3 - Nobody Found/Cyrillic_Guys_Nobody_Found.csv')\n",
    "online_lookup_guys_0_found = load_csv_dataset('CSVs of edge cases - for F/Group 3 - Nobody Found/Online lookup required - 0 names found.csv')\n",
    "\n",
    "\n",
    "lookup_dudes = load_csv_dataset('CSVs of edge cases - for F/\"other\" group/lookup_required_dudes.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***81***\n",
      "***99***\n",
      "99 found a candidate rrow in weird lookup dudes\n",
      "***395***\n",
      "395 found a row of this in odd candidates\n",
      "***526***\n",
      "***530***\n",
      "***535***\n",
      "***540***\n",
      "***586***\n",
      "586 found a candidate rrow in weird lookup dudes\n",
      "***625***\n",
      "***769***\n",
      "***804***\n",
      "986 xxxxx - found a matching set of names in the match date filter left 0 dataset ['Alexander López', 'Luis López'] [\"['Luis López', 'Alexander López', 'Iván López', 'Kevin López', 'Roberto López']\"]\n",
      "***986***\n",
      "***1022***\n",
      "***1110***\n",
      "***1150***\n",
      "***1181***\n",
      "***1255***\n",
      "***1320***\n",
      "***1573***\n",
      "1605 xxxxx - found a matching set of names in the match date filter left 0 dataset ['Luis López', 'Alexander López'] [\"['Luis López', 'Alexander López', 'Iván López', 'Kevin López', 'Roberto López']\"]\n",
      "***1605***\n",
      "1620 xxxxx - found a matching set of names in the match date filter left 0 dataset ['Luis López', 'Alexander López'] [\"['Luis López', 'Alexander López', 'Iván López', 'Kevin López', 'Roberto López']\"]\n",
      "***1620***\n",
      "***1713***\n",
      "***1765***\n",
      "***1771***\n",
      "***1956***\n",
      "1956 found a row of this in cyrillic candidates\n",
      "***1961***\n",
      "***2247***\n",
      "***2248***\n",
      "***2358***\n",
      "***2465***\n",
      "2465 found a candidate rrow in weird lookup dudes\n",
      "***2484***\n",
      "2484 found a candidate rrow in weird lookup dudes\n",
      "***2613***\n",
      "***2690***\n",
      "***2906***\n",
      "2906 found a candidate rrow in weird lookup dudes\n",
      "***2910***\n",
      "2910 found a candidate rrow in weird lookup dudes\n",
      "***2912***\n",
      "2912 found a candidate rrow in weird lookup dudes\n",
      "***2935***\n",
      "2935 found a candidate rrow in weird lookup dudes\n",
      "***2937***\n",
      "***3026***\n",
      "***3030***\n",
      "***3126***\n",
      "***3131***\n",
      "***3134***\n",
      "***3136***\n",
      "***3137***\n",
      "***3139***\n",
      "***3243***\n",
      "***3413***\n",
      "3413 found a candidate rrow in weird lookup dudes\n",
      "***3416***\n",
      "3416 found a candidate rrow in weird lookup dudes\n",
      "***3451***\n",
      "***3697***\n",
      "***3707***\n",
      "***3809***\n",
      "***3810***\n",
      "***3829***\n",
      "***3874***\n",
      "3874 found a candidate rrow in weird lookup dudes\n",
      "***3964***\n",
      "***3967***\n",
      "***3992***\n",
      "***4045***\n",
      "***4047***\n",
      "***4109***\n",
      "***4225***\n",
      "***4229***\n",
      "***4232***\n",
      "***4247***\n",
      "4247 found a candidate rrow in weird lookup dudes\n",
      "4259 xxxxx - found a matching set of names in Terrell's wrong DF ['Álvaro Pereira', 'Maximiliano Pereira'] [\"['Álvaro Pereira', 'Maximiliano Pereira', 'Guzmán Pereira']\"]\n",
      "***4259***\n",
      "4261 xxxxx - found a matching set of names in Terrell's wrong DF ['Mark González', 'Marcos González'] [\"['Osvaldo González', 'Marcos González', 'Mark González', 'Ricardo González', 'Daniel González']\"]\n",
      "***4261***\n",
      "***4267***\n",
      "4267 found a candidate rrow in weird lookup dudes\n",
      "***4269***\n",
      "***4270***\n",
      "***4274***\n",
      "***4282***\n",
      "***4306***\n",
      "***4332***\n",
      "***4334***\n",
      "***4337***\n",
      "***4347***\n",
      "***4349***\n",
      "***4350***\n",
      "***4351***\n",
      "***4353***\n",
      "***4354***\n",
      "***4367***\n",
      "***4372***\n",
      "***4373***\n",
      "***4375***\n",
      "***4382***\n",
      "***4388***\n",
      "***4389***\n",
      "***4393***\n",
      "***4394***\n",
      "***4406***\n",
      "***4408***\n",
      "***4411***\n",
      "***4412***\n",
      "***4413***\n",
      "***4414***\n",
      "***4415***\n",
      "***4419***\n",
      "***4421***\n",
      "***4455***\n",
      "4455 found a candidate rrow in weird lookup dudes\n",
      "***4461***\n",
      "***4471***\n",
      "***4472***\n",
      "***4475***\n",
      "***4484***\n",
      "***4486***\n",
      "***4491***\n",
      "4491 found a candidate rrow in weird lookup dudes\n",
      "***4492***\n",
      "4492 found a candidate rrow in weird lookup dudes\n",
      "***4501***\n",
      "***4502***\n",
      "***4505***\n",
      "***4508***\n",
      "4528 xxxxx - found a matching set of names in the large dataset which said that 0 ['Detre Bell', 'Justin Bell'] [\"['Detre Bell', 'Ashton Bell', 'Tahj Bell', 'Justin Bell']\"]\n",
      "***4528***\n",
      "4528 found a candidate rrow in weird lookup dudes\n",
      "***4541***\n",
      "***4542***\n",
      "***4543***\n",
      "***4547***\n",
      "***4554***\n",
      "***4560***\n",
      "***4564***\n",
      "***4567***\n",
      "***4569***\n",
      "***4570***\n",
      "4571 xxxxx - found a matching set of names in Terrell's wrong DF ['Robert Rojas', 'Matías Rojas'] [\"['Jorge Rojas', 'Robert Rojas', 'Rodrigo Rojas', 'Santiago Rojas', 'Matías Rojas']\"]\n",
      "***4571***\n",
      "***4581***\n",
      "***4582***\n",
      "***4589***\n",
      "***4596***\n",
      "***4599***\n",
      "***4602***\n",
      "***4616***\n",
      "***4617***\n",
      "***4621***\n",
      "***4626***\n",
      "***4638***\n",
      "***4646***\n",
      "***4649***\n",
      "***4653***\n",
      "***4656***\n",
      "***4662***\n",
      "***4673***\n",
      "***4689***\n",
      "***4690***\n",
      "***4695***\n",
      "***4700***\n",
      "***4703***\n",
      "***4707***\n",
      "***4710***\n",
      "***4711***\n",
      "***4712***\n",
      "***4720***\n",
      "***4722***\n",
      "4731 xxxxx - found a matching set of names in the large dataset which said that 0 ['Glenson Prince', 'Hubert Prince', 'Joslyn Prince'] [\"['Glenson Prince', 'Hubert Prince', 'Joslyn Prince', 'Joel Prince']\"]\n",
      "***4731***\n",
      "***4732***\n",
      "***4733***\n",
      "***4740***\n",
      "***4741***\n",
      "4741 found a candidate rrow in weird lookup dudes\n",
      "***4743***\n",
      "***4744***\n",
      "***4748***\n",
      "***4749***\n",
      "***4761***\n",
      "***4762***\n",
      "4762 found a row of this in cyrillic candidates\n",
      "***4763***\n",
      "***4774***\n",
      "***4807***\n",
      "***4810***\n",
      "***4813***\n",
      "***4816***\n",
      "***4820***\n",
      "***4825***\n",
      "***4830***\n",
      "***4832***\n",
      "***4838***\n",
      "***4853***\n",
      "***4856***\n",
      "***4857***\n",
      "***4873***\n",
      "***4874***\n",
      "***4876***\n",
      "***4880***\n",
      "***4887***\n",
      "***4947***\n",
      "4947 found a row of this in cyrillic candidates\n",
      "***4992***\n",
      "4992 found a row of this in cyrillic candidates\n",
      "***4998***\n",
      "***5015***\n",
      "5015 found a row of this in cyrillic candidates\n",
      "***5122***\n",
      "***5145***\n",
      "***5149***\n",
      "***5172***\n",
      "***5275***\n",
      "***5276***\n",
      "***5321***\n",
      "5321 found a row of this in cyrillic candidates\n",
      "***5450***\n",
      "***5451***\n",
      "***5598***\n",
      "***5682***\n",
      "5682 found a candidate rrow in weird lookup dudes\n",
      "***5732***\n",
      "***5753***\n",
      "5753 found a candidate rrow in weird lookup dudes\n",
      "***5813***\n",
      "***5874***\n",
      "***5878***\n",
      "204 0 196 8 0\n"
     ]
    }
   ],
   "source": [
    "instances = 0\n",
    "still_working_counter = 0\n",
    "success_counter = 0\n",
    "second_part_success_counter = 0\n",
    "single_mandem_counter = 0\n",
    "for i in range(0, 5957):#0, 5957\n",
    "    this_row_removals = 0\n",
    "    case_found = False\n",
    "    names_found = ''\n",
    "    num_identical_rows = 0\n",
    "    num_names_this = 0\n",
    "    index = i\n",
    "    row = group_1_fixes_complete_csv_ii.loc[index]\n",
    "    # Extract relevant columns\n",
    "    match = row['Match']\n",
    "    date = row['Date']\n",
    "    names_found_string = row['Names_Found']\n",
    "    if '‘' in names_found_string:\n",
    "        names_found_string = names_found_string.replace('‘', \"\\'\")\n",
    "        names_found_string = names_found_string.replace('’', \"\\'\")\n",
    "\n",
    "    original_jersey = row['ORIGINAL JERSEY']\n",
    "    nationality = row['Nationality']\n",
    "    if type(names_found_string) == str:\n",
    "        if names_found_string.startswith('['):\n",
    "            names_found = eval(names_found_string)\n",
    "        else: \n",
    "            0==0\n",
    "            #names_found = list(names_found_string)\n",
    "    elif type(names_found_string) == list:\n",
    "        names_found = names_found_string\n",
    "    if len(names_found) >= 2: #1st ROUND CHECKING FOR SINGLES TO REMOVE\n",
    "        #print(i, \"Multiple names remaining\", len(names_found), names_found, match, date, nationality)\n",
    "        instances +=1\n",
    "\n",
    "        #COMBINED DF LARGE\n",
    "        combined_df_large_rows_matching = combined_df_large[(combined_df_large['Match'] == match) & \n",
    "                                                            (combined_df_large['Date'] == date) & \n",
    "                                                            ((combined_df_large['Name(s) Found'] == f\"{names_found}\") | (combined_df_large['ORIGINAL JERSEY'] == original_jersey) | ((combined_df_large['Name(s) Found'].apply(lambda x: isinstance(x, list) and any(name in x for name in row['Names_Found'])))))]\n",
    "        if len(combined_df_large_rows_matching) >= 1:\n",
    "            success_counter += 1\n",
    "            print(i, 'Found', name_single, names_found)\n",
    "            #remove the name you FOUND from names_found\n",
    "            names_found.remove(name_single)\n",
    "            this_row_removals += 1\n",
    "            print(f\"{i} - found using BIG dataset. names found is now {names_found}. removed {name_single}\")\n",
    "            #add the updated names_found to group_1_fixes_complete_csv_ii\n",
    "            group_1_fixes_complete_csv_ii.at[i, 'Names_Found'] = names_found\n",
    "            if len(names_found) == 1:\n",
    "                #change the row in group_1_fixes_complete_csv_ii status to Success\n",
    "                    group_1_fixes_complete_csv_ii.at[i, 'Status'] = 'Success'\n",
    "            #break this loop \n",
    "                #break\n",
    "        for name_single in names_found:\n",
    "            combined_df_large_rows_matching = combined_df_large[(combined_df_large['Match'] == match) & \n",
    "            (combined_df_large['Date'] == date) & \n",
    "            ((combined_df_large['Name(s) Found'] == name_single) | (combined_df_large['ORIGINAL JERSEY'] == original_jersey) | (combined_df_large['Name(s) Found'] == f\"[{name_single}]\"))]\n",
    "\n",
    "            if len(combined_df_large_rows_matching) >= 1:\n",
    "                success_counter += 1\n",
    "                print(i, 'Found', name_single, names_found)\n",
    "                #remove the name you FOUND from names_found\n",
    "                names_found.remove(name_single)\n",
    "                this_row_removals += 1\n",
    "                print(f\"{i} - found using BIG dataset. names found is now {names_found}. removed {name_single}\")\n",
    "                #add the updated names_found to group_1_fixes_complete_csv_ii\n",
    "                group_1_fixes_complete_csv_ii.at[i, 'Names_Found'] = names_found\n",
    "                if len(names_found) == 1:\n",
    "                    #change the row in group_1_fixes_complete_csv_ii status to Success\n",
    "                        group_1_fixes_complete_csv_ii.at[i, 'Status'] = 'Success'\n",
    "                #break this loop \n",
    "                    #break\n",
    "\n",
    "        #NEW DF \n",
    "        new_df_rows_matching = terrell_new_df[(terrell_new_df['Match'] == match) & \n",
    "                                                            (terrell_new_df['Date'] == date) & \n",
    "                                                            ((terrell_new_df['Name(s) Found'] == f\"{names_found}\") | (terrell_new_df['ORIGINAL JERSEY'] == original_jersey) | ((terrell_new_df['Name(s) Found'].apply(lambda x: isinstance(x, list) and any(name in x for name in row['Names_Found'])))))]\n",
    "        if len(new_df_rows_matching) >= 1:\n",
    "            success_counter += 1\n",
    "            print(i, 'Found', name_single, names_found)\n",
    "            #remove the name you FOUND from names_found\n",
    "            names_found.remove(name_single)\n",
    "            this_row_removals += 1\n",
    "            print(f\"{i} - found using Terrell NEW DF. names found is now {names_found}. removed {name_single}\")\n",
    "            #add the updated names_found to group_1_fixes_complete_csv_ii\n",
    "            group_1_fixes_complete_csv_ii.at[i, 'Names_Found'] = names_found\n",
    "            if len(names_found) == 1:\n",
    "                #change the row in group_1_fixes_complete_csv_ii status to Success\n",
    "                    group_1_fixes_complete_csv_ii.at[i, 'Status'] = 'Success'\n",
    "            #break this loop \n",
    "                #break\n",
    "        for name_single in names_found:\n",
    "            new_df_rows_matching = terrell_new_df[(terrell_new_df['Match'] == match) & \n",
    "            (terrell_new_df['Date'] == date) & \n",
    "            ((terrell_new_df['Name(s) Found'] == name_single) | (terrell_new_df['ORIGINAL JERSEY'] == original_jersey) | (terrell_new_df['Name(s) Found'] == f\"[{name_single}]\"))]\n",
    "            if len(new_df_rows_matching) >= 1:\n",
    "                success_counter += 1\n",
    "                print(i, 'Found', name_single, names_found)\n",
    "                #remove the name you FOUND from names_found\n",
    "                names_found.remove(name_single)\n",
    "                this_row_removals += 1\n",
    "                if name_single in names_found:\n",
    "                    names_found.remove(name_single)\n",
    "                print(f\"{i} - found using Terrell NEW DF dataset. names found is now {names_found}. removed {name_single}\")\n",
    "                #add the updated names_found to group_1_fixes_complete_csv_ii\n",
    "                group_1_fixes_complete_csv_ii.at[i, 'Names_Found'] = names_found\n",
    "                if len(names_found) == 1:\n",
    "                    #change the row in group_1_fixes_complete_csv_ii status to Success\n",
    "                        group_1_fixes_complete_csv_ii.at[i, 'Status'] = 'Success'\n",
    "                #break this loop \n",
    "                    #break\n",
    "\n",
    "        #Cyrillic Found guys\n",
    "        cyrillic_Found_rows_matching = Cyrillic_Found_Guys[(Cyrillic_Found_Guys['Match'] == match) & \n",
    "                                                            (Cyrillic_Found_Guys['Date'] == date) & \n",
    "                                                            ((Cyrillic_Found_Guys['Match(es) Found'] == f\"{names_found}\") | (Cyrillic_Found_Guys['English Name'] == original_jersey) | ((Cyrillic_Found_Guys['Match(es) Found'].apply(lambda x: isinstance(x, list) and any(name in x for name in row['Names_Found'])))))]\n",
    "        if len(cyrillic_Found_rows_matching) >= 1:\n",
    "            success_counter += 1\n",
    "            print(i, 'Found', name_single, names_found)\n",
    "            #remove the name you FOUND from names_found\n",
    "            names_found.remove(name_single)\n",
    "            this_row_removals += 1\n",
    "            print(f\"{i} - found using CYRILLIC FOUND DF. names found is now {names_found}. removed {name_single}\")\n",
    "            #add the updated names_found to group_1_fixes_complete_csv_ii\n",
    "            group_1_fixes_complete_csv_ii.at[i, 'Names_Found'] = names_found\n",
    "            if len(names_found) == 1:\n",
    "                #change the row in group_1_fixes_complete_csv_ii status to Success\n",
    "                    group_1_fixes_complete_csv_ii.at[i, 'Status'] = 'Success'\n",
    "            #break this loop \n",
    "                #break\n",
    "        for name_single in names_found:\n",
    "            cyrillic_Found_rows_matching = Cyrillic_Found_Guys[(Cyrillic_Found_Guys['Match'] == match) & \n",
    "            (Cyrillic_Found_Guys['Date'] == date) & \n",
    "            (Cyrillic_Found_Guys.apply(lambda row: name_single in eval(row['Match(es) Found']), axis=1) | \n",
    "             (Cyrillic_Found_Guys['ORIGINAL JERSEY'] == original_jersey) | (Cyrillic_Found_Guys['Match(es) Found'] == f\"[{name_single}]\"))]\n",
    "            #cyrillic_Found_rows_matching = Cyrillic_Found_Guys[(Cyrillic_Found_Guys['Match'] == match) & \n",
    "            #(Cyrillic_Found_Guys['Date'] == date) & \n",
    "            #((Cyrillic_Found_Guys['Name(s) Found'] == name_single) | (Cyrillic_Found_Guys['ORIGINAL JERSEY'] == original_jersey))]\n",
    "            if len(cyrillic_Found_rows_matching) >= 1:\n",
    "                #print(f'name is {name_single}. len is {len(cyrillic_Found_rows_matching)}. index is {cyrillic_Found_rows_matching.index}')\n",
    "                success_counter += 1\n",
    "                print(i, 'Found', name_single, names_found)\n",
    "                #remove the name you FOUND from names_found\n",
    "                names_found.remove(name_single)\n",
    "                this_row_removals += 1\n",
    "                print(f\"{i} - found using CYRILLIC FOUND DF dataset. names found is now {names_found}. removed {name_single}\")\n",
    "                #add the updated names_found to group_1_fixes_complete_csv_ii\n",
    "                group_1_fixes_complete_csv_ii.at[i, 'Names_Found'] = names_found\n",
    "                if len(names_found) == 1:\n",
    "                    #change the row in group_1_fixes_complete_csv_ii status to Success\n",
    "                       group_1_fixes_complete_csv_ii.at[i, 'Status'] = 'Success'\n",
    "                #break this loop \n",
    "                    #break\n",
    "        #lookup req'd matches \n",
    "        lookup_reqd_rows_matching = lookup_required_matches[(lookup_required_matches['Match'] == match) & \n",
    "                                                            (lookup_required_matches['Date'] == date) & \n",
    "                                                            ((lookup_required_matches['Name Match'] == f\"{names_found}\") | (lookup_required_matches['Jersey Name'] == original_jersey) | ((lookup_required_matches['Name Match'].apply(lambda x: isinstance(x, list) and any(name in x for name in row['Names_Found'])))))]\n",
    "        if len(lookup_reqd_rows_matching) >= 1:\n",
    "            success_counter += 1\n",
    "            print(i, 'Found', name_single, names_found)\n",
    "            #remove the name you FOUND from names_found\n",
    "            names_found.remove(name_single)\n",
    "            this_row_removals += 1\n",
    "            print(f\"{i} - found using LOOKUP REQD DF. names found is now {names_found}. removed {name_single}\")\n",
    "            #add the updated names_found to group_1_fixes_complete_csv_ii\n",
    "            group_1_fixes_complete_csv_ii.at[i, 'Names_Found'] = names_found\n",
    "            if len(names_found) == 1:\n",
    "                #change the row in group_1_fixes_complete_csv_ii status to Success\n",
    "                    group_1_fixes_complete_csv_ii.at[i, 'Status'] = 'Success'\n",
    "            #break this loop \n",
    "                #break\n",
    "        for name_single in names_found:\n",
    "            lookup_reqd_rows_matching = lookup_required_matches[(lookup_required_matches['Match'] == match) & \n",
    "            (lookup_required_matches['Date'] == date) & \n",
    "            (lookup_required_matches.apply(lambda row_1: name_single in eval(row_1['Name Match']), axis=1) | \n",
    "             (lookup_required_matches['Jersey Name'] == original_jersey))]\n",
    "            #cyrillic_Found_rows_matching = Cyrillic_Found_Guys[(Cyrillic_Found_Guys['Match'] == match) & \n",
    "            #(Cyrillic_Found_Guys['Date'] == date) & \n",
    "            #((Cyrillic_Found_Guys['Name(s) Found'] == name_single) | (Cyrillic_Found_Guys['ORIGINAL JERSEY'] == original_jersey))]\n",
    "            if len(lookup_reqd_rows_matching) >= 1:\n",
    "                #print(f'name is {name_single}. len is {len(cyrillic_Found_rows_matching)}. index is {cyrillic_Found_rows_matching.index}')\n",
    "                success_counter += 1\n",
    "                print(i, 'Found', name_single, names_found)\n",
    "                #remove the name you FOUND from names_found\n",
    "                names_found.remove(name_single)\n",
    "                this_row_removals += 1\n",
    "                print(f\"{i} - found using LOOKUP REQD dataset. names found is now {names_found}. removed {name_single}\")\n",
    "                #add the updated names_found to group_1_fixes_complete_csv_ii\n",
    "                group_1_fixes_complete_csv_ii.at[i, 'Names_Found'] = names_found\n",
    "                if len(names_found) == 1:\n",
    "                    #change the row in group_1_fixes_complete_csv_ii status to Success\n",
    "                        group_1_fixes_complete_csv_ii.at[i, 'Status'] = 'Success'\n",
    "                #break this loop \n",
    "                    #break\n",
    "\n",
    "        \n",
    "        #online lookup req'd 1 name found\n",
    "        online_lookup_required_df_large_rows_matching = online_lookup_required_matches[(online_lookup_required_matches['Match'] == match) & \n",
    "                                                            (online_lookup_required_matches['Date'] == date) & \n",
    "                                                            ((online_lookup_required_matches['Name(s) Found'] == f\"{names_found}\") | (online_lookup_required_matches['ORIGINAL JERSEY'] == original_jersey) | ((online_lookup_required_matches['Name(s) Found'].apply(lambda x: isinstance(x, list) and any(name in x for name in row['Names_Found'])))))]\n",
    "        if len(online_lookup_required_df_large_rows_matching) >= 1:\n",
    "            success_counter += 1\n",
    "            print(i, 'Found', name_single, names_found)\n",
    "            #remove the name you FOUND from names_found\n",
    "            names_found.remove(name_single)\n",
    "            this_row_removals += 1\n",
    "            print(f\"{i} - found using ONLINE LOOKUP REQD DF. names found is now {names_found}. removed {name_single}\")\n",
    "            #add the updated names_found to group_1_fixes_complete_csv_ii\n",
    "            group_1_fixes_complete_csv_ii.at[i, 'Names_Found'] = names_found\n",
    "            if len(names_found) == 1:\n",
    "                #change the row in group_1_fixes_complete_csv_ii status to Success\n",
    "                    group_1_fixes_complete_csv_ii.at[i, 'Status'] = 'Success'\n",
    "            #break this loop \n",
    "                #break\n",
    "        for name_single in names_found:\n",
    "            online_lookup_required_df_large_rows_matching = online_lookup_required_matches[(online_lookup_required_matches['Match'] == match) & \n",
    "            (online_lookup_required_matches['Date'] == date) & \n",
    "            ((online_lookup_required_matches['Name(s) Found'] == name_single) | (online_lookup_required_matches['ORIGINAL JERSEY'] == original_jersey) | (online_lookup_required_matches['Name(s) Found'] == f\"[{name_single}]\"))]\n",
    "\n",
    "            if len(online_lookup_required_df_large_rows_matching) >= 1:\n",
    "                success_counter += 1\n",
    "                print(i, 'Found', name_single, names_found)\n",
    "                #remove the name you FOUND from names_found\n",
    "                names_found.remove(name_single)\n",
    "                this_row_removals += 1\n",
    "                print(f\"{i} - found using ONLINE LOOKUP REQUIRED dataset. names found is now {names_found}. removed {name_single}\")\n",
    "                #add the updated names_found to group_1_fixes_complete_csv_ii\n",
    "                group_1_fixes_complete_csv_ii.at[i, 'Names_Found'] = names_found\n",
    "                if len(names_found) == 1:\n",
    "                    #change the row in group_1_fixes_complete_csv_ii status to Success\n",
    "                        group_1_fixes_complete_csv_ii.at[i, 'Status'] = 'Success'\n",
    "                #break this loop \n",
    "                    #break\n",
    "    if len(names_found) >= 2: #SECOND ROUND checking for similar arrays\n",
    "        if this_row_removals != 0:\n",
    "            0==0\n",
    "        else:\n",
    "            still_working_counter += 1\n",
    "        #if still multiple:\n",
    "        #check if there's an overlapping set of multiple dudes in other datasets as well \n",
    "\n",
    "            #if theres a matching multi set, assign one name to each row.\n",
    "\n",
    "        #combined DF large\n",
    "        name_conditions = [combined_df_large['Name(s) Found'].str.contains(name) for name in names_found]\n",
    "        combined_condition = reduce(lambda x, y: x & y, name_conditions)\n",
    "        final_condition = (\n",
    "            (combined_df_large['Date'] == date) & \n",
    "            (combined_df_large['Match'] == match) & \n",
    "            (\n",
    "                (combined_df_large['Name(s) Found'] == f\"{names_found}\") | \n",
    "                combined_condition\n",
    "            )\n",
    "        )\n",
    "        combined_df_large_match = combined_df_large[final_condition]\n",
    "        if len(combined_df_large_match) >= 1:\n",
    "            print(i, 'xxxxx - found a matching set of names in combined DF', names_found, combined_df_large_match['Name(s) Found'].unique())\n",
    "            second_part_success_counter += 1\n",
    "            still_working_counter -= 1\n",
    "\n",
    "        #terrell new DF \n",
    "        name_conditions = [terrell_new_df['Name(s) Found'].str.contains(name) for name in names_found]\n",
    "        combined_condition = reduce(lambda x, y: x & y, name_conditions)\n",
    "        final_condition = (\n",
    "            (terrell_new_df['Date'] == date) & \n",
    "            (terrell_new_df['Match'] == match) & \n",
    "            (\n",
    "                (terrell_new_df['Name(s) Found'] == f\"{names_found}\") | \n",
    "                combined_condition\n",
    "            )\n",
    "        )\n",
    "        t_new_df_match = terrell_new_df[final_condition]\n",
    "        if len(t_new_df_match) >= 1:\n",
    "            print(i, 'xxxxx - found a matching set of names in terrells new DF', names_found, t_new_df_match['Name(s) Found'].unique())\n",
    "            second_part_success_counter += 1\n",
    "            still_working_counter -= 1\n",
    "\n",
    "        #large Dataset said 0 - Name(s) Found column\n",
    "        name_conditions = [large_dataset_said_0['Name(s) Found'].str.contains(name) for name in names_found]\n",
    "        combined_condition = reduce(lambda x, y: x & y, name_conditions)\n",
    "        final_condition = (\n",
    "            (large_dataset_said_0['Date'] == date) & \n",
    "            (large_dataset_said_0['Match'] == match) & \n",
    "            (\n",
    "                (large_dataset_said_0['Name(s) Found'] == f\"{names_found}\") | \n",
    "                combined_condition\n",
    "            )\n",
    "        )\n",
    "        large_dataset_0_match = large_dataset_said_0[final_condition]\n",
    "        if len(large_dataset_0_match) >= 1:\n",
    "            print(i, 'xxxxx - found a matching set of names in the large dataset which said that 0', names_found, large_dataset_0_match['Name(s) Found'].unique())\n",
    "            second_part_success_counter += 1\n",
    "            still_working_counter -= 1\n",
    "            if len(names_found) == len(eval(large_dataset_0_match['Name(s) Found'].unique()[0])):\n",
    "                if len(names_found) == 2:\n",
    "                    large_dataset_said_0.at[large_dataset_0_match.index[0], 'Name(s) Found'] = names_found[0]\n",
    "                    group_1_fixes_complete_csv_ii.at[i, 'Names_Found'] = names_found[1]\n",
    "                else:\n",
    "                    print(len(names_found), 'oddity')\n",
    "        \n",
    "        #match date 0 guys - Name column \n",
    "        name_conditions = [match_date_zero_guys['Name'].str.contains(name) for name in names_found]\n",
    "        combined_condition = reduce(lambda x, y: x & y, name_conditions)\n",
    "        final_condition = (\n",
    "            (match_date_zero_guys['Date'] == date) & \n",
    "            (match_date_zero_guys['Match'] == match) & \n",
    "            (\n",
    "                (match_date_zero_guys['Name'] == f\"{names_found}\") | \n",
    "                combined_condition\n",
    "            )\n",
    "        )\n",
    "        match_date_0_match = match_date_zero_guys[final_condition]\n",
    "        if len(match_date_0_match) >= 1:\n",
    "            print(i, 'xxxxx - found a matching set of names in the match date filter left 0 dataset', names_found, match_date_0_match['Name'].unique())\n",
    "            second_part_success_counter += 1\n",
    "            still_working_counter -= 1\n",
    "            if len(names_found) == len(eval(match_date_0_match['Name'].unique()[0])):\n",
    "                if len(names_found) == 2:\n",
    "                    match_date_zero_guys.at[match_date_0_match.index[0], 'Name'] = names_found[0]\n",
    "                    group_1_fixes_complete_csv_ii.at[i, 'Names_Found'] = names_found[1]\n",
    "                else:\n",
    "                    print(len(names_found), 'oddity')\n",
    "\n",
    "        #wrong DF - Found Name column\n",
    "        name_conditions = [terrell_wrong_df['Found Name'].str.contains(name) for name in names_found]\n",
    "        combined_condition = reduce(lambda x, y: x & y, name_conditions)\n",
    "        final_condition = (\n",
    "            (terrell_wrong_df['Date'] == date) & \n",
    "            (terrell_wrong_df['Match'] == match) & \n",
    "            (\n",
    "                (terrell_wrong_df['Found Name'] == f\"{names_found}\") | \n",
    "                combined_condition\n",
    "            )\n",
    "        )\n",
    "        wrong_df_match = terrell_wrong_df[final_condition]\n",
    "        if len(wrong_df_match) >= 1:\n",
    "            print(i, 'xxxxx - found a matching set of names in Terrell\\'s wrong DF', names_found, wrong_df_match['Found Name'].unique())\n",
    "            second_part_success_counter += 1\n",
    "            still_working_counter -= 1\n",
    "            if len(names_found) == len(eval(wrong_df_match['Found Name'].unique()[0])):\n",
    "                if len(names_found) == 2:\n",
    "                    terrell_wrong_df.at[wrong_df_match.index[0], 'Found Name'] = names_found[0]\n",
    "                    group_1_fixes_complete_csv_ii.at[i, 'Names_Found'] = names_found[1]\n",
    "                else:\n",
    "                    print(len(names_found), 'oddity')\n",
    "\n",
    "    if len(names_found) >= 2: #3 check for empty rows w the same jersey\n",
    "        print(f\"***{i}***\")\n",
    "        odd_name_candidates = odd_name_0_cases[(odd_name_0_cases['Match'] == match) &\n",
    "                 (odd_name_0_cases['Date'] == date) & \n",
    "                 (odd_name_0_cases['Nationality'] == nationality)]\n",
    "        \n",
    "        if len(odd_name_candidates) >= 1:\n",
    "             print(i, 'found a row of this in odd candidates')\n",
    "\n",
    "        cyrillic_row_candidates = cyrillic_guys_0_found[(cyrillic_guys_0_found['Match'] == match) &\n",
    "                        (cyrillic_guys_0_found['Date'] == date) & \n",
    "                 (cyrillic_guys_0_found['Nationality'] == nationality)]\n",
    "        \n",
    "        if len(cyrillic_row_candidates) >= 1:\n",
    "             print(i, 'found a row of this in cyrillic candidates')\n",
    "\n",
    "        online_lookup_row_candidates = online_lookup_guys_0_found[(online_lookup_guys_0_found['Match'] == match) &\n",
    "                        (online_lookup_guys_0_found['Date'] == date) & \n",
    "                 (online_lookup_guys_0_found['Nationality'] == nationality)]\n",
    "        \n",
    "        if len(online_lookup_row_candidates) >= 1:\n",
    "             print(i, 'Found a row of this in online lookup guys')\n",
    "\n",
    "        lookup_dudes_candidates = lookup_dudes[(lookup_dudes['Match'] == match) & \n",
    "                                               (lookup_dudes['Date'] == date) & \n",
    "                                               (lookup_dudes['Nationality'] == nationality)]\n",
    "        \n",
    "        if len(lookup_dudes_candidates) >= 1:\n",
    "             print(i, 'found a candidate rrow in weird lookup dudes')\n",
    "\n",
    "\n",
    "        \n",
    "print(instances, success_counter, still_working_counter, second_part_success_counter, single_mandem_counter)\n",
    "\n",
    "\n",
    "#STILL MULTIPLE\n",
    "\n",
    "#check across the weird datasets to find this jersey, match and date but a FAIL\n",
    "\n",
    "    #if theres 1 row with the right looking jersey and 0 for a name\n",
    "        #and you have 2 names in the set\n",
    "\n",
    "    #same if you find 2 rows with a 0 and there are 3 names in the set \n",
    "\n",
    "    #group 3\n",
    "        #odd_name_found_0_cases\n",
    "        #Cyrillic Guys Nobody Found\n",
    "        #Online lookup required - 0 names found\n",
    "\n",
    "    #other group\n",
    "        #lookup_required_dudes\n",
    "\n",
    "\n",
    "\n",
    "#prev run - 271 instances, 64 success. 191 still working. 16 success in part 2\n",
    "#2nd - 212 instances, 0 success. 196 still working. 16 success in part 2\n",
    "#3rd - 204 instances, 0 success. 196 still working. 8 success in part 2\n",
    "\n",
    "\n",
    "#figure out if there are 2 names in names found\n",
    "#and 2 names in the array it found.\n",
    "\n",
    "#if so, assign 1 name to each dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group_1_fixes_complete_csv_ii.to_csv('updated_Group1_fixes_complete_ii.csv')\n",
    "#large_dataset_said_0.to_csv('updated_large_dataset_said_0.csv')\n",
    "#combined_df_large.to_csv('updated_large_combined_DF.csv')\n",
    "#terrell_wrong_df.to_csv('updated Terrell lookup cases - wrong DF.csv')\n",
    "#match_date_zero_guys.to_csv('updated match date said zero guys.csv')\n",
    "#terrell_new_df.to_csv('updated Terrell new DF.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
