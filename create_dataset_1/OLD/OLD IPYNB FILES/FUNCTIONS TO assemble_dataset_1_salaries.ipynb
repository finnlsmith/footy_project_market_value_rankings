{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from difflib import get_close_matches\n",
    "import numpy as np\n",
    "import re\n",
    "from transliterate import translit\n",
    "from unidecode import unidecode\n",
    "import Levenshtein\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "import wikipediaapi\n",
    "\n",
    "import wikipedia\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import difflib\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import calendar\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for salary lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GETTING NULL NAMES - SALARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names_with_conditions_salary(df):\n",
    "    # Create an empty list to store names that satisfy the conditions\n",
    "    result_names = []\n",
    "\n",
    "    # Iterate through unique names in the DataFrame\n",
    "    for name in df['Name'].unique():\n",
    "        # Create a subset of the DataFrame for the current name\n",
    "        subset = df[df['Name'] == name].reset_index()\n",
    "\n",
    "        # Check conditions: length of subset is 1 and 'Weekly Salary' is NaN\n",
    "        if len(subset) == 1 and pd.isna(subset['Weekly Salary'].iloc[0]):\n",
    "            result_names.append(name)\n",
    "\n",
    "    return result_names\n",
    "\n",
    "#result_names_null = get_names_with_conditions_salary(leagues_salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single name - string process / lookup - Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cyrillic(input_string):\n",
    "    # Check if the string contains non-ASCII characters\n",
    "    return not input_string.isascii()\n",
    "\n",
    "def cyrillic_to_latin(input_string):\n",
    "    try:\n",
    "        # Use the \"translit\" function to convert Cyrillic to Latin\n",
    "        latin_string = translit(input_string, 'ru', reversed=True)\n",
    "        return latin_string\n",
    "    except Exception as e:\n",
    "        # Handle exceptions, e.g., if the input is not valid Cyrillic\n",
    "        print(f\"Error: {e}\")\n",
    "        return input_string\n",
    "\n",
    "def find_best_match(array, final_tokens, ORIGINAL_STRING):\n",
    "    # Concatenate final tokens to form the expected full name\n",
    "    expected_name = ' '.join(final_tokens)\n",
    "    \n",
    "    # Filter names that start with the initial letter\n",
    "    filtered_names = [name for name in array if name.startswith(final_tokens[0])]\n",
    "    \n",
    "    if not filtered_names:\n",
    "        if(len(final_tokens) == 1):\n",
    "            filtered_names = [name for name in array if name.endswith(final_tokens[-1])]\n",
    "            final_tokens = [final_tokens[-1]]\n",
    "        # Try switching the order of final tokens\n",
    "        else:\n",
    "            filtered_names = [name for name in array if name.startswith(final_tokens[1])]\n",
    "            final_tokens = [final_tokens[1], final_tokens[0]]\n",
    "    \n",
    "    if not filtered_names:\n",
    "        return None  # No matching names found\n",
    "    \n",
    "    # Check if the ORIGINAL_STRING contains a backtick/apostrophe\n",
    "    has_backtick_apostrophe = \"'\" in ORIGINAL_STRING or \"`\" in ORIGINAL_STRING\n",
    "    \n",
    "    # Filter names based on the presence of a backtick/apostrophe\n",
    "    filtered_names = [name for name in filtered_names if \"'\" in name or \"`\" in name] if has_backtick_apostrophe else filtered_names\n",
    "    \n",
    "    if not filtered_names:\n",
    "        return None  # No matching names found\n",
    "    \n",
    "    # Calculate Levenshtein distance between the expected name and each remaining name\n",
    "    distances = [Levenshtein.distance(unidecode(expected_name), unidecode(name.replace(\" \", \"\"))) for name in filtered_names]\n",
    "    \n",
    "        # Find the minimum distance\n",
    "    min_distance = min(distances)\n",
    "\n",
    "    # Find the indices of names with the minimum distance\n",
    "    min_distance_indices = [i for i, distance in enumerate(distances) if distance == min_distance]\n",
    "\n",
    "    # Return all names with the minimum distance\n",
    "    result_names = [filtered_names[i] for i in min_distance_indices]\n",
    "\n",
    "    # Print or return the result based on your requirement\n",
    "    return result_names\n",
    "    #print(result_names)\n",
    "\n",
    "def process_string_newest_ii(input_string):\n",
    "    cleaned_string = re.sub(r'^\\d{1,2}[. ]', '', input_string)\n",
    "    tokens = cleaned_string.split()\n",
    "    final_string = \"\"\n",
    "\n",
    "    if len(tokens) >= 1 and not re.match(r'^[A-Za-zÀ-ÖØ-öø-ÿ]{2,}$', tokens[0]):\n",
    "        initial_match = re.match(r'^([A-Za-zÀ-ÖØ-öø-ÿ]+\\.)+$|[A-Za-zÀ-ÖØ-öø-ÿ]\\.$|[A-Za-zÀ-ÖØ-öø-ÿ]$', tokens[0])\n",
    "        if initial_match:\n",
    "            final_string += initial_match.group()\n",
    "\n",
    "    main_phrase = \" \".join(word for word in tokens if len(\"\".join(char for char in word if char.isalpha())) >= 2)\n",
    "    if main_phrase:\n",
    "        final_string += \" \" + main_phrase\n",
    "\n",
    "    if len(tokens) >= 2 and not re.match(r'^[A-Za-zÀ-ÖØ-öø-ÿ]{2,}$', tokens[1]):\n",
    "        end_initial_match = re.match(r'^([A-Za-zÀ-ÖØ-öø-ÿ]+\\.)+$|[A-Za-zÀ-ÖØ-öø-ÿ]\\.$|[A-Za-zÀ-ÖØ-öø-ÿ]$', tokens[1])\n",
    "        if end_initial_match:\n",
    "            final_string = \"\".join(char for char in end_initial_match.group() if char.isalpha()) + \" \" + final_string\n",
    "\n",
    "    # Check if the final phrase ends in a period\n",
    "    if final_string.endswith(\".\"):\n",
    "        # Extract the last word, remove the period, and move it to the start of final_string\n",
    "        last_word = final_string.split()[-1].rstrip('.')\n",
    "        final_string = last_word + \" \" + final_string\n",
    "\n",
    "        # Remove the last word from the end of the string\n",
    "        final_string = ' '.join(final_string.split()[:-1])\n",
    "\n",
    "        final_string = final_string.strip()\n",
    "\n",
    "    # Separate the final string by \" \" and remove non-alphabet characters for each token\n",
    "    final_tokens = [re.sub(r'[^A-Za-zÀ-ÖØ-öø-ÿćč-]', '', token) for token in final_string.split()]\n",
    "\n",
    "    # If the first two tokens are the same, remove one token\n",
    "    if len(final_tokens) >= 2 and final_tokens[0] == final_tokens[1]:\n",
    "        final_tokens.pop(0)\n",
    "\n",
    "    joined_string = \" \".join(final_tokens)\n",
    "    return joined_string, final_tokens\n",
    "\n",
    "    #return \" \".join(final_tokens)\n",
    "\n",
    "def extract_first_name(match_apostrophes_accounted, lastname_match):\n",
    "    # Check if lastname_match is part of match_apostrophes_accounted\n",
    "    if lastname_match in match_apostrophes_accounted:\n",
    "        # Split the string using lastname_match as the separator\n",
    "        first_name = match_apostrophes_accounted.split(lastname_match)[0].strip()\n",
    "        return first_name\n",
    "    else:\n",
    "        # Handle the case where lastname_match is not found in match_apostrophes_accounted\n",
    "        print(f\"{lastname_match} not found in {match_apostrophes_accounted}\")\n",
    "        return None\n",
    "    \n",
    "def add_backticks(lastname_match, original_string_nojersey):\n",
    "    # Find the indices of backticks/apostrophes in the original string\n",
    "    special_indices = [i for i, char in enumerate(original_string_nojersey) if char in (\"`\", \"'\")]\n",
    "\n",
    "    # Add backticks in the corresponding places in the last name match\n",
    "    for index in special_indices:\n",
    "        # Check if the index is within the range of the last_name_match\n",
    "        if 0 <= index < len(lastname_match):\n",
    "            # Insert backtick in the appropriate position\n",
    "            lastname_match = lastname_match[:index] + original_string_nojersey[index] + lastname_match[index:]\n",
    "\n",
    "    return lastname_match\n",
    "\n",
    "def find_closest_string_newEST(input_string, string_list, input_final_tokens, ORIGINAL_NAME_STRING):\n",
    "    #replace nationality name list with string_list\n",
    "    #replace string_for_search with input_string \n",
    "    #replace final_tokens with input_final_tokens\n",
    "    #replace input_string with ORIGINAL_NAME_STRING\n",
    "\n",
    "    closest_match = get_close_matches(input_string, string_list, n=1, cutoff=0.8)\n",
    "\n",
    "    closest_match_4 = []\n",
    "    closest_match_3 = []\n",
    "    closest_match_2 = []\n",
    "    index_match = \"\"\n",
    "    matching_indices = []\n",
    "\n",
    "    if closest_match:\n",
    "        #TIGHT CLOSE MATCH FUNCTION RETURNS A NAME\n",
    "        #print('0', closest_match[0], closest_match in string_list)\n",
    "        #RETURN HERE\n",
    "        return closest_match[0]\n",
    "    else:\n",
    "        #Reduce match constraints\n",
    "        closest_match_ii = get_close_matches(input_string, string_list)\n",
    "        #Produces a match\n",
    "        if closest_match_ii:\n",
    "            if((type(closest_match_ii) == list) & (len(closest_match_ii) >= 2)):\n",
    "                #Closest match II returns 1 name\n",
    "                original_string_nojersey = re.sub(r'^\\d+(\\.)?\\s*', '', ORIGINAL_NAME_STRING)\n",
    "                #Find best match from set of names\n",
    "                result_1 = find_best_match(closest_match_ii, input_final_tokens, original_string_nojersey)\n",
    "                if(type(result_1) == list):\n",
    "                    closest_match_ii = result_1\n",
    "                    #print(closest_match_ii)\n",
    "                    return closest_match_ii\n",
    "                elif(pd.isna(result_1)):\n",
    "                    #none of the names from closest match ii were a good match\n",
    "                    0==0\n",
    "                else:\n",
    "                    #1 of the names from closest match ii were a good match\n",
    "                    #print('closest match ii best match: ' + result_1)\n",
    "                    closest_match_ii = result_1\n",
    "                    #RETURN HERE\n",
    "                    return closest_match_ii\n",
    "            else:\n",
    "                #Closest match II returns 1 name\n",
    "                #print('1 match ' + closest_match_ii[0])\n",
    "                #RETURN HERE\n",
    "                return closest_match_ii[0]\n",
    "        else:\n",
    "            # no close matches\n",
    "            last_word = input_string.split()[-1]\n",
    "            #KREJCI CASE \n",
    "            if(last_word == 'Krejčí'):\n",
    "                match_krejci = get_close_matches(last_word, string_list, n=1, cutoff=0.380952)\n",
    "                if(type(match_krejci) == list):\n",
    "                    if(len(match_krejci) == 1):\n",
    "                        match_krejci = match_krejci[0]\n",
    "                        return match_krejci\n",
    "                    \n",
    "                \n",
    "                \n",
    "\n",
    "            \n",
    "            #if this is an initial you need to save it as an initial or a word start \n",
    "            #if(last_word)\n",
    "\n",
    "            # Return strings from the list if the last word is in those strings\n",
    "            matching_strings = [s for s in string_list if last_word in s]\n",
    "\n",
    "            if matching_strings:\n",
    "                if(len(matching_strings) == 1):\n",
    "                    #print('match string ' + matching_strings[0])\n",
    "                    #RETURN HERE\n",
    "                    return matching_strings[0]\n",
    "                else:\n",
    "                    #print(matching_strings)\n",
    "                    setofmatches = matching_strings\n",
    "                    \n",
    "        #elif(closest_match):       \n",
    "        if(closest_match):\n",
    "            #RETURN\n",
    "            print('1', closest_match[0], closest_match[0] in string_list)\n",
    "        elif(closest_match_ii):\n",
    "            if(type(closest_match_ii) == str):\n",
    "                print(closest_match_ii, closest_match_ii in string_list)\n",
    "            #RETURN\n",
    "            else:\n",
    "                print('ii', closest_match_ii)#, closest_match_ii[0] in string_list\n",
    "        elif(closest_match_3):\n",
    "            #RETURN\n",
    "            print('3', closest_match_3[0], closest_match_3[0] in string_list)\n",
    "        elif(closest_match_2):\n",
    "            #RETURN\n",
    "            print('2', closest_match_2[0], closest_match_2[0] in string_list)\n",
    "        elif(closest_match_4):\n",
    "            #RETURN\n",
    "            print('4', closest_match_4[0], closest_match_4[0] in string_list)\n",
    "        elif(index_match != \"\"):\n",
    "            #RETURN\n",
    "            print('end ' + index_match + ORIGINAL_NAME_STRING, matching_indices)\n",
    "            return(index_match)\n",
    "        else:\n",
    "            #RETURN\n",
    "            return(\"No close match found.\")\n",
    "            #print(\"No close match found.\")\n",
    "\n",
    "def filter_candidates(NAMESTRING, LISTCANDIDATES):\n",
    "    # Get the first token of the NAMESTRING\n",
    "    first_token = re.split(r'\\s', NAMESTRING)[0]\n",
    "\n",
    "    # Create a regex pattern for matching candidates that start with the first token\n",
    "    pattern = re.compile(fr'^{re.escape(first_token)}', re.IGNORECASE)\n",
    "\n",
    "    # Filter candidates based on the pattern\n",
    "    filtered_candidates = list(filter(lambda x: re.match(pattern, x), LISTCANDIDATES))\n",
    "\n",
    "    return filtered_candidates\n",
    "\n",
    "def remove_accents_from_strings(input_array):\n",
    "    # Ensure the input is a numpy array\n",
    "    if not isinstance(input_array, np.ndarray) or input_array.dtype != np.dtype('O'):\n",
    "        raise ValueError(\"Input must be a NumPy array of strings\")\n",
    "\n",
    "    # Define a function to remove accents from a single string\n",
    "    def remove_accents_single_string(s):\n",
    "        return unidecode(s)\n",
    "\n",
    "    # Vectorize the function to apply it element-wise to the array\n",
    "    remove_accents_vectorized = np.vectorize(remove_accents_single_string)\n",
    "\n",
    "    # Apply the vectorized function to each element in the array\n",
    "    result_array = remove_accents_vectorized(input_array)\n",
    "\n",
    "    return result_array\n",
    "\n",
    "def find_names_with_accents(target_name, name_array):\n",
    "    # Ensure the input is a numpy array\n",
    "    if not isinstance(name_array, np.ndarray) or name_array.dtype != np.dtype('O'):\n",
    "        raise ValueError(\"Input must be a NumPy array of strings\")\n",
    "\n",
    "    # Remove accents from the target name\n",
    "    target_name_without_accents = unidecode(target_name)\n",
    "\n",
    "    # Define a function to check if a name with accents matches the target name\n",
    "    def has_accent_match(name):\n",
    "        return unidecode(name) == target_name_without_accents\n",
    "\n",
    "    # Vectorize the function to apply it element-wise to the array\n",
    "    has_accent_match_vectorized = np.vectorize(has_accent_match)\n",
    "\n",
    "    # Apply the vectorized function to each element in the array\n",
    "    matching_names = name_array[has_accent_match_vectorized(name_array)]\n",
    "\n",
    "    if(len(matching_names) == 1):\n",
    "        return matching_names[0]\n",
    "\n",
    "    return matching_names\n",
    "\n",
    "def remove_apostrophes_backticks(input_array):\n",
    "    # Ensure the input is a numpy array\n",
    "    if not isinstance(input_array, np.ndarray) or input_array.dtype != np.dtype('O'):\n",
    "        raise ValueError(\"Input must be a NumPy array of strings\")\n",
    "\n",
    "    # Define a function to remove apostrophes and backticks from a single string\n",
    "    def remove_chars_single_string(s):\n",
    "        return np.char.replace(np.char.replace(s, \"'\", ''), \"`\", '')\n",
    "\n",
    "    # Vectorize the function to apply it element-wise to the array\n",
    "    remove_chars_vectorized = np.vectorize(remove_chars_single_string)\n",
    "\n",
    "    # Apply the vectorized function to each element in the array\n",
    "    result_array = remove_chars_vectorized(input_array)\n",
    "\n",
    "    return result_array\n",
    "\n",
    "def transform_korean_name(name):\n",
    "    # Split the name into parts\n",
    "    parts = name.split()\n",
    "\n",
    "    # Check if the name has at least two parts\n",
    "    if len(parts) >= 2:\n",
    "        # Format the name as \"Ja-cheol Koo\"\n",
    "        transformed_name = f\"{parts[1].capitalize()}-{parts[0].capitalize()}\"\n",
    "        return transformed_name\n",
    "    else:\n",
    "        # Return the original name if it doesn't have at least two parts\n",
    "        return name\n",
    "    \n",
    "def remove_apostrophes_backticks_single_string(input_string):\n",
    "    # Ensure the input is a string\n",
    "    if not isinstance(input_string, str):\n",
    "        raise ValueError(\"Input must be a string\")\n",
    "\n",
    "    # Define a function to remove apostrophes and backticks from a single string\n",
    "    def remove_chars_single_string(s):\n",
    "        return s.replace(\"'\", '').replace(\"`\", '')\n",
    "\n",
    "    # Apply the function to the input string\n",
    "    result_string = remove_chars_single_string(input_string)\n",
    "\n",
    "    return result_string\n",
    "\n",
    "def filter_names_first_initial_lastname(database, search_string):\n",
    "    # Filter out non-string elements from the database\n",
    "    string_database = [str(item) for item in database if isinstance(item, str)]\n",
    "    \n",
    "    # Convert search string to lowercase for case-insensitive matching\n",
    "    search_string_lower = search_string.lower()\n",
    "    \n",
    "    # Split the search string into parts\n",
    "    parts = search_string_lower.split()\n",
    "    \n",
    "    # Filter names based on conditions\n",
    "    filtered_names = [name for name in string_database if all(part in name.lower() for part in parts)]\n",
    "    \n",
    "    return filtered_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GETTING NULL NAMES - VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names_with_conditions_values(df):\n",
    "    # Create an empty list to store names that satisfy the conditions\n",
    "    result_names = []\n",
    "\n",
    "    # Iterate through unique names in the DataFrame\n",
    "    for name in df['Name'].unique():\n",
    "        # Create a subset of the DataFrame for the current name\n",
    "        subset = df[df['Name'] == name].reset_index()\n",
    "\n",
    "        # Check conditions: length of subset is 1 and 'Market Value' is equal to '-'\n",
    "        if len(subset) == 1 and subset['Market Value'].iloc[0] == '-':\n",
    "            result_names.append(name)\n",
    "\n",
    "    return result_names\n",
    "\n",
    "#result_names_null_values = get_names_with_conditions_values(leagues_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Name in Database / use online search / impute - FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_name(input_name, input_nationality, input_match_date, using_salaries_boolean):\n",
    "\n",
    "    example_problem = input_name\n",
    "    natl_test = input_nationality\n",
    "    input_year_test = input_match_date\n",
    "\n",
    "    if(using_salaries_boolean == True): #salaries_or_values == 'salary'\n",
    "        database_name = leagues_salary\n",
    "        money_column_name = \"Inflation-Adjusted Yearly Salary\"\n",
    "    elif(using_salaries_boolean == False): #salaries_or_values == 'value'\n",
    "        database_name = leagues_value\n",
    "        money_column_name = \"Market Value\"\n",
    "\n",
    "        \n",
    "    candidate_name = \"\"\n",
    "    candidates_set = []\n",
    "    match_type = \"\"\n",
    "\n",
    "    #players from their country \n",
    "    dataset_nationality = database_name[database_name['Nationality'] == f\"{natl_test}\"]['Name'].unique()\n",
    "\n",
    "    if(is_cyrillic(example_problem)):\n",
    "        #change from cyrillic to english\n",
    "        example_problem = cyrillic_to_latin(example_problem)\n",
    "\n",
    "    #remove jersey Nums and order initials correctly. \n",
    "    search_name, final_tokens_name = process_string_newest_ii(example_problem) \n",
    "\n",
    "    if(len(dataset_nationality) == 0):\n",
    "        print(f'length of {input_nationality} dataset is 0')\n",
    "        match_type = \"none\"\n",
    "        return 0, match_type, search_name\n",
    "    else:\n",
    "\n",
    "        #look their name up in the list of names from their nationality. \n",
    "        result = find_closest_string_newEST(search_name, dataset_nationality, final_tokens_name, example_problem)\n",
    "        if((type(result) == list) & (len(result) >= 2)):\n",
    "            candidates_set = result\n",
    "        elif(result[0] in dataset_nationality):\n",
    "            candidate_name = result[0]\n",
    "        elif(result in dataset_nationality):\n",
    "            #print(result)\n",
    "            #RETURN \n",
    "            candidate_name = result\n",
    "        else:\n",
    "            #no match found after first call \n",
    "            print('no initial match found: ', search_name)\n",
    "            nationality_names_accents_removed = remove_accents_from_strings(dataset_nationality)\n",
    "            match_accent_accounted = find_closest_string_newEST(search_name, nationality_names_accents_removed,final_tokens_name, example_problem)\n",
    "            if(match_accent_accounted in nationality_names_accents_removed):\n",
    "                #print(match_accent_accounted)\n",
    "                matching_names_with_accents = find_names_with_accents(match_accent_accounted, dataset_nationality)\n",
    "                if(type(matching_names_with_accents) == str):\n",
    "                    print(matching_names_with_accents)\n",
    "                    #RETURN\n",
    "                    candidate_name = matching_names_with_accents\n",
    "                elif(len(matching_names_with_accents) == 0):\n",
    "                    print(f'accent-less name found: {match_accent_accounted}. But name not in original dataset')\n",
    "                else:\n",
    "                    print(f'multiple names found after adding accents: {matching_names_with_accents}')\n",
    "                    candidates_set = matching_names_with_accents\n",
    "\n",
    "\n",
    "                #MAKE SURE THE NAME WITH ACCENTS IS IN DATASET NATIONALITY \n",
    "            else:\n",
    "                print('no accent match found:', search_name)\n",
    "\n",
    "                dataset_nationality_backticks = remove_apostrophes_backticks(dataset_nationality) #dataset_nationality_updated\n",
    "                match_apostrophes_accounted = find_closest_string_newEST(search_name, dataset_nationality_backticks,final_tokens_name, example_problem)\n",
    "            \n",
    "                if(match_apostrophes_accounted in dataset_nationality_backticks):\n",
    "\n",
    "                    lastname_match = match_apostrophes_accounted.split()[-1] \n",
    "                    original_string_nojersey = re.sub(r'^\\d+(\\.)?\\s*', '', example_problem)\n",
    "                    correct_lastname = add_backticks(lastname_match, original_string_nojersey)\n",
    "                    correct_firstname = extract_first_name(match_apostrophes_accounted, lastname_match)\n",
    "                    \n",
    "                    correct_name_full = correct_firstname + ' ' + correct_lastname\n",
    "\n",
    "                    if(correct_name_full in dataset_nationality):\n",
    "                        print(correct_name_full)\n",
    "                        #RETURN\n",
    "                        candidate_name = correct_name_full\n",
    "                    elif(correct_name_full.replace('`', \"'\") in dataset_nationality):\n",
    "                        print(correct_name_full.replace('`', \"'\"))\n",
    "                        #RETURN\n",
    "                        candidate_name = correct_name_full.replace('`', \"'\")\n",
    "                    elif(type(match_apostrophes_accounted) != str):\n",
    "                        print(f'multiple names found after adding backticks: {match_apostrophes_accounted}')\n",
    "                        candidates_set = match_apostrophes_accounted\n",
    "                    else:\n",
    "                        print(f'backtick-less name found: {match_apostrophes_accounted}. But name not in original dataset')\n",
    "                \n",
    "                else:\n",
    "                    print('no backtick match found:', search_name)\n",
    "\n",
    "        if(candidate_name != \"\"):\n",
    "            match_type = \"single\"\n",
    "            return candidate_name, match_type, search_name\n",
    "        else:\n",
    "            if(candidates_set != []):\n",
    "                match_type = \"multiple\"\n",
    "                return candidates_set, match_type, search_name\n",
    "            else:\n",
    "                match_type = \"none\"\n",
    "                return 0, match_type, search_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_money_info_from_name(input_name, input_nationality, input_match_date, using_salaries_boolean):\n",
    "\n",
    "    candidate_name = input_name\n",
    "    input_year_test = input_match_date\n",
    "\n",
    "    if(using_salaries_boolean == True): #salaries_or_values\n",
    "        database_name = leagues_salary\n",
    "        money_column_name = \"Inflation-Adjusted Yearly Salary\"\n",
    "        nulldb_name_list = result_names_null\n",
    "    elif(using_salaries_boolean == False): #salaries_or_values\n",
    "        database_name = leagues_value\n",
    "        money_column_name = \"Market Value\"\n",
    "        nulldb_name_list = result_names_null_values\n",
    "\n",
    "    yearstr = input_year_test.split(\".\")[2]\n",
    "    full_num = '20' + yearstr\n",
    "\n",
    "    return_case = \"\"\n",
    "    lookup_required = False\n",
    "    imputed_salary = False\n",
    "\n",
    "    final_salary = 0\n",
    "\n",
    "    #Check current season \n",
    "    that_season_that_guy = database_name[(database_name['Name'] == candidate_name) & (database_name['Season'] == int(full_num))]\n",
    "\n",
    "    ###CHECK AGAINST NULL LIST\n",
    "    if(candidate_name in nulldb_name_list):\n",
    "        \n",
    "        #NULL NAME - NOT IN DB SEASON OF MATCH\n",
    "        if(len(that_season_that_guy) == 0):\n",
    "            prev_season_that_guy = database_name[(database_name['Name'] == candidate_name) & (database_name['Season'] == (int(full_num) + 1))]\n",
    "            next_season_that_guy = database_name[(database_name['Name'] == candidate_name) & (database_name['Season'] == (int(full_num) - 1))]\n",
    "            thatguy_3seasons = pd.concat([that_season_that_guy, prev_season_that_guy, next_season_that_guy], ignore_index=True)\n",
    "            \n",
    "            #NOT IN DB FOR YEAR BEFORE OR AFTER THE MATCH\n",
    "            if(len(thatguy_3seasons) == 0):\n",
    "                #print(f'{candidate_name} wasn\\'t in the db in {full_num}, {int(full_num) + 1} or {int(full_num) - 1} ')\n",
    "\n",
    "                ###AAA\n",
    "                return_case = \"not in DB any of 3 seasons.\"\n",
    "                lookup_required = True\n",
    "            else:\n",
    "\n",
    "                ####how do we make sure this guy was really the guy??? \n",
    "                ####ZZZZ   \n",
    "                print(f'{candidate_name} was not in the db in {full_num}, but was in {int(full_num) + 1} or {int(full_num) - 1} ')\n",
    "                lookup_required = True\n",
    "                return_case = \"was in DB in before or after season\"\n",
    "            \n",
    "        else:\n",
    "        ###IMPUTE CASE 1###\n",
    "        #NULL NAME, BUT THEY WERE IN THE DB THE SEASON OF THE MATCH\n",
    "            league = that_season_that_guy.reset_index().at[0, 'League']\n",
    "            season = that_season_that_guy.reset_index().at[0, 'Season']\n",
    "            age = that_season_that_guy.reset_index().at[0, 'Age']\n",
    "            mean_salary_values = pd.to_numeric(database_name[(database_name['League'] == league) & (database_name['Season'] == season) & (database_name['Age'] == age)][f'{money_column_name}'])\n",
    "            meansalary = mean_salary_values.mean()\n",
    "            final_salary = meansalary\n",
    "\n",
    "            #print(f\"For {candidate_name}, imputed salary of {meansalary} using {league}, {season} season and age {age}\")\n",
    "            \n",
    "            ####IS THERE ANY WAY THIS GUY IS NOT THE GUY?\n",
    "            imputed_salary = True\n",
    "            return_case = \"working - imputed\"\n",
    "        \n",
    "\n",
    "    else:\n",
    "        #NON-NULL NAME\n",
    "\n",
    "        #NO ROWS FOR YEAR OF THE MATCH\n",
    "        if(len(that_season_that_guy) == 0):\n",
    "\n",
    "            #check season prior and following \n",
    "            prev_season_that_guy = database_name[(database_name['Name'] == candidate_name) & (database_name['Season'] == (int(full_num) + 1))]\n",
    "            next_season_that_guy = database_name[(database_name['Name'] == candidate_name) & (database_name['Season'] == (int(full_num) - 1))]\n",
    "            thatguy_3seasons = pd.concat([that_season_that_guy, prev_season_that_guy, next_season_that_guy], ignore_index=True)\n",
    "            \n",
    "            #NO DATA FOR YEAR BEFORE OR AFTER THE MATCH\n",
    "            if(len(thatguy_3seasons) == 0):\n",
    "                #print(f'{candidate_name} wasn\\'t in the db in {full_num}, {int(full_num) + 1} or {int(full_num) - 1} ')\n",
    "                ###AAA\n",
    "                return_case = \"not in DB any of 3 seasons.\"\n",
    "                lookup_required = True\n",
    "            \n",
    "            #SOME DATA IN THE YEAR BEFORE OR AFTER THE MATCH\n",
    "            else:\n",
    "                ####how do we make sure this guy was really the guy???\n",
    "                ####ZZZZ  \n",
    "                #print(f'{candidate_name} was not in the db in {full_num}, but was in {int(full_num) + 1} or {int(full_num) - 1} ')\n",
    "                lookup_required = True\n",
    "                return_case = \"was in DB in before or after season\"\n",
    "                \n",
    "        \n",
    "        #SOME ROWS FOR YEAR OF THE MATCH\n",
    "        else:\n",
    "            print(f\"{candidate_name} is in dataset for {int(full_num)}\")\n",
    "\n",
    "            #THEIR SALARY OR VALUE THAT SEASON\n",
    "            array_with_nan = that_season_that_guy[f'{money_column_name}'].unique()\n",
    "            numeric_values = pd.to_numeric(array_with_nan, errors='coerce')\n",
    "\n",
    "            #CURRENT SEASON IS NULL DATA\n",
    "            if np.isnan(numeric_values).all():\n",
    "                #print(\"1 season: All-NaN slice encountered\")\n",
    "                #USING THEIR OWN SALARIES FROM SZN BEFORE / AFTER\n",
    "                prev_season_that_guy = database_name[(database_name['Name'] == candidate_name) & (database_name['Season'] == (int(full_num) + 1))]\n",
    "                next_season_that_guy = database_name[(database_name['Name'] == candidate_name) & (database_name['Season'] == (int(full_num) - 1))]\n",
    "                thatguy_3seasons = pd.concat([that_season_that_guy, prev_season_that_guy, next_season_that_guy], ignore_index=True)\n",
    "                \n",
    "                szn_array_with_nan = thatguy_3seasons[f'{money_column_name}'].unique()\n",
    "                numeric_values_3szn = pd.to_numeric(szn_array_with_nan, errors='coerce')\n",
    "\n",
    "                #PREV / FOLLOWING SEASON IS NULL DATA\n",
    "                if np.isnan(numeric_values_3szn).all():\n",
    "\n",
    "                    #print(\"3 seasons: All-NaN slice encountered\")\n",
    "                    ###IMPUTE CASE 2###\n",
    "\n",
    "                    #USING LEAGUE AVG SALARIES\n",
    "                    league = that_season_that_guy.reset_index().at[0, 'League']\n",
    "                    season = that_season_that_guy.reset_index().at[0, 'Season']\n",
    "                    age = that_season_that_guy.reset_index().at[0, 'Age']\n",
    "                    mean_salary_values = pd.to_numeric(database_name[(database_name['League'] == league) & (database_name['Season'] == season) & (database_name['Age'] == age)][f'{money_column_name}'])\n",
    "                    meansalary = mean_salary_values.mean()\n",
    "                    final_salary = meansalary\n",
    "\n",
    "                    #print(f\"For {candidate_name}, imputed salary of {meansalary} using {league}, {season} season and age {age}\")\n",
    "                    imputed_salary = True\n",
    "                    return_case = \"working - imputed\"\n",
    "                    \n",
    "                else:\n",
    "                    #There are salaries in the 3 season dataset\n",
    "                    print(f'for {candidate_name}, {money_column_name} info is not in {int(full_num)} but IS in {int(full_num) + 1} or {int(full_num) - 1}')\n",
    "                    ####ZZZZ\n",
    "                    lookup_required = True\n",
    "                    return_case = \"was in DB in before or after season\"\n",
    "\n",
    "            else:\n",
    "                # Find the maximum value excluding NaNs\n",
    "                max_value_excluding_nan = np.nanmax(numeric_values)\n",
    "                final_salary = max_value_excluding_nan\n",
    "                #print(f\"{money_column_name} in {input_year_test} is {max_value_excluding_nan}\")\n",
    "                return_case = \"working\"\n",
    "\n",
    "    return final_salary, return_case, lookup_required, imputed_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourth_try_name_search(input_search_name, input_nationality):\n",
    "    fourthsearch_name = \"\"\n",
    "    match_type = \"\"\n",
    "    fourthsearch_list = []\n",
    "\n",
    "    search_name = input_search_name\n",
    "    natl_test = input_nationality\n",
    "    #search_name = name_match[2]\n",
    "    result = filter_names_first_initial_lastname(leagues_salary['Name'].unique(), search_name)\n",
    "    list_left = filter_candidates(search_name, result)\n",
    "\n",
    "    natl_list = []\n",
    "    for i in range(0, len(list_left)):\n",
    "        if(natl_test in leagues_salary[leagues_salary['Name'] == list_left[i]]['Nationality'].unique()):\n",
    "            natl_list.append(list_left[i])\n",
    "\n",
    "    if(len(natl_list) == 1):\n",
    "        \n",
    "        #one match remaining. \n",
    "        #RETURN\n",
    "        #print(f'after filtering 4th time found {natl_list[0]}')\n",
    "\n",
    "        fourthsearch_name = natl_list[0]\n",
    "        match_type = 'single'\n",
    "        \n",
    "\n",
    "    elif(len(natl_list) >= 2):\n",
    "        #still not quite matched up\n",
    "        #print something. probably should search this guy as part of \n",
    "        \n",
    "        match_type = 'multiple'\n",
    "        fourthsearch_list = natl_list\n",
    "    else:\n",
    "        match_type = 'none'\n",
    "        #print(f'no match after 4: {search_name}')\n",
    "\n",
    "    if(match_type == 'single'):\n",
    "        return match_type, fourthsearch_name \n",
    "    elif(match_type == 'multiple'):\n",
    "        return match_type, fourthsearch_list \n",
    "    else:\n",
    "        #no match\n",
    "        print(f'no match found: {search_name}')\n",
    "        return match_type, search_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_candidates_using_year(input_list_of_names, input_nationality, input_match_date, using_salaries_boolean):\n",
    "\n",
    "    list_of_names = input_list_of_names\n",
    "    natl_test = input_nationality\n",
    "    input_year_test = input_match_date\n",
    "\n",
    "    if(using_salaries_boolean == True): #salaries_or_values == \"salary\"\n",
    "        database_name = leagues_salary\n",
    "        #money_column_name = \"Inflation-Adjusted Yearly Salary\"\n",
    "    elif(using_salaries_boolean == False): #salaries_or_values == \"value\"\n",
    "        database_name = leagues_value\n",
    "        #money_column_name = \"Market Value\"\n",
    "\n",
    "    yearstr = input_year_test.split(\".\")[2]\n",
    "    full_num = '20' + yearstr\n",
    "    season_num = int(full_num)\n",
    "\n",
    "    result_name_array = list()\n",
    "    for i in range(0, len(list_of_names)):\n",
    "        this_name = list_of_names[i]\n",
    "\n",
    "        checking_name_subset = database_name[(database_name['Name'] == this_name) & (database_name['Nationality'] == natl_test) & (database_name['Season'] == season_num)]\n",
    "\n",
    "        if(len(checking_name_subset) >= 1):\n",
    "            result_name_array.append(this_name)  \n",
    "\n",
    "    return result_name_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Common last name case functions - find correct name using Transfermarkt International History\n",
    "\n",
    "Lovren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_date_format_for_transfermarkt_lookup(input_date):\n",
    "    try:\n",
    "        # Try to parse the input date in different formats\n",
    "        date_formats = [\"%Y-%m-%d\", \"%m.%d.%y\", \"%d-%m-%Y\", \"%d-%b-%y\", \"%d.%m.%y\"]\n",
    "        parsed_date = None\n",
    "        for format_str in date_formats:\n",
    "            try:\n",
    "                parsed_date = datetime.strptime(input_date, format_str)\n",
    "\n",
    "                # If parsing is successful, break out of the loop\n",
    "                break\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "        # If parsing is successful, format the date in MM-DD-YY\n",
    "        if parsed_date:\n",
    "            output_date = parsed_date.strftime(\"%m/%d/%y\")\n",
    "            return output_date\n",
    "        else:\n",
    "            return \"Invalid date format\"\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# Example usage:\n",
    "# case 1 - YYYY-MM-DD\n",
    "#input_date = \"2022-01-22\"\n",
    "# case 2 - DD.MM.YY\n",
    "#input_date = \"08.06.19\"\n",
    "#case 3 - DD-MM-YYYY\n",
    "#input_date = \"29-03-2017\"\n",
    "#case 4 - DD-MMM-YY (text)\n",
    "#input_date = \"05-Jun-21\"\n",
    "#output_date = process_date_format_for_transfermarkt_lookup(input_date)\n",
    "#print(output_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_name(name):\n",
    "    # Remove apostrophes or backticks\n",
    "    name_without_apostrophe = name.replace(\"'\", \"\").replace(\"`\", \"\")\n",
    "\n",
    "    # Replace spaces with dashes and convert to lowercase\n",
    "    transformed_name = '-'.join(name_without_apostrophe.split()).lower()\n",
    "\n",
    "    return transformed_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_transfer_pagesoup(input_url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36'}\n",
    "    pageTree = requests.get (input_url, headers = headers)\n",
    "    pageSoup_club = BeautifulSoup (pageTree.content, 'html.parser')\n",
    "    \n",
    "    return pageSoup_club"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_leading_zeros(date_str):\n",
    "    components = date_str.split('/')\n",
    "    components = [component.zfill(2) if len(component) == 1 else component for component in components]\n",
    "    return '/'.join(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_transfermarkt_pagesoup_player(input_playername, input_nationality_string):\n",
    "    #search = f\"{vet_for_match_season[1]} Transfermarkt\"\n",
    "    search = f\"{input_playername} Transfermarkt {input_nationality_string} National Team\"\n",
    "\n",
    "\n",
    "    url = 'https://www.google.com/search'\n",
    "\n",
    "    headers = {'Accept' : '*/*', 'Accept-Language': 'en-US,en;q=0.5','User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.82',\n",
    "    }\n",
    "    parameters = {'q': search}\n",
    "\n",
    "    content = requests.get(url, headers = headers, params = parameters).text\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    search = soup.find(id = 'search')\n",
    "    first_link = search.find('a')\n",
    "\n",
    "    url_tosplit = (first_link['href'])\n",
    "\n",
    "    player_name_string = url_tosplit.split('/')[3]\n",
    "    player_code = url_tosplit.split('/')[6]\n",
    "\n",
    "    find_national_team_history_URL = f\"https://www.transfermarkt.us/{player_name_string}/nationalmannschaft/spieler/{player_code}\"\n",
    "\n",
    "    if((input_playername.lower() in find_national_team_history_URL) or (input_playername.lower() in find_national_team_history_URL) or (transform_name(input_playername.lower()) in find_national_team_history_URL)):\n",
    "        page_soup_history_pg = grab_transfer_pagesoup(find_national_team_history_URL)\n",
    "    else:\n",
    "        page_soup_history_pg = None\n",
    "    \n",
    "    return page_soup_history_pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_national_team_in_player_history(pagesoup_input, nationality_input):\n",
    "\n",
    "    page_soup_history_pg = pagesoup_input\n",
    "\n",
    "    try:\n",
    "        # FINDING THE NATIONAL TEAMS THEY PLAYED FOR \n",
    "        boxes = page_soup_history_pg.find_all(\"div\", {\"class\": \"box\"})\n",
    "        nationalteamcareer_table = boxes[0].find(\"table\")\n",
    "    except (IndexError, AttributeError):\n",
    "        # Handle errors if the table is not found or an attribute is missing\n",
    "        return False\n",
    "\n",
    "    nationalteamcareer_table = boxes[0].find(\"table\")\n",
    "\n",
    "    rows = nationalteamcareer_table.find_all('tr')\n",
    "\n",
    "    teams_played_for_array = []\n",
    "\n",
    "    for i in range(0, len(rows)): \n",
    "        if(i == 0):\n",
    "            0==0\n",
    "        elif(i % 2 != 0):\n",
    "            0==0\n",
    "        else:\n",
    "            this_row =  rows[i]\n",
    "\n",
    "            nat_team_name = this_row.find_all(\"td\", {\"class\": \"hauptlink no-border-links hide-for-small\"})[0].text.strip().encode().decode(\"utf-8\")\n",
    "            teams_played_for_array.append(nat_team_name)\n",
    "\n",
    "    if nationality_input in teams_played_for_array:\n",
    "        return True \n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_match_date_in_player_history(input_date, pagesoup_input):\n",
    "    date_found = False\n",
    "\n",
    "    # #FINDING MATCH DATE IN THEIR HISTORY\n",
    "    # datestring_lookup = input_date.replace(\".\", \"/\")\n",
    "\n",
    "    # month, day, year = datestring_lookup.split('/')\n",
    "    # # Swap the month and day\n",
    "    # swapped_date = f'{day}/{month}/{year}'\n",
    "\n",
    "    # swapped_date\n",
    "    correctly_formatted_date = process_date_format_for_transfermarkt_lookup(input_date)\n",
    "\n",
    "\n",
    "    #boxes[2].find(\"table\")#.find_all(\"tr\", {\"class\": \"bg_gelb_20\"})\n",
    "\n",
    "    #.find_all(\"div\", {\"class\": \"responsive-table\"})\n",
    "    table_test = pagesoup_input.find_all(\"div\", {\"class\": \"responsive-table\"})[1].find_all(\"tbody\")[0]\n",
    "\n",
    "    for i in range(0, len(table_test.find_all('tr'))):\n",
    "        this_tr_row = table_test.find_all('tr')[i]\n",
    "\n",
    "        data_row = this_tr_row.find_all(\"td\", {\"class\": \"zentriert\"})\n",
    "\n",
    "        if(len(data_row) == 1):\n",
    "            0==0\n",
    "        elif(len(data_row) == 7):\n",
    "            0==0\n",
    "            #wasn't in the squad\n",
    "        elif(len(data_row) == 12):\n",
    "\n",
    "            match_date_row = data_row[1].text.strip()\n",
    "            if(match_date_row == correctly_formatted_date): \n",
    "                date_found = True\n",
    "                return True\n",
    "                #print(i, match_date_row)\n",
    "            elif(add_leading_zeros(match_date_row) == correctly_formatted_date):\n",
    "                date_found = True\n",
    "                return True\n",
    "                #print(i, add_leading_zeros(match_date_row))\n",
    "        else:\n",
    "            print(i, len(data_row))\n",
    "\n",
    "    if(date_found == False):\n",
    "        switched_date = datetime.strptime(correctly_formatted_date, \"%m/%d/%y\").strftime(\"%d/%m/%y\")\n",
    "\n",
    "        for i in range(0, len(table_test.find_all('tr'))):\n",
    "            this_tr_row = table_test.find_all('tr')[i]\n",
    "\n",
    "            data_row = this_tr_row.find_all(\"td\", {\"class\": \"zentriert\"})\n",
    "\n",
    "            if(len(data_row) == 1):\n",
    "                0==0\n",
    "            elif(len(data_row) == 7):\n",
    "                0==0\n",
    "                #wasn't in the squad\n",
    "            elif(len(data_row) == 12):\n",
    "\n",
    "                match_date_row = data_row[1].text.strip()\n",
    "                #print(match_date_row)\n",
    "                if((match_date_row == switched_date)):  #remember this was swapped_date\n",
    "                    date_found = True\n",
    "                    return True\n",
    "                elif(add_leading_zeros(match_date_row) == switched_date): #remember this was swapped_date\n",
    "                    date_found = True\n",
    "                    return True\n",
    "            else:\n",
    "                print(i, len(data_row))\n",
    "\n",
    "\n",
    "    if(date_found == False):\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_transfermarkt_INFO_player(input_playername):\n",
    "    #search = f\"{vet_for_match_season[1]} Transfermarkt\"\n",
    "    search = f\"{input_playername} Transfermarkt\"\n",
    "\n",
    "\n",
    "    url = 'https://www.google.com/search'\n",
    "\n",
    "    headers = {'Accept' : '*/*', 'Accept-Language': 'en-US,en;q=0.5','User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.82',\n",
    "    }\n",
    "    parameters = {'q': search}\n",
    "\n",
    "    content = requests.get(url, headers = headers, params = parameters).text\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    search = soup.find(id = 'search')\n",
    "    first_link = search.find('a')\n",
    "\n",
    "    url_tosplit = (first_link['href'])\n",
    "\n",
    "    player_name_string = url_tosplit.split('/')[3]\n",
    "    player_code = url_tosplit.split('/')[6]\n",
    "\n",
    "    find_value_history_URL = f\"https://www.transfermarkt.us/{player_name_string}/marktwertverlauf/spieler/{player_code}\"\n",
    "\n",
    "    \n",
    "    return find_value_history_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiNameMatchDateLookup(input_list_of_names, input_nationality, input_year_of_match):\n",
    "    list_vetted_for_match_season = input_list_of_names\n",
    "    natl_test = input_nationality\n",
    "    input_year_test = input_year_of_match\n",
    "\n",
    "    playernames_testing = list_vetted_for_match_season\n",
    "    datematch_required = False\n",
    "    result_array_aftertest = []\n",
    "    players_pagesoup_dictionary = {}\n",
    "\n",
    "    for j in range(0, len(playernames_testing)):\n",
    "        currplayer = playernames_testing[j]\n",
    "        transfermarkt_page_soup = find_transfermarkt_pagesoup_player(currplayer, natl_test)\n",
    "\n",
    "        if(find_national_team_in_player_history(transfermarkt_page_soup, natl_test) == True):\n",
    "            result_array_aftertest.append(currplayer)\n",
    "            players_pagesoup_dictionary[currplayer] = transfermarkt_page_soup\n",
    "\n",
    "    if(len(result_array_aftertest) == 1):\n",
    "        #After this leg there's one match \n",
    "        candidate_name_r6 = result_array_aftertest[0]\n",
    "        #money_thisplayer = find_money_info_from_name(candidate_name_r6, natl_test, input_year_test, salary_boolean)\n",
    "        return candidate_name_r6\n",
    "        \n",
    "    elif((len(playernames_testing) == len(result_array_aftertest)) | (len(result_array_aftertest) >= 2)):\n",
    "        #still multiple players w the same name who played for the nat'l team  \n",
    "        datematch_required = True\n",
    "        second_array_test = []\n",
    "\n",
    "        for k in range(0, len(result_array_aftertest)):\n",
    "            currplayer_2 = result_array_aftertest[k]\n",
    "\n",
    "            pagesoup_this_guy = players_pagesoup_dictionary[currplayer_2]\n",
    "\n",
    "            if(find_match_date_in_player_history(input_year_test, pagesoup_this_guy) == True):\n",
    "                second_array_test.append(currplayer_2)\n",
    "\n",
    "    else:\n",
    "        #RESULTS ARRAY AFTER TEST LENGTH IS 0\n",
    "        0==0\n",
    "        #print('error ', result_array_aftertest)\n",
    "        return result_array_aftertest #128201\n",
    "\n",
    "    #if there were two guys w the same last name who both played for the national team \n",
    "    #and we needed to make sure one of them played on that date \n",
    "    if(datematch_required == True):\n",
    "\n",
    "        if(len(second_array_test) == 1):\n",
    "            candidate_name_r6 = second_array_test[0]\n",
    "            #money_thisplayer = find_money_info_from_name(candidate_name_r6, natl_test, input_year_test, salary_boolean)\n",
    "            #stop that\n",
    "            return candidate_name_r6\n",
    "\n",
    "        else:\n",
    "            if(len(second_array_test) >= 2):\n",
    "                0==0\n",
    "                return second_array_test #print(list_vetted_for_match_season, second_array_test)\n",
    "            else:\n",
    "                #LENGTH OF SECOND ARRAY IS ZERO\n",
    "                0==0\n",
    "                return second_array_test #128202\n",
    "\n",
    "            \n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AAA Case functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_name_match_lookup(input_name, input_nationality):\n",
    "    name_match = ['', '', input_name]\n",
    "    natl_test = input_nationality\n",
    "    search = f\"{name_match[2]} {natl_test} national team\"\n",
    "\n",
    "    url = 'https://www.google.com/search'\n",
    "    headers = {\n",
    "        'Accept': '*/*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.82',\n",
    "    }\n",
    "    parameters = {'q': search}\n",
    "\n",
    "    content = requests.get(url, headers=headers, params=parameters).text\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    search_results = soup.find_all('a')\n",
    "\n",
    "    # Collect links from Wikipedia or Transfermarkt\n",
    "    wiki_links = []\n",
    "    transfermarkt_links = []\n",
    "\n",
    "    for result in search_results:\n",
    "        link = result.get('href')\n",
    "        if link:\n",
    "            if 'wikipedia.org' in link:\n",
    "                wiki_links.append(link)\n",
    "            elif 'transfermarkt' in link:\n",
    "                transfermarkt_links.append(link)\n",
    "\n",
    "    # Extract and clean names from Wikipedia links\n",
    "    wiki_names = []\n",
    "    for wiki_link in wiki_links:\n",
    "        match = re.search(r'/wiki/([^/]+)$', wiki_link)\n",
    "        if match:\n",
    "            raw_name = match.group(1).replace('_', ' ')\n",
    "            cleaned_name = re.sub(r'%27', \"'\", urllib.parse.unquote(raw_name))  # Handle %27 as an apostrophe\n",
    "            cleaned_name = ' '.join([part.capitalize() for part in cleaned_name.split()])\n",
    "            wiki_names.append(cleaned_name)\n",
    "\n",
    "    # Extract and clean names from Transfermarkt links\n",
    "    transfermarkt_names = []\n",
    "    for tm_link in transfermarkt_links:\n",
    "        parts = tm_link.split('/')\n",
    "        if len(parts) >= 5 and parts[5] == 'spieler':\n",
    "            raw_name = parts[3].replace('-', ' ')\n",
    "            cleaned_name = ' '.join([part.capitalize() for part in raw_name.split()])\n",
    "            transfermarkt_names.append(cleaned_name)\n",
    "\n",
    "    # Get unique names for each website\n",
    "    unique_wiki_names = set(wiki_names)\n",
    "    unique_transfermarkt_names = set(transfermarkt_names)\n",
    "\n",
    "    # Get unique names for each website\n",
    "    unique_wiki_names = set(wiki_names)\n",
    "    unique_transfermarkt_names = set(transfermarkt_names)\n",
    "\n",
    "    # Combine unique names, accounting for variations\n",
    "    combined_unique_names = set()\n",
    "\n",
    "    for wiki_name in unique_wiki_names:\n",
    "        for tm_name in unique_transfermarkt_names:\n",
    "            # Normalize names by removing spaces, accents, and converting to lowercase\n",
    "            normalized_wiki_name = re.sub(r'[^a-zA-Z0-9]', '', wiki_name.lower())\n",
    "            normalized_tm_name = re.sub(r'[^a-zA-Z0-9]', '', tm_name.lower())\n",
    "\n",
    "            # Check if normalized names match\n",
    "            if normalized_wiki_name == normalized_tm_name:\n",
    "                # Choose the name with apostrophes from Wikipedia list\n",
    "                combined_unique_names.add(wiki_name)\n",
    "\n",
    "    # Print or use the combined unique names\n",
    "    # print(\"Combined Unique Names:\")\n",
    "    # for name in combined_unique_names:\n",
    "    #     print(name)\n",
    "    return combined_unique_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SELENIUM Case Functions - VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ala addin mahdi case functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_match(candidate_name, possible_names):\n",
    "    # Get closest matches using difflib\n",
    "    matches = difflib.get_close_matches(candidate_name, possible_names)\n",
    "\n",
    "    if matches:\n",
    "        # Return the closest match\n",
    "        return matches[0]\n",
    "    else:\n",
    "        # No close match found\n",
    "        return None\n",
    "\n",
    "def findNamesOnPageUsingSoup(input_beautifulsoup):\n",
    "\n",
    "    search_results = input_beautifulsoup.find_all('a')\n",
    "\n",
    "    transfermarkt_links = []\n",
    "\n",
    "    for result in search_results:\n",
    "        link = result.get('href')\n",
    "        if link:\n",
    "            if ('transfermarkt' in link) and ('spieler' in link):\n",
    "                transfermarkt_links.append(link)\n",
    "\n",
    "    #def extract_domain_substring(url):\n",
    "    phrases = ['.us/', '.tr/', '.in/', '.com/', '.uk/', '.de/', '.fr/']\n",
    "\n",
    "    #url = transfermarkt_links[3]\n",
    "\n",
    "\n",
    "    #parsed_url = urlparse(url)\n",
    "    #path = parsed_url.path\n",
    "\n",
    "    names_in_links = []\n",
    "    link_indexes = []\n",
    "    original_links = []\n",
    "\n",
    "    names_dataframe = pd.DataFrame()\n",
    "\n",
    "    rows_counter = 0\n",
    "\n",
    "    for i in range(0, len(transfermarkt_links)):\n",
    "        url = transfermarkt_links[i]\n",
    "\n",
    "        for phrase in phrases:\n",
    "            if phrase in url:\n",
    "\n",
    "                #print(i, url)\n",
    "                #original_links.append(url)\n",
    "                #link_indexes.append(i)\n",
    "                start_index = url.find(phrase) + len(phrase)\n",
    "                end_index = url.find('/', start_index)\n",
    "                if end_index != -1:\n",
    "                    correct_name_from_url = url[start_index:end_index]\n",
    "                    #names_in_links.append(correct_name_from_url)\n",
    "                    #print(i, correct_name_from_url)\n",
    "                    new_row = {'Link': url, 'Name': correct_name_from_url}\n",
    "\n",
    "                    new_row  = pd.DataFrame(new_row, index=[rows_counter])\n",
    "\n",
    "                    # Add the new row to the DataFrame\n",
    "                    names_dataframe = pd.concat([names_dataframe, new_row], ignore_index=True) \n",
    "\n",
    "                    rows_counter += 1\n",
    "                    \n",
    "                else:\n",
    "                    print('2', url[start_index:])\n",
    "\n",
    "    filtered_dataframe_names = names_dataframe[names_dataframe['Name'].apply(lambda x: bool(re.match(\"^[a-zA-Z-]+$\", x)))].reset_index().drop(columns='index', axis=1)\n",
    "\n",
    "    filtered_dataframe_names['Name'] = filtered_dataframe_names['Name'].apply(lambda x: x.replace('-', ' '))\n",
    "\n",
    "    filtered_dataframe_names['Name'] = filtered_dataframe_names['Name'].apply(lambda x: x.title())\n",
    "\n",
    "    for i in range(len(filtered_dataframe_names['Link'])):\n",
    "        link = filtered_dataframe_names.at[i, 'Link']\n",
    "        occurrences = link.count('https:')\n",
    "        \n",
    "        # If there are more than 1 \"https:\", keep only the part starting from the second occurrence\n",
    "        if occurrences > 1:\n",
    "            second_occurrence_index = link.find('https:', link.find('https:') + 1)\n",
    "            filtered_dataframe_names.at[i, 'Link'] = link[second_occurrence_index:]\n",
    "\n",
    "    return filtered_dataframe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Husain Ali Pele case functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_link(url):\n",
    "    try:\n",
    "        options = Options()\n",
    "        options.headless = True\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        \n",
    "        # Open the URL with the headless browser\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Get the final URL after page load\n",
    "        final_url = driver.current_url\n",
    "        \n",
    "        # Compare the original URL with the final URL\n",
    "        return url == final_url\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def extract_information_from_link(link):\n",
    "    # Define regular expressions for extracting information\n",
    "    competition_regex = r'(?:\\.com/|\\.us/|\\.tr/)([^/]+)'\n",
    "    spieler_regex = r'spieler/([^/]+)'\n",
    "\n",
    "    # Extract information using regular expressions\n",
    "    competition_match = pd.Series(link).str.extract(competition_regex, expand=False).iloc[0]\n",
    "    spieler_match = pd.Series(link).str.extract(spieler_regex, expand=False).iloc[0]\n",
    "\n",
    "    return competition_match, spieler_match\n",
    "\n",
    "def remove_row_by_link(dataframe, target_link):\n",
    "    # Check if the target link exists in the 'Link' column\n",
    "    index_to_remove = dataframe[dataframe['Link'] == target_link].index\n",
    "    \n",
    "    # Remove the row if the link is found\n",
    "    if not index_to_remove.empty:\n",
    "        dataframe = dataframe.drop(index_to_remove)\n",
    "        #print(f\"Row with link {target_link} removed.\")\n",
    "    else:\n",
    "        print(f\"Link {target_link} not found in the dataframe.\")\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "def filter_similar_names(target_name, names_array, threshold=0.6):\n",
    "    reversed_target_name = ' '.join(reversed(target_name.split()))\n",
    "    \n",
    "    similar_names = [name for name in names_array if (\n",
    "        Levenshtein.ratio(target_name.lower(), name.lower()) >= threshold\n",
    "        or Levenshtein.ratio(reversed_target_name.lower(), name.lower()) >= threshold\n",
    "    )]\n",
    "    \n",
    "    return similar_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Djeparov case functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_trainer_url_not_player_url(url):\n",
    "    # Check if \"trainer\" is in the URL\n",
    "    condition1 = \"trainer\" in url.lower()\n",
    "\n",
    "    # Check if \"spieler\" is not in the URL\n",
    "    condition2 = \"spieler\" not in url.lower()\n",
    "\n",
    "    # Return True if both conditions are met, otherwise False\n",
    "    return condition1 and condition2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_date_format_for_market_value_table_lookup(input_date):\n",
    "    try:\n",
    "        # Try to parse the input date in different formats\n",
    "        date_formats = [\"%Y-%m-%d\", \"%d.%m.%y\", \"%d-%m-%Y\", \"%d-%b-%y\"]\n",
    "        parsed_date = None\n",
    "        for format_str in date_formats:\n",
    "            try:\n",
    "                parsed_date = datetime.strptime(input_date, format_str)\n",
    "\n",
    "                # If parsing is successful, break out of the loop\n",
    "                break\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "        # If parsing is successful, format the date in YYYY-MM-DD\n",
    "        if parsed_date:\n",
    "            output_date = parsed_date.strftime(\"%Y-%m-%d\")\n",
    "            return output_date\n",
    "        else:\n",
    "            return \"Invalid date format\"\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# Example usage:\n",
    "# case 1 - YYYY-MM-DD\n",
    "#input_date = \"2022-01-22\"\n",
    "\n",
    "# case 2 - DD.MM.YY\n",
    "#input_date = \"03.09.15\"\n",
    "\n",
    "# case 3 - DD-MM-YYYY\n",
    "#input_date = \"03-09-2015\"\n",
    "\n",
    "# case 4 - DD-MMM-YY\n",
    "#input_date = \"03-Sep-15\"\n",
    "\n",
    "#output_date = process_date_format_for_market_value_table_lookup(input_date)\n",
    "#print(output_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mahmoud abu warda cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(entry_date):\n",
    "    has_month = False  # Initialize the boolean variable\n",
    "\n",
    "    try:\n",
    "        # Try to parse as 'Sep 22' format\n",
    "        date_obj = datetime.strptime(entry_date, '%b %y')\n",
    "        year = date_obj.strftime('%y')\n",
    "        month = date_obj.strftime('%b')\n",
    "        has_month = True  # Set to True if parsing succeeds\n",
    "    except ValueError:\n",
    "        try:\n",
    "            # Try to parse as 'MAR 2021' format\n",
    "            date_obj = datetime.strptime(entry_date, '%b %Y')\n",
    "            year = date_obj.strftime('%y')\n",
    "            month = date_obj.strftime('%b')\n",
    "            has_month = True  # Set to True if parsing succeeds\n",
    "        except ValueError:\n",
    "            # If parsing fails, assume it's just a year like '2017'\n",
    "            try:\n",
    "                date_obj = datetime.strptime(entry_date, '%Y')\n",
    "                year = date_obj.strftime('%y')\n",
    "                month = 'Dec'  # Default month if only year is provided\n",
    "            except ValueError:\n",
    "                # If it's neither of the formats, handle accordingly\n",
    "                year = None\n",
    "                month = None\n",
    "\n",
    "    return year, month, has_month\n",
    "\n",
    "#parse_date('Sep 2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSeleniumURL(input_name_for_lookup, input_nationality, input_date_of_match):\n",
    "\n",
    "    search = f'{input_name_for_lookup} {input_nationality} transfermarkt' #replace this\n",
    "\n",
    "    url = 'https://www.google.com/search'\n",
    "\n",
    "    headers = {'Accept': '*/*', 'Accept-Language': 'en-US,en;q=0.5', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.82'}\n",
    "\n",
    "    parameters = {'q': search}\n",
    "\n",
    "    content = requests.get(url, headers=headers, params=parameters).text\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    search = soup.find(id='search')\n",
    "    first_link = search.find('a')\n",
    "\n",
    "    url_tosplit = first_link['href']\n",
    "\n",
    "    # Use urlsplit to extract the domain\n",
    "    parsed_url = urlsplit(url_tosplit)\n",
    "\n",
    "    link_invalid_bool = False\n",
    "\n",
    "    if parsed_url.netloc == 'www.transfermarkt.us' or parsed_url.netloc == \"www.transfermarkt.com\":\n",
    "\n",
    "        #print('1')\n",
    "        ###CHECK IF URL IS VALID HERE###\n",
    "        ###IF NOT DO SOMETHING ELSE\n",
    "        ###IF YES JUST RETURN IT\n",
    "        if is_valid_link(url_tosplit):\n",
    "            print(\"The link is valid.\")\n",
    "            if(is_trainer_url_not_player_url(first_link['href']) == True):\n",
    "            #URL IS A MANAGER URL\n",
    "                print('this is a manager URL', url_tosplit)\n",
    "                \n",
    "                search = f'{input_name_for_lookup} {input_nationality} spieler transfermarkt' #replace this\n",
    "\n",
    "                url = 'https://www.google.com/search'\n",
    "\n",
    "                headers = {'Accept': '*/*', 'Accept-Language': 'en-US,en;q=0.5', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.82'}\n",
    "\n",
    "                parameters = {'q': search}\n",
    "\n",
    "                content = requests.get(url, headers=headers, params=parameters).text\n",
    "                soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "                search = soup.find(id='search')\n",
    "                first_link = search.find('a')\n",
    "\n",
    "                url_tosplit = first_link['href']\n",
    "                if is_valid_link(url_tosplit):\n",
    "                    print(\"The link is valid.\")\n",
    "                    link_invalid_bool = True\n",
    "                    url_is_transfermarkt = True\n",
    "                    name_for_final_url = url_tosplit.split('/')[3]\n",
    "                    code_for_final_url = url_tosplit.split('/')[6]\n",
    "                    url_tosplit = f\"https://www.transfermarkt.us/{name_for_final_url}/marktwertverlauf/spieler/{code_for_final_url}\"\n",
    "                else:\n",
    "                    print(\"The link is not valid.\")\n",
    "                    url_tosplit = \"\"\n",
    "\n",
    "\n",
    "            else:\n",
    "                #URL IS A PLAYER URL\n",
    "                url_is_transfermarkt = True\n",
    "                link_invalid_bool = True\n",
    "\n",
    "                name_for_final_url = url_tosplit.split('/')[3]\n",
    "                code_for_final_url = url_tosplit.split('/')[6]\n",
    "                url_tosplit = f\"https://www.transfermarkt.us/{name_for_final_url}/marktwertverlauf/spieler/{code_for_final_url}\"\n",
    "                #print('yes')\n",
    "        else:\n",
    "            print(\"The link is not valid.\")\n",
    "            url_tosplit = \"\"\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        \n",
    "        url_is_transfermarkt = False #may need to comment this out \n",
    "        print(f'link not from transfermarkt. link is from {parsed_url.netloc}')\n",
    "\n",
    "        names_on_page = findNamesOnPageUsingSoup(soup)\n",
    "        all_names = set(names_on_page['Name'])\n",
    "        candidate_name_links_search = input_name_for_lookup \n",
    "\n",
    "        closest_match_result = closest_match(candidate_name_links_search, all_names)\n",
    "\n",
    "        if(closest_match_result == None):\n",
    "            print('no closest match result for:', input_name_for_lookup, 'this is in getSeleniumURL') \n",
    "            ###IMPUTE CASE 3### SPECIFIC TO VALUES\n",
    "            #DOES THIS JUST RETURN THO? MAYBE YOU TAKE CARE OF IT LATER \n",
    "        else:\n",
    "            #print(closest_match_result)\n",
    "\n",
    "            dataframe_filtered = names_on_page[names_on_page['Name'] == closest_match_result].reset_index()\n",
    "\n",
    "            url_is_transfermarkt = True\n",
    "\n",
    "            correct_link_for_name = dataframe_filtered.at[0, 'Link']\n",
    "            print('2')\n",
    "            if is_valid_link(correct_link_for_name):\n",
    "                print(\"The link is valid.\")\n",
    "                name_for_final_url = correct_link_for_name.split('/')[3]\n",
    "                code_for_final_url = correct_link_for_name.split('/')[6]\n",
    "                url_tosplit = f\"https://www.transfermarkt.us/{name_for_final_url}/marktwertverlauf/spieler/{code_for_final_url}\" \n",
    "                link_invalid_bool = True\n",
    "            else:\n",
    "                print(\"The link is not valid.\")\n",
    "                url_tosplit = \"\"\n",
    "                #link_invalid_bool = True\n",
    "\n",
    "    if(link_invalid_bool == False):\n",
    "        names_on_page = findNamesOnPageUsingSoup(soup)\n",
    "        df = remove_row_by_link(names_on_page, first_link['href']).reset_index().drop(columns='index', axis=1)\n",
    "        df[['Name URL style', 'Spieler']] = df['Link'].apply(extract_information_from_link).apply(pd.Series)\n",
    "\n",
    "        df = df.drop_duplicates(subset=['Name URL style', 'Spieler'], keep='first').reset_index().drop(columns='index', axis=1)\n",
    "\n",
    "        result_array_aftertest = []\n",
    "        players_pagesoup_dictionary = {}\n",
    "\n",
    "        for i in range(0, len(df)):\n",
    "            this_url_name = df.at[i, 'Name URL style']\n",
    "            this_code_name = df.at[i, 'Spieler']\n",
    "            find_national_team_history_URL = f\"https://www.transfermarkt.us/{this_url_name}/nationalmannschaft/spieler/{this_code_name}\"\n",
    "\n",
    "            print(df.at[i, 'Name'], find_national_team_history_URL)\n",
    "\n",
    "            page_soup_history_pg = grab_transfer_pagesoup(find_national_team_history_URL)\n",
    "\n",
    "            if(find_national_team_in_player_history(page_soup_history_pg, input_nationality) == True):\n",
    "                result_array_aftertest.append(df.at[i, 'Name'])\n",
    "                players_pagesoup_dictionary[df.at[i, 'Name']] = page_soup_history_pg\n",
    "\n",
    "\n",
    "        #need to get the page soup from a guy (national page), then you can run it \n",
    "\n",
    "        #get the page soup\n",
    "        filtered_names = filter_similar_names(input_name_for_lookup, result_array_aftertest)\n",
    "        #for each of these remaining names\n",
    "\n",
    "        #loop through the dictionary that you made \n",
    "        #use the names as keys to get their page soup \n",
    "        #do the \"find match date in history\" function with the page_soup_history_pg and the input_year_test\n",
    "\n",
    "        date_match_array_players = []\n",
    "\n",
    "        for i in range(0, len(filtered_names)):\n",
    "            #print(filtered_names[i], players_pagesoup_dictionary[filtered_names[i]])\n",
    "            if(find_match_date_in_player_history(input_date_of_match, players_pagesoup_dictionary[filtered_names[i]]) == True):\n",
    "                print('true', filtered_names[i])\n",
    "                ###when it works###\n",
    "                date_match_array_players.append(filtered_names[i])\n",
    "            else:\n",
    "                print('false', filtered_names[i])\n",
    "                \n",
    "\n",
    "        if(len(date_match_array_players) == 1):\n",
    "            candidate_name_r6 = date_match_array_players[0]\n",
    "            print('match found. you never finished this!!')\n",
    "            #you have to return this guy's profile URL \n",
    "\n",
    "        elif(len(date_match_array_players) >= 2):\n",
    "            print('multiple matches', date_match_array_players)\n",
    "        else:\n",
    "            print('0 matches', filtered_names)\n",
    "            url_tosplit = \"\"\n",
    "            url_is_transfermarkt = False\n",
    "            ###IMPUTE CASE 4 ### SPECIFIC TO VALUES\n",
    "            #wtf is going on here \n",
    "    else:\n",
    "        0==0\n",
    "\n",
    "    return url_tosplit, url_is_transfermarkt\n",
    "\n",
    "def new_seleniumFindMarketValueGraph(input_history_URL, max_retries=5):\n",
    "    #max_retries = 5\n",
    "    for retry in range(max_retries):\n",
    "        service = Service()\n",
    "        options = webdriver.ChromeOptions()\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        try:\n",
    "            ID = \"id\"\n",
    "            NAME = \"name\"\n",
    "            XPATH = \"xpath\"\n",
    "            LINK_TEXT = \"link text\"\n",
    "            PARTIAL_LINK_TEXT = \"partial link text\"\n",
    "            TAG_NAME = \"tag name\"\n",
    "            CLASS_NAME = \"class name\"\n",
    "            CSS_SELECTOR = \"css selector\"\n",
    "\n",
    "            driver.get(input_history_URL)\n",
    "\n",
    "            elements = driver.find_elements(By.XPATH, '/html/body/div/main/div[3]/div[1]/div/tm-market-value-development-graph-extended/div/div')\n",
    "\n",
    "            # Check if elements were found\n",
    "            if elements:\n",
    "                # Access the first element in the list\n",
    "                first_element = elements[0]\n",
    "\n",
    "                # Get the text content of the element\n",
    "                element_text = first_element.text\n",
    "                #print(f\"Text Content: {element_text}\")\n",
    "\n",
    "                # Alternatively, get the outer HTML of the element\n",
    "                element_html = first_element.get_attribute('outerHTML')\n",
    "\n",
    "                if element_html == '<div class=\"content-box-headline\">Loading...</div>':\n",
    "                    print(\"Retrying...\")\n",
    "                    raise ValueError(\"Loading... message detected\")\n",
    "\n",
    "            else:\n",
    "                elements = driver.find_elements(By.XPATH, '/html/body/div/main/div[2]/div[1]/div/tm-market-value-development-graph-extended/div/div')\n",
    "                if elements:\n",
    "                    # Access the first element in the list\n",
    "                    first_element = elements[0]\n",
    "\n",
    "                    # Get the text content of the element\n",
    "                    element_text = first_element.text\n",
    "                    #print(f\"Text Content: {element_text}\")\n",
    "\n",
    "                    # Alternatively, get the outer HTML of the element\n",
    "                    element_html = first_element.get_attribute('outerHTML')\n",
    "\n",
    "                    if element_html == '<div class=\"content-box-headline\">Loading...</div>':\n",
    "                        print(\"Retrying...\")\n",
    "                        raise ValueError(\"Loading... message detected\")\n",
    "\n",
    "                else:\n",
    "                    print('nayem case')\n",
    "                    element_html = \"\"\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            driver.quit()\n",
    "            continue\n",
    "        else:\n",
    "            # Close the WebDriver if successful\n",
    "            driver.quit()\n",
    "            if len(element_html) == 0:\n",
    "                # It didn't return anything\n",
    "                return \"\"\n",
    "            else:\n",
    "                # print(f\"Outer HTML: {element_html}\")\n",
    "                return element_html\n",
    "\n",
    "    # If max retries reached and still not successful, return an empty string\n",
    "    return \"\"\n",
    "\n",
    "def OLDER_PRE_JAN_25_seleniumFindMarketValueGraph (input_history_URL):\n",
    "    service = Service()\n",
    "    options = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    ID = \"id\"\n",
    "    NAME = \"name\"\n",
    "    XPATH = \"xpath\"\n",
    "    LINK_TEXT = \"link text\"\n",
    "    PARTIAL_LINK_TEXT = \"partial link text\"\n",
    "    TAG_NAME = \"tag name\"\n",
    "    CLASS_NAME = \"class name\"\n",
    "    CSS_SELECTOR = \"css selector\"\n",
    "\n",
    "    driver.get(input_history_URL)\n",
    "\n",
    "    elements = driver.find_elements(By.XPATH, '/html/body/div/main/div[3]/div[1]/div/tm-market-value-development-graph-extended/div/div')\n",
    "\n",
    "    # Check if elements were found\n",
    "    if elements:\n",
    "        # Access the first element in the list\n",
    "        first_element = elements[0]\n",
    "\n",
    "        # Get the text content of the element\n",
    "        element_text = first_element.text\n",
    "        print(f\"Text Content: {element_text}\")\n",
    "\n",
    "        # Alternatively, get the outer HTML of the element\n",
    "        element_html = first_element.get_attribute('outerHTML')\n",
    "    else:\n",
    "        #print('2nd attempt')\n",
    "        elements = driver.find_elements(By.XPATH, '/html/body/div/main/div[2]/div[1]/div/tm-market-value-development-graph-extended/div/div')\n",
    "        if elements:\n",
    "            # Access the first element in the list\n",
    "            first_element = elements[0]\n",
    "\n",
    "            # Get the text content of the element\n",
    "            element_text = first_element.text\n",
    "            print(f\"Text Content: {element_text}\")\n",
    "\n",
    "            # Alternatively, get the outer HTML of the element\n",
    "            element_html = first_element.get_attribute('outerHTML')\n",
    "        else:\n",
    "            print('nayem case')\n",
    "            element_html = \"\"\n",
    "\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "    if(len(element_html) == 0):\n",
    "        #it didn't return anything\n",
    "        return \"\"\n",
    "    else:\n",
    "        #print(f\"Outer HTML: {element_html}\")\n",
    "        return element_html\n",
    "\n",
    "def extract_axis_points(outer_html):\n",
    "    soup = BeautifulSoup(outer_html, 'html.parser')\n",
    "\n",
    "    # Extract X axis points\n",
    "    x_axis_points = []\n",
    "    x_axis_elements = soup.select('.axis.svelte-oklk3z text')\n",
    "    \n",
    "    # Add the origin point\n",
    "    x_axis_points.append({'value': '0', 'coordinates': (0, 320)})\n",
    "\n",
    "    # Extract other X axis points\n",
    "    for i, element in enumerate(x_axis_elements):\n",
    "        value = element.get_text().strip()\n",
    "        transform_attribute = element.find_parent('g')['transform']\n",
    "        x_coordinate = float(transform_attribute.split('(')[1].split(',')[0])\n",
    "        y_coordinate = float(transform_attribute.split(',')[1].split(')')[0])\n",
    "        x_axis_points.append({'value': value, 'coordinates': (x_coordinate, y_coordinate)})\n",
    "\n",
    "    # Adjust the last X axis point\n",
    "    #x_axis_points[-1]['coordinates'] = (0, 42.727272727272734)\n",
    "\n",
    "    # Calculate coordinates for other X axis points\n",
    "    # x_interval = 55.4545454545\n",
    "    # for i in range(len(x_axis_points) - 2, 0, -1):\n",
    "    #     x_axis_points[i]['coordinates'] = (0, x_axis_points[i+1]['coordinates'][1] + x_interval)\n",
    "\n",
    "    # Extract Y axis points\n",
    "    y_axis_points = []\n",
    "    y_axis_elements = soup.select('.axis.svelte-3ta12v text')\n",
    "    for element in y_axis_elements:\n",
    "        value = element.get_text().strip()\n",
    "        transform_attribute = element.find_parent('g')['transform']\n",
    "        x_coordinate = float(transform_attribute.split('(')[1].split(',')[0])\n",
    "        y_coordinate = float(transform_attribute.split(',')[1].split(')')[0])\n",
    "        y_axis_points.append({'value': value, 'coordinates': (x_coordinate, y_coordinate)})\n",
    "    \n",
    "    ###Y AXIS THING TO COMMENT OUT###\n",
    "    #y_axis_points.append({'value': 2023, 'coordinates': (701.634619143, 0.0)})\n",
    "\n",
    "    if str(x_axis_points[-1]['coordinates'][1]).startswith('88'):\n",
    "        #print(\"The value starts with '88'\")\n",
    "        for i in range(0, len(x_axis_points)):\n",
    "            differencearray = []\n",
    "            if(i != 0):\n",
    "                currnumber = x_axis_points[i]['coordinates'][1]\n",
    "                prevnumber = x_axis_points[i-1]['coordinates'][1]\n",
    "                difference = prevnumber - currnumber\n",
    "                differencearray.append(difference)\n",
    "                #print(i, currnumber, difference)\n",
    "\n",
    "        coordinate_tickmark_difference = np.average(differencearray)\n",
    "\n",
    "        x_axis_points.append({'value': '0', 'coordinates': (0, x_axis_points[-1]['coordinates'][1] - coordinate_tickmark_difference)})\n",
    "\n",
    "\n",
    "    return x_axis_points, y_axis_points\n",
    "\n",
    "def extract_data_points(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    data_points = soup.select('image')\n",
    "\n",
    "    result = []\n",
    "    for i, data_point in enumerate(data_points, start=1):\n",
    "        x = float(data_point['x'])\n",
    "        y = float(data_point['y'])\n",
    "        result.append(f\"Point {i}: x= {x}, y= {y}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def create_x_points_dataframe(data):\n",
    "    # Create lists to store data\n",
    "    x_list, y_list, value_list = [], [], []\n",
    "\n",
    "    # Iterate through data and fill lists\n",
    "    for entry in data:\n",
    "        x, y = entry['coordinates']\n",
    "        value = entry['value']\n",
    "\n",
    "        # Convert value to numeric format\n",
    "        if value.endswith(('k', 'K')):\n",
    "            value = float(value[:-1]) * 1000\n",
    "        elif value.endswith(('m', 'M')):\n",
    "            value = float(value[:-1]) * 1000000\n",
    "        else:\n",
    "            value = float(value)\n",
    "\n",
    "        # Append to lists\n",
    "        x_list.append(x)\n",
    "        y_list.append(y)\n",
    "        value_list.append(int(value))\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({'X': x_list, 'Y': y_list, 'Value': value_list})\n",
    "\n",
    "    \n",
    "    row_value_difference_array = []\n",
    "    if(df.iloc[len(df) - 1]['Value'] == 0):\n",
    "        #print('yes', df)\n",
    "        for i in range(len(df)):\n",
    "            this_row_value = df.at[i, 'Value']\n",
    "            if(i == len(df) - 1):\n",
    "                0==0\n",
    "            elif(i != 0):\n",
    "                difference_row_to_previous = this_row_value - df.at[i-1, 'Value']\n",
    "                row_value_difference_array.append(difference_row_to_previous)\n",
    "\n",
    "        value_difference_between_rows = np.average(row_value_difference_array)\n",
    "        \n",
    "        df.at[len(df)-1, 'Value'] = df.at[len(df)-2, 'Value'] + value_difference_between_rows\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_y_points_dataframe(data):\n",
    "    # Create lists to store data\n",
    "    x_list, y_list, date_list = [], [], []\n",
    "\n",
    "    # Iterate through data and fill lists\n",
    "    for entry in data:\n",
    "        x, y = entry['coordinates']\n",
    "        x_list.append(x)\n",
    "        y_list.append(y)\n",
    "        # Example usage:\n",
    "        year, month, has_month = parse_date(entry['value'])\n",
    "\n",
    "        if(has_month):\n",
    "            last_day = calendar.monthrange(int(f'20{year}'), list(calendar.month_abbr).index(month))[1]\n",
    "\n",
    "            \n",
    "            date_str_withmonth = f\"{month} {last_day}, {int(year):02d}\"\n",
    "            date_month_included = datetime.strptime(date_str_withmonth, \"%b %d, %y\")\n",
    "            #print(date_month_included)\n",
    "            date_list.append(date_month_included)\n",
    "        else:\n",
    "            print(entry, year)\n",
    "            date_str = f\"Dec 31, {int(year):02d}\"\n",
    "            date = datetime.strptime(date_str, \"%b %d, %y\")\n",
    "            \n",
    "            date_list.append(date)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({'X': x_list, 'Y': y_list, 'Date': date_list})\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "\n",
    "    ###UNFORESEEN 4###\n",
    "    result = calculate_differences_date_X_coordinate(df)\n",
    "\n",
    "    last_row_length = df.at[len(df)-1, 'X'] + result[0]\n",
    "\n",
    "    original_timestamp = df.at[len(df)-1, 'Date']\n",
    "    days_to_add = result[1] \n",
    "\n",
    "    new_timestamp = original_timestamp + pd.to_timedelta(days_to_add, unit='D')\n",
    "\n",
    "    # New row data\n",
    "    new_row = {'X': last_row_length, 'Y': 0, 'Date': new_timestamp}\n",
    "\n",
    "    new_row  = pd.DataFrame(new_row, index=[len(df)])\n",
    "\n",
    "    # Add the new row to the DataFrame\n",
    "    df = pd.concat([df, new_row], ignore_index=True)  \n",
    "\n",
    "    return df\n",
    "\n",
    "def create_data_points_table(data):\n",
    "    # Create lists to store data\n",
    "    point_list, x_list, y_list, market_value_list, date_list = [], [], [], [], []\n",
    "\n",
    "    # Iterate through data and fill lists\n",
    "    for entry in data:\n",
    "        # Extract point number\n",
    "        point_number = int(entry.split(':')[0].split()[-1])\n",
    "\n",
    "        # Extract x and y coordinates\n",
    "        x_coordinate = float(entry.split('x= ')[1].split(',')[0])\n",
    "        y_coordinate = float(entry.split('y= ')[1])\n",
    "\n",
    "        # Append to lists\n",
    "        point_list.append(point_number)\n",
    "        x_list.append(x_coordinate)\n",
    "        y_list.append(y_coordinate)\n",
    "        #market_value_list.append(0)  # Default market value\n",
    "        market_value_list.append(np.float64(0))\n",
    "        date_list.append(datetime(2000, 1, 1))  # Default date\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Point': point_list,\n",
    "        'X': x_list,\n",
    "        'Y': y_list,\n",
    "        'Market Value': market_value_list,\n",
    "        'Date': date_list\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_differences_date_X_coordinate(input_df):\n",
    "    # Convert the 'Date' column to datetime type\n",
    "    input_df['Date'] = pd.to_datetime(input_df['Date'])\n",
    "    \n",
    "    # Calculate the difference in 'X' column\n",
    "    x_differences = input_df['X'].diff().mean()\n",
    "    \n",
    "    # Calculate the difference in 'Date' column\n",
    "    date_differences = (input_df['Date'].diff().mean()).days\n",
    "    \n",
    "    return x_differences, date_differences\n",
    "\n",
    "def estimate_value(input_y_mv, x_points_dataframe):\n",
    "    # Find the two closest Y values in the DataFrame\n",
    "    closest_y_values = x_points_dataframe['Y'].nsmallest(2)\n",
    "    \n",
    "    # Extract the corresponding values and Y coordinates\n",
    "    value1 = x_points_dataframe.loc[x_points_dataframe['Y'] == closest_y_values.iloc[0], 'Value'].values[0]\n",
    "    value2 = x_points_dataframe.loc[x_points_dataframe['Y'] == closest_y_values.iloc[1], 'Value'].values[0]\n",
    "    y1 = closest_y_values.iloc[0]\n",
    "    y2 = closest_y_values.iloc[1]\n",
    "\n",
    "    # Calculate the proportion of input_y_mv between the two closest Y values\n",
    "    proportion = (input_y_mv - y1) / (y2 - y1)\n",
    "\n",
    "    # Interpolate the value based on the proportion\n",
    "    estimated_value = value1 + proportion * (value2 - value1)\n",
    "\n",
    "    return estimated_value\n",
    "\n",
    "def estimate_date(input_x_date, x_dates_dataframe):\n",
    "    # Find the two closest X values in the DataFrame\n",
    "    closest_x_values = x_dates_dataframe['X'].nsmallest(2)\n",
    "    \n",
    "    # Extract the corresponding dates and X coordinates\n",
    "    date1 = pd.to_datetime(x_dates_dataframe.loc[x_dates_dataframe['X'] == closest_x_values.iloc[0], 'Date'].values[0]).date()\n",
    "    date2 = pd.to_datetime(x_dates_dataframe.loc[x_dates_dataframe['X'] == closest_x_values.iloc[1], 'Date'].values[0]).date()\n",
    "    x1 = closest_x_values.iloc[0]\n",
    "    x2 = closest_x_values.iloc[1]\n",
    "\n",
    "    # Calculate the proportion of input_x_date between the two closest X values\n",
    "    proportion = (input_x_date - x1) / (x2 - x1)\n",
    "\n",
    "    # Interpolate the date based on the proportion\n",
    "    estimated_date = date1 + pd.to_timedelta(proportion * (date2 - date1))\n",
    "\n",
    "    return estimated_date\n",
    "\n",
    "def add_date_difference(input_data_points_table, input_x_points_dataframe, input_y_points_dataframe):\n",
    "    x_points_dataframe = input_x_points_dataframe\n",
    "    y_points_dataframe = input_y_points_dataframe\n",
    "    data_points_table = input_data_points_table\n",
    "    for i in range(0, len(data_points_table)):\n",
    "        # Handling MV\n",
    "        this_row_mv_coordinate = data_points_table.at[i, 'Y']\n",
    "        this_row_mv = estimate_value(this_row_mv_coordinate, x_points_dataframe)\n",
    "        data_points_table.at[i, 'Market Value'] = this_row_mv\n",
    "\n",
    "        # Handling Date\n",
    "        this_row_date_coordinate = data_points_table.at[i, 'X']\n",
    "        this_row_date = estimate_date(this_row_date_coordinate, y_points_dataframe)\n",
    "        data_points_table.at[i, 'Date'] = pd.Timestamp(this_row_date)\n",
    "\n",
    "        # Set appropriate data types\n",
    "        data_points_table['Market Value'] = data_points_table['Market Value'].astype(float)\n",
    "        data_points_table['Date'] = pd.to_datetime(data_points_table['Date'], errors='coerce')\n",
    "\n",
    "    return data_points_table\n",
    "\n",
    "def findMarketValueFromTable(df, input_date_str):\n",
    "    # Convert input_date_str to datetime object\n",
    "    #input_date = datetime.strptime(input_date_str, '%d.%m.%y')\n",
    "    input_date = process_date_format_for_market_value_table_lookup(input_date_str)\n",
    "\n",
    "    # Convert the 'Date' column in the DataFrame to datetime objects\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    # Calculate the absolute difference between each date and the input date\n",
    "    #print('input date is: ', input_date, 'first date in table is: ', df.at[0, 'Date'])\n",
    "\n",
    "    input_date = pd.to_datetime(input_date)\n",
    "\n",
    "\n",
    "    df['DateDifference'] = abs(df['Date'] - input_date)\n",
    "\n",
    "    # Find the row with the minimum date difference\n",
    "    closest_row = df.loc[df['DateDifference'].idxmin()]\n",
    "\n",
    "    # Get the market value from the closest row\n",
    "    market_value = closest_row['Market Value']\n",
    "\n",
    "    # Get the number of days from the game \n",
    "    days_from_match = closest_row['DateDifference']\n",
    "\n",
    "    if(days_from_match.days >= 375):\n",
    "        \n",
    "        return 0#, days_from_match\n",
    "    else:\n",
    "        return market_value#, days_from_match\n",
    "#the return statement from this last function is the market value at that time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seleniumLookUpValueWrapperFunction (input_name_player, input_nationality_player, input_match_date_player):\n",
    "\n",
    "    selenium_URL, transfermarkt_boolean = getSeleniumURL(input_name_player, input_nationality_player, input_match_date_player) ####add input_date_match\n",
    "\n",
    "    if(transfermarkt_boolean == True):\n",
    "        print('transfefrmarkt link boolean true')\n",
    "        #go through with it\n",
    "\n",
    "        market_value_graph_html = new_seleniumFindMarketValueGraph(selenium_URL) #was seleniumFindMarketValueGraph(selenium_URL)\n",
    "\n",
    "        if(market_value_graph_html == \"\"):\n",
    "            #then it didn't work\n",
    "            print('wrapper function nayem case working')\n",
    "            return 0\n",
    "\n",
    "        else:\n",
    "            axis_points_x, axis_points_y = extract_axis_points(market_value_graph_html)\n",
    "\n",
    "            data_points_graph = extract_data_points(market_value_graph_html)\n",
    "\n",
    "            #print(axis_points_x)\n",
    "\n",
    "            x_points_df = create_x_points_dataframe(axis_points_x)\n",
    "\n",
    "            #print(axis_points_y)\n",
    "\n",
    "            y_points_df = create_y_points_dataframe(axis_points_y)\n",
    "\n",
    "            data_points_table_df = create_data_points_table(data_points_graph)\n",
    "\n",
    "            data_points_table_df_updated = add_date_difference(data_points_table_df, x_points_df, y_points_df)\n",
    "\n",
    "            #input_match_date_player\n",
    "\n",
    "            #findMarketValueFromTable\n",
    "\n",
    "            result_value_time_of_match = findMarketValueFromTable(data_points_table_df_updated, input_match_date_player)\n",
    "\n",
    "            return result_value_time_of_match\n",
    "    else:\n",
    "        0==0\n",
    "        #print('Link wasnt transfermarkt, and Searching for links did not yield any results.')\n",
    "        print('husain ali pele case working')\n",
    "        return 0 \n",
    "        ###IMPUTE CASE 5### specific to values \n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELENIUM Case - SALARIES VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_earnings_history_sections(html_content):\n",
    "    # Parse the HTML content\n",
    "\n",
    "    # Find all <div> elements with class=\"col s12\"\n",
    "    col_s12_elements = html_content.find_all(\"div\", {\"class\": \"content-block\"})[0].find_all('div', class_='col s12')\n",
    "\n",
    "    # List to store the index of correct sections\n",
    "    correct_section_indices = []\n",
    "\n",
    "    # Iterate through each col s12 element\n",
    "    for index, col_s12 in enumerate(col_s12_elements):\n",
    "        # Check if it contains the expected structure for earnings history\n",
    "        if col_s12.find('h4', class_='section-title') and col_s12.find('table', class_='table-bordered'):\n",
    "            correct_section_indices.append(index)\n",
    "\n",
    "    return correct_section_indices\n",
    "\n",
    "def process_string_currency(input_value_string):\n",
    "    if(input_value_string.startswith('$')):\n",
    "        number_part = int(input_value_string.split('$')[1].replace(',', ''))\n",
    "        number_part_usd = number_part * 0.91\n",
    "        return number_part_usd\n",
    "    \n",
    "    elif(input_value_string.startswith('€')):\n",
    "        number_part_eur = int(input_value_string.split('€')[1].replace(',', ''))\n",
    "        return number_part_eur\n",
    "    \n",
    "    elif(input_value_string.startswith('£')):\n",
    "        number_part = int(input_value_string.split('£')[1].replace(',', ''))\n",
    "        number_part_gbp = number_part * 1.16\n",
    "        return number_part_gbp\n",
    "    \n",
    "    else:\n",
    "        print(input_value_string)\n",
    "\n",
    "def convert_year_to_date(year):\n",
    "    year = year.split('-')[0]\n",
    "    # Assuming July 1 for each year\n",
    "    return datetime(int(year), 7, 1)\n",
    "\n",
    "def multiply_by_factor(row, input_char_currency):\n",
    "    if input_char_currency == '$':\n",
    "        return row * 0.91\n",
    "    elif input_char_currency == '£':\n",
    "        return row * 1.16\n",
    "    else:\n",
    "        return row\n",
    "    \n",
    "def find_closest_date(df, input_date):\n",
    "    df['Year'] = pd.to_datetime(df['Year'], format='%Y-%m-%d')  # Convert 'Year' column to datetime format\n",
    "    input_date = pd.to_datetime(input_date, format='%d.%m.%y')  # Convert input_date to datetime format\n",
    "\n",
    "    closest_date_row = df.iloc[(df['Year'] - input_date).abs().argsort()[0]]\n",
    "    closest_date_yearly_salary = closest_date_row['Yearly Salary']\n",
    "    return closest_date_yearly_salary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareNames(nameROW, nameURL):\n",
    "    if(remove_apostrophes_backticks_single_string(nameROW) == createNameFromUrl(nameURL)):\n",
    "        return True\n",
    "    if(remove_apostrophes_backticks_single_string(nameROW) in createNameFromUrl(nameURL)):\n",
    "        return True\n",
    "    if(createNameFromUrl(nameURL) in remove_apostrophes_backticks_single_string(nameROW)):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def createNameFromUrl(input_url_name):\n",
    "    # Extract name from the URL\n",
    "    match = re.search(r'/player/([\\w-]+)/', input_url_name)\n",
    "    \n",
    "    if match:\n",
    "        raw_name = match.group(1)\n",
    "        # Remove numbers from the name\n",
    "        raw_name = re.sub(r'\\d', '', raw_name)\n",
    "        # Split the name into words\n",
    "        words = re.findall(r'\\b\\w+\\b', raw_name)\n",
    "        \n",
    "        # Capitalize words that are two characters or longer\n",
    "        capitalized_words = []\n",
    "        for i, word in enumerate(words):\n",
    "            if len(word) == 1:\n",
    "                # Handle single characters only if they're the last word in the phrase\n",
    "                if i == len(words) - 1:\n",
    "                    if i > 0:\n",
    "                        capitalized_words[-1] += word\n",
    "                else:\n",
    "                    words[i + 1] = word + words[i + 1]\n",
    "            else:\n",
    "                capitalized_words.append(word.capitalize())\n",
    "        \n",
    "        # Join the words to form the final name\n",
    "        this_name = ' '.join(capitalized_words)\n",
    "        \n",
    "        return this_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capology_selenium_lookup(input_name_to_check, input_nationality, input_match_date):\n",
    "    input_year_test = input_match_date\n",
    "    natl_test = input_nationality\n",
    "\n",
    "    yearstr = input_year_test.split(\".\")[2]\n",
    "    full_num = '20' + yearstr\n",
    "    full_num = int(full_num)\n",
    "    #for Ndiaye case use transfermarkt_filtered_result_AAA\n",
    "    #search = f\"{transfermarkt_filtered_result_AAA} {natl_test} capology {full_num}\"\n",
    "\n",
    "    #Zelarayan case\n",
    "    #search = f\"{name_match[2]} {natl_test} capology {full_num}\"\n",
    "    nameforsearch = input_name_to_check\n",
    "    search = f\"{nameforsearch} {natl_test} capology {full_num}\"\n",
    "\n",
    "    url = 'https://www.google.com/search'\n",
    "\n",
    "    headers = {'Accept' : '*/*', 'Accept-Language': 'en-US,en;q=0.5','User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.82',\n",
    "    }\n",
    "    parameters = {'q': search}\n",
    "\n",
    "    content = requests.get(url, headers = headers, params = parameters).text\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    search = soup.find(id = 'search')\n",
    "    first_link = search.find('a')\n",
    "\n",
    "    url_tosplit = (first_link['href'])\n",
    "\n",
    "    if(compareNames(nameforsearch, url_tosplit)): #name_match[2] #transfermarkt_filtered_result_AAA\n",
    "        correct_URL_Found = True\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url_tosplit)\n",
    "\n",
    "        # Wait for elements to be present (adjust timeout as needed)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        \n",
    "        # Add a retry loop to handle the case where element_text_string is empty\n",
    "        max_retries = 3\n",
    "        current_retry = 0\n",
    "        while current_retry < max_retries:\n",
    "            elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, \"//*\")))\n",
    "            \n",
    "            element_num = 0\n",
    "            element_text_string = []\n",
    "            for element in elements:\n",
    "                \n",
    "                if element_num == 0:\n",
    "                    try:\n",
    "                        text = element.text\n",
    "\n",
    "                        # Check if the element contains both \"$\" signs and \"-\" characters\n",
    "                        if \"$\" in text and \"-\" in text:\n",
    "                            element_num += 1\n",
    "                            element_text_string.append(text)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error retrieving text: {e}\")\n",
    "\n",
    "            driver.quit()\n",
    "\n",
    "            # Check if element_text_string is not empty after retry\n",
    "            if element_text_string:\n",
    "                break\n",
    "            else:\n",
    "                current_retry += 1\n",
    "                print(f\"Retrying to retrieve text. Retry {current_retry}/{max_retries}\")\n",
    "\n",
    "    else:\n",
    "        #print('first URL not right')\n",
    "        # Extract and print text of all links on the first page of Google\n",
    "        # Extract and print text of all links on the first page of Google\n",
    "        # Extract and print text of all links on the first page of Google\n",
    "        all_links = search.find_all('a')\n",
    "        \n",
    "        # Counter to keep track of matches found\n",
    "        match_counter = 0\n",
    "        name_to_find = remove_apostrophes_backticks_single_string(nameforsearch) #transfermarkt_filtered_result_AAA\n",
    "        \n",
    "        for link in all_links:\n",
    "            try:\n",
    "                text = link.find('h3').text  # Assuming the result title is wrapped in an 'h3' tag\n",
    "                url = link.get('href')\n",
    "                #print(f\"Result: {text}\\nURL: {url}\\n\")\n",
    "\n",
    "                # Check if name_to_find or transfermarkt_filtered_result_AAA is present in the link\n",
    "                if name_to_find.lower() in url.lower() or \"transfermarkt_filtered_result_AAA\" in url:\n",
    "                    print(\"Match found!\", name_to_find, url)\n",
    "                    match_counter += 1\n",
    "\n",
    "            except AttributeError:\n",
    "                pass  # Handle cases where 'h3' tag is not found or has no text\n",
    "\n",
    "        # Check if no matches were found\n",
    "        if match_counter == 0:\n",
    "            #print(\"No matches found online.\")\n",
    "            correct_URL_Found = False\n",
    "            #return 0\n",
    "\n",
    "    if(correct_URL_Found == True):\n",
    "\n",
    "        if element_text_string:\n",
    "\n",
    "            split_by_line_elements = element_text_string[0].split('\\n')\n",
    "\n",
    "            correct_elements_indexes_array = []\n",
    "\n",
    "            for i in range(0, len(split_by_line_elements)):\n",
    "                if split_by_line_elements[i].startswith('20') and \"$\" in split_by_line_elements[i]:\n",
    "\n",
    "                    character_to_split_on = '$'\n",
    "\n",
    "                    correct_element = split_by_line_elements[i]\n",
    "                    correct_elements_indexes_array.append(i)\n",
    "\n",
    "                    #print(i, correct_element)\n",
    "                elif split_by_line_elements[i].startswith('20') and \"€\" in split_by_line_elements[i]:\n",
    "\n",
    "                    character_to_split_on = '€'\n",
    "\n",
    "                    correct_element = split_by_line_elements[i]\n",
    "                    correct_elements_indexes_array.append(i)\n",
    "                    \n",
    "                elif split_by_line_elements[i].startswith('20') and \"£\" in split_by_line_elements[i]:\n",
    "\n",
    "                    character_to_split_on = '£'\n",
    "\n",
    "                    correct_element = split_by_line_elements[i]\n",
    "                    correct_elements_indexes_array.append(i)\n",
    "\n",
    "            rows_list = []\n",
    "\n",
    "            for j in range(0, len(correct_elements_indexes_array)):\n",
    "                this_index = correct_elements_indexes_array[j]\n",
    "                correct_element = split_by_line_elements[this_index]\n",
    "\n",
    "                this_row_year = correct_element.split(' ')[0]\n",
    "\n",
    "                correctelement_without_year = correct_element.split(this_row_year)[1][1:]\n",
    "                split_string = correctelement_without_year.split(character_to_split_on) #'$'\n",
    "\n",
    "                try:\n",
    "                    # Extract the numerical values and remove commas\n",
    "                    number_1 = split_string[1].replace(',', '').strip()\n",
    "                    number_2 = split_string[2].replace(',', '').strip()\n",
    "                    number_3 = split_string[3].split('-')[0].replace(',', '').strip()\n",
    "\n",
    "                    # Extract the team\n",
    "                    team = split_string[3].split('-')[1].strip()\n",
    "\n",
    "                    # Append row to the list\n",
    "                    rows_list.append([number_1, number_2, number_3, team, this_row_year])\n",
    "\n",
    "                except IndexError:\n",
    "                    print(j, \"Skipping row due to IndexError.\")\n",
    "\n",
    "            # Create a DataFrame\n",
    "            columns = [\"Weekly Salary\", \"Yearly Salary\", \"Inflation-Adjusted Yearly Salary\", \"Team\", \"Year\"]\n",
    "            df = pd.DataFrame(rows_list, columns=columns)\n",
    "\n",
    "            df['Year'] = df['Year'].apply(convert_year_to_date)\n",
    "            columns_to_convert = ['Weekly Salary', 'Yearly Salary', 'Inflation-Adjusted Yearly Salary']\n",
    "\n",
    "            df[columns_to_convert] = df[columns_to_convert].astype(int)\n",
    "\n",
    "            # Apply the multiplication based on the character_to_split_on\n",
    "            df[columns_to_convert] = df[columns_to_convert].apply(lambda x: multiply_by_factor(x, character_to_split_on))\n",
    "\n",
    "            if(df['Weekly Salary'].sum() == 0):\n",
    "                ###IMPUTE CASE 6### SPECIFIC TO SALARIES\n",
    "                0==0\n",
    "                print('available Capology data shows 0 earnings.')\n",
    "                return 0\n",
    "\n",
    "            else:\n",
    "                #impute_required_salary = False\n",
    "                closest_date_row = find_closest_date(df, input_year_test)\n",
    "\n",
    "            return closest_date_row\n",
    "        \n",
    "        else:\n",
    "            print('Unable to retrieve text after multiple attempts.')\n",
    "            \n",
    "    else: #didn't find correct URL \n",
    "        ###IMPUTE CASE 7### SPECIFIC TO SALARIES\n",
    "        0==0\n",
    "        print('no available Capology data or URL page')\n",
    "        return 0\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
