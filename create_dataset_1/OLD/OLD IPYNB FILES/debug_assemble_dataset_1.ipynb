{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT PACKAGES\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from difflib import get_close_matches\n",
    "import numpy as np\n",
    "import re\n",
    "from transliterate import translit\n",
    "from unidecode import unidecode\n",
    "import Levenshtein\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "import wikipediaapi\n",
    "\n",
    "import wikipedia\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import difflib\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import calendar\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT FUNCTIONS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEBUGGING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium SALARY Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_year_test = input_match_date\n",
    "#natl_test = input_nationality\n",
    "\n",
    "# transfermarkt_filtered_result_AAA = 'Mamadou N\\'diaye'\n",
    "\n",
    "#debug - get URL for selenium lookup capology\n",
    "yearstr = input_year_test.split(\".\")[2]\n",
    "full_num = '20' + yearstr\n",
    "full_num = int(full_num)\n",
    "#for Ndiaye case \n",
    "#search = f\"{transfermarkt_filtered_result_AAA} {natl_test} capology {full_num}\"\n",
    "\n",
    "#Zelarayan case\n",
    "search = f\"{name_match[2]} {natl_test} capology {full_num}\"\n",
    "\n",
    "url = 'https://www.google.com/search'\n",
    "\n",
    "headers = {'Accept' : '*/*', 'Accept-Language': 'en-US,en;q=0.5','User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.82',\n",
    "}\n",
    "parameters = {'q': search}\n",
    "\n",
    "content = requests.get(url, headers = headers, params = parameters).text\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "search_content = soup.find(id = 'search')\n",
    "first_link = search_content.find('a')\n",
    "\n",
    "url_tosplit = (first_link['href'])\n",
    "\n",
    "###NEW from the if statement down \n",
    "if(compareNames(name_match[2], url_tosplit)): #transfermarkt_filtered_result_AAA\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url_tosplit)\n",
    "\n",
    "    # Wait for elements to be present (adjust timeout as needed)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    \n",
    "    # Add a retry loop to handle the case where element_text_string is empty\n",
    "    max_retries = 3\n",
    "    current_retry = 0\n",
    "    while current_retry < max_retries:\n",
    "        elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, \"//*\")))\n",
    "        \n",
    "        element_num = 0\n",
    "        element_text_string = []\n",
    "        for element in elements:\n",
    "            \n",
    "            if element_num == 0:\n",
    "                try:\n",
    "                    text = element.text\n",
    "\n",
    "                    # Check if the element contains both \"$\" signs and \"-\" characters\n",
    "                    if \"$\" in text and \"-\" in text:\n",
    "                        element_num += 1\n",
    "                        element_text_string.append(text)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error retrieving text: {e}\")\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "        # Check if element_text_string is not empty after retry\n",
    "        if element_text_string:\n",
    "            break\n",
    "        else:\n",
    "            current_retry += 1\n",
    "            print(f\"Retrying to retrieve text. Retry {current_retry}/{max_retries}\")\n",
    "\n",
    "else:\n",
    "    #print('first URL not right')\n",
    "    # Extract and print text of all links on the first page of Google\n",
    "    # Extract and print text of all links on the first page of Google\n",
    "    # Extract and print text of all links on the first page of Google\n",
    "    all_links = search.find_all('a')\n",
    "    \n",
    "    # Counter to keep track of matches found\n",
    "    match_counter = 0\n",
    "    name_to_find = remove_apostrophes_backticks_single_string(transfermarkt_filtered_result_AAA)\n",
    "    \n",
    "    for link in all_links:\n",
    "        try:\n",
    "            text = link.find('h3').text  # Assuming the result title is wrapped in an 'h3' tag\n",
    "            url = link.get('href')\n",
    "            #print(f\"Result: {text}\\nURL: {url}\\n\")\n",
    "\n",
    "            # Check if name_to_find or transfermarkt_filtered_result_AAA is present in the link\n",
    "            if name_to_find.lower() in url.lower() or \"transfermarkt_filtered_result_AAA\" in url:\n",
    "                print(\"Match found!\", name_to_find, url)\n",
    "                match_counter += 1\n",
    "\n",
    "        except AttributeError:\n",
    "            pass  # Handle cases where 'h3' tag is not found or has no text\n",
    "\n",
    "    # Check if no matches were found\n",
    "    if match_counter == 0:\n",
    "        print(\"No matches found online.\")\n",
    "        #return 0 \n",
    "        #return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old \"if comparenames is true\" bit\n",
    "\n",
    "if(compareNames(transfermarkt_filtered_result_AAA, url_tosplit)): #name_match[2]\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url_tosplit)\n",
    "\n",
    "    # Wait for elements to be present (adjust timeout as needed)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    elements = wait.until(EC.presence_of_all_elements_located((By.XPATH, \"//*\")))\n",
    "\n",
    "    element_num = 0\n",
    "    element_text_string = []\n",
    "    for element in elements:\n",
    "        \n",
    "        if(element_num == 0):\n",
    "            try:\n",
    "                text = element.text\n",
    "\n",
    "                # Check if the element contains both \"$\" signs and \"-\" characters\n",
    "                if \"$\" in text and \"-\" in text:\n",
    "                    #print('element number ',element_num, text)\n",
    "                    element_num += 1\n",
    "                    element_text_string.append(text)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error retrieving text: {e}\")\n",
    "\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rest of the shit\n",
    "if element_text_string:\n",
    "    split_by_line_elements = element_text_string[0].split('\\n')\n",
    "\n",
    "    correct_elements_indexes_array = []\n",
    "\n",
    "    for i in range(0, len(split_by_line_elements)):\n",
    "        if split_by_line_elements[i].startswith('20') and \"$\" in split_by_line_elements[i]:\n",
    "\n",
    "            character_to_split_on = '$'\n",
    "\n",
    "            correct_element = split_by_line_elements[i]\n",
    "            correct_elements_indexes_array.append(i)\n",
    "\n",
    "            #print(i, correct_element)\n",
    "        elif split_by_line_elements[i].startswith('20') and \"€\" in split_by_line_elements[i]:\n",
    "\n",
    "            character_to_split_on = '€'\n",
    "\n",
    "            correct_element = split_by_line_elements[i]\n",
    "            correct_elements_indexes_array.append(i)\n",
    "            \n",
    "        elif split_by_line_elements[i].startswith('20') and \"£\" in split_by_line_elements[i]:\n",
    "\n",
    "            character_to_split_on = '£'\n",
    "\n",
    "            correct_element = split_by_line_elements[i]\n",
    "            correct_elements_indexes_array.append(i)\n",
    "\n",
    "    rows_list = []\n",
    "\n",
    "    for j in range(0, len(correct_elements_indexes_array)):\n",
    "        this_index = correct_elements_indexes_array[j]\n",
    "        correct_element = split_by_line_elements[this_index]\n",
    "\n",
    "        this_row_year = correct_element.split(' ')[0]\n",
    "\n",
    "        correctelement_without_year = correct_element.split(this_row_year)[1][1:]\n",
    "        split_string = correctelement_without_year.split(character_to_split_on) #'$'\n",
    "\n",
    "        try:\n",
    "            # Extract the numerical values and remove commas\n",
    "            number_1 = split_string[1].replace(',', '').strip()\n",
    "            number_2 = split_string[2].replace(',', '').strip()\n",
    "            number_3 = split_string[3].split('-')[0].replace(',', '').strip()\n",
    "\n",
    "            # Extract the team\n",
    "            team = split_string[3].split('-')[1].strip()\n",
    "\n",
    "            # Append row to the list\n",
    "            rows_list.append([number_1, number_2, number_3, team, this_row_year])\n",
    "\n",
    "        except IndexError:\n",
    "            print(j, \"Skipping row due to IndexError.\")\n",
    "\n",
    "    # Create a DataFrame\n",
    "    columns = [\"Weekly Salary\", \"Yearly Salary\", \"Inflation-Adjusted Yearly Salary\", \"Team\", \"Year\"]\n",
    "    df = pd.DataFrame(rows_list, columns=columns)\n",
    "\n",
    "    df['Year'] = df['Year'].apply(convert_year_to_date)\n",
    "    columns_to_convert = ['Weekly Salary', 'Yearly Salary', 'Inflation-Adjusted Yearly Salary']\n",
    "\n",
    "    df[columns_to_convert] = df[columns_to_convert].astype(int)\n",
    "\n",
    "    # Apply the multiplication based on the character_to_split_on\n",
    "    df[columns_to_convert] = df[columns_to_convert].apply(lambda x: multiply_by_factor(x, character_to_split_on))\n",
    "\n",
    "    if(df['Weekly Salary'].sum() == 0):\n",
    "        impute_required_salary = True\n",
    "\n",
    "    else:\n",
    "\n",
    "\n",
    "        closest_date_row = find_closest_date(df, input_year_test)\n",
    "\n",
    "else:\n",
    "    print('Unable to retrieve text after multiple attempts.')\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium VALUE Lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING WRAPPER FXN\n",
    "#NDIAYE CASE\n",
    "#selenium_lookup_name = transfermarkt_filtered_result_AAA\n",
    "\n",
    "#ZELARAYAN CASE\n",
    "# selenium_lookup_name = name_match[2]\n",
    "\n",
    "#YEMEN case \n",
    "name_match = [0, 0, 'Ala Addin Mahdi']\n",
    "natl_test = 'Yemen'\n",
    "\n",
    "selenium_lookup_name = name_match[2]\n",
    "\n",
    "selenium_URL, transfermarkt_boolean = getSeleniumURL(selenium_lookup_name, natl_test, input_year_test) ####add input_date_match\n",
    "#boolean tells you if the first link found was from transfermarkt\n",
    "\n",
    "if(transfermarkt_boolean == True):\n",
    "    market_value_graph_html = new_seleniumFindMarketValueGraph(selenium_URL)\n",
    "\n",
    "    if(market_value_graph_html == \"\"):\n",
    "        #then it didn't work\n",
    "        0==0\n",
    "    else:\n",
    "        axis_points_x, axis_points_y = extract_axis_points(market_value_graph_html)\n",
    "\n",
    "        data_points_graph = extract_data_points(market_value_graph_html)\n",
    "\n",
    "        x_points_df = create_x_points_dataframe(axis_points_x)\n",
    "\n",
    "        y_points_df = create_y_points_dataframe(axis_points_y)\n",
    "\n",
    "        data_points_table_df = create_data_points_table(data_points_graph)\n",
    "\n",
    "        data_points_table_df_updated = add_date_difference(data_points_table_df, x_points_df, y_points_df)\n",
    "\n",
    "        #input_match_date_player\n",
    "\n",
    "        #findMarketValueFromTable\n",
    "\n",
    "        result_value_time_of_match = findMarketValueFromTable(data_points_table_df_updated, input_year_test)\n",
    "\n",
    "        print(result_value_time_of_match)\n",
    "else:\n",
    "    0==0\n",
    "    print('yemen case is working')\n",
    "    #boolean was false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Piece by piece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get selenium URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug getSeleniumURL\n",
    "\n",
    "#for Ndiaye case use transfermarkt_filtered_result_AAA\n",
    "#NDiaye case\n",
    "#search = f\"{transfermarkt_filtered_result_AAA} {natl_test} transfermarkt\"\n",
    "\n",
    "\n",
    "#Zelarayan case\n",
    "#search = f\"{name_match[2]} {natl_test} transfermarkt\"\n",
    "#search = f\"Zelarayan armenia transfermarkt\"\n",
    "\n",
    "#YEMEN CASE \n",
    "#search = f'{name_match[2]} {natl_test} transfermarkt'\n",
    "\n",
    "###NEW### UNFORESEEN ERROR 3\n",
    "#search = f'Mudir Al Radaei transfermarkt'\n",
    "\n",
    "#SAUDI CASE\n",
    "#search = f'{name_match[2]} {natl_test} transfermarkt'\n",
    "#search = 'Neymar brazil transfermarkt'\n",
    "#search = 'Martin Skrtel transfermarkt'\n",
    "\n",
    "\n",
    "#HUSAIN ALI PELE CASE   \n",
    "# name_match = [0, 0, 'Husain Ali Pele'] \n",
    "# natl_test = 'Bahrain'\n",
    "# input_year_test = '08.10.15'\n",
    "\n",
    "#HESHAM NAYEM CASE\n",
    "# name_match = [0, 0, \"Hesham Nayem\"]\n",
    "# natl_test = 'Bahrain'\n",
    "# input_year_test = \"24.03.16\"\n",
    "\n",
    "#DJEPAROV CASE\n",
    "# name_match = [0, 0, \"S. Djeparov\"]\n",
    "# natl_test = 'Uzbekistan'\n",
    "# input_year_test = \"03.09.2015\"\n",
    "\n",
    "#replace name_match[2] with input_name_for_lookup\n",
    "#replace input_year_test with input_date_of_match\n",
    "#replace natl_test with input_nationality\n",
    "\n",
    "#MOHAMED ABU WARDA CASE\n",
    "# name_match = [0, 0, \"Mahmoud Abu Warda\"]\n",
    "# natl_test = 'Palestine'\n",
    "# input_year_test = \"14-11-2019\"\n",
    "# search = f'{name_match[2]} {natl_test} transfermarkt' #replace this\n",
    "#print(f'google search = {search}')\n",
    "\n",
    "#MUDIR AL RADAEI CASE \n",
    "search = f'{name_match[2]} {natl_test} transfermarkt' #replace this\n",
    "\n",
    "\n",
    "url = 'https://www.google.com/search'\n",
    "\n",
    "headers = {'Accept': '*/*', 'Accept-Language': 'en-US,en;q=0.5', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.82'}\n",
    "\n",
    "parameters = {'q': search}\n",
    "\n",
    "content = requests.get(url, headers=headers, params=parameters).text\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "search = soup.find(id='search')\n",
    "first_link = search.find('a')\n",
    "\n",
    "url_tosplit = first_link['href']\n",
    "\n",
    "# Use urlsplit to extract the domain\n",
    "parsed_url = urlsplit(url_tosplit)\n",
    "\n",
    "link_invalid_bool = False\n",
    "\n",
    "if parsed_url.netloc == 'www.transfermarkt.us' or parsed_url.netloc == \"www.transfermarkt.com\":\n",
    "\n",
    "    #print('1')\n",
    "    ###CHECK IF URL IS VALID HERE###\n",
    "    ###IF NOT DO SOMETHING ELSE\n",
    "    ###IF YES JUST RETURN IT\n",
    "    if is_valid_link(url_tosplit):\n",
    "        print(\"The link is valid.\")\n",
    "        #ANOTHER NAME CHECK HERE\n",
    "\n",
    "        if(is_trainer_url_not_player_url(first_link['href']) == True):\n",
    "            #URL IS A MANAGER URL\n",
    "            print('this is a manager URL', url_tosplit)\n",
    "            \n",
    "            search = f'{name_match[2]} {natl_test} spieler transfermarkt' #replace this\n",
    "\n",
    "            url = 'https://www.google.com/search'\n",
    "\n",
    "            headers = {'Accept': '*/*', 'Accept-Language': 'en-US,en;q=0.5', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.82'}\n",
    "\n",
    "            parameters = {'q': search}\n",
    "\n",
    "            content = requests.get(url, headers=headers, params=parameters).text\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            search = soup.find(id='search')\n",
    "            first_link = search.find('a')\n",
    "\n",
    "            url_tosplit = first_link['href']\n",
    "            if is_valid_link(url_tosplit):\n",
    "                print(\"The link is valid.\")\n",
    "                link_invalid_bool = True\n",
    "                url_is_transfermarkt = True\n",
    "                name_for_final_url = url_tosplit.split('/')[3]\n",
    "                code_for_final_url = url_tosplit.split('/')[6]\n",
    "                url_tosplit = f\"https://www.transfermarkt.us/{name_for_final_url}/marktwertverlauf/spieler/{code_for_final_url}\"\n",
    "            else:\n",
    "                print(\"The link is not valid.\")\n",
    "                url_tosplit = \"\"\n",
    "\n",
    "\n",
    "        else:\n",
    "            #URL IS A PLAYER URL\n",
    "            url_is_transfermarkt = True\n",
    "            link_invalid_bool = True\n",
    "\n",
    "            name_for_final_url = url_tosplit.split('/')[3]\n",
    "            code_for_final_url = url_tosplit.split('/')[6]\n",
    "            url_tosplit = f\"https://www.transfermarkt.us/{name_for_final_url}/marktwertverlauf/spieler/{code_for_final_url}\"\n",
    "        #print('yes')\n",
    "    else:\n",
    "        print(\"The link is not valid.\")\n",
    "        url_tosplit = \"\"\n",
    "    \n",
    "else:\n",
    "    \n",
    "    url_is_transfermarkt = False #may need to comment this out \n",
    "\n",
    "    names_on_page = findNamesOnPageUsingSoup(soup)\n",
    "    all_names = set(names_on_page['Name'])\n",
    "    candidate_name_links_search = name_match[2] #replace this\n",
    "\n",
    "    closest_match_result = closest_match(candidate_name_links_search, all_names)\n",
    "\n",
    "    if(closest_match_result == None):\n",
    "        print('no closest match result for:', name_match[2]) #replace this\n",
    "        ###IMPUTE HERE VALUES\n",
    "        #DOES THIS JUST RETURN THO? MAYBE YOU TAKE CARE OF IT LATER \n",
    "    else:\n",
    "        #print(closest_match_result)\n",
    "        print(f'link not from transfermarkt. link is from {parsed_url.netloc}')\n",
    "\n",
    "        dataframe_filtered = names_on_page[names_on_page['Name'] == closest_match_result].reset_index()\n",
    "\n",
    "        url_is_transfermarkt = True\n",
    "\n",
    "        correct_link_for_name = dataframe_filtered.at[0, 'Link']\n",
    "        print('2')\n",
    "        if is_valid_link(correct_link_for_name):\n",
    "            print(\"The link is valid.\")\n",
    "            name_for_final_url = correct_link_for_name.split('/')[3]\n",
    "            code_for_final_url = correct_link_for_name.split('/')[6]\n",
    "            url_tosplit = f\"https://www.transfermarkt.us/{name_for_final_url}/marktwertverlauf/spieler/{code_for_final_url}\" \n",
    "            link_invalid_bool = True\n",
    "        else:\n",
    "            print(\"The link is not valid.\")\n",
    "            url_tosplit = \"\"\n",
    "            #link_invalid_bool = True\n",
    "\n",
    "if(link_invalid_bool == False):\n",
    "    names_on_page = findNamesOnPageUsingSoup(soup)\n",
    "    df = remove_row_by_link(names_on_page, first_link['href']).reset_index().drop(columns='index', axis=1)\n",
    "    df[['Name URL style', 'Spieler']] = df['Link'].apply(extract_information_from_link).apply(pd.Series)\n",
    "\n",
    "    df = df.drop_duplicates(subset=['Name URL style', 'Spieler'], keep='first').reset_index().drop(columns='index', axis=1)\n",
    "\n",
    "    result_array_aftertest = []\n",
    "    players_pagesoup_dictionary = {}\n",
    "\n",
    "    for i in range(0, len(df)):\n",
    "        this_url_name = df.at[i, 'Name URL style']\n",
    "        this_code_name = df.at[i, 'Spieler']\n",
    "        find_national_team_history_URL = f\"https://www.transfermarkt.us/{this_url_name}/nationalmannschaft/spieler/{this_code_name}\"\n",
    "\n",
    "        print(df.at[i, 'Name'], find_national_team_history_URL)\n",
    "\n",
    "        page_soup_history_pg = grab_transfer_pagesoup(find_national_team_history_URL)\n",
    "\n",
    "        if(find_national_team_in_player_history(page_soup_history_pg, natl_test) == True):\n",
    "            result_array_aftertest.append(df.at[i, 'Name'])\n",
    "            players_pagesoup_dictionary[df.at[i, 'Name']] = page_soup_history_pg\n",
    "\n",
    "\n",
    "    #need to get the page soup from a guy (national page), then you can run it \n",
    "\n",
    "    #get the page soup\n",
    "    filtered_names = filter_similar_names(name_match[2], result_array_aftertest)\n",
    "    #for each of these remaining names\n",
    "\n",
    "    #loop through the dictionary that you made \n",
    "    #use the names as keys to get their page soup \n",
    "    #do the \"find match date in history\" function with the page_soup_history_pg and the input_year_test\n",
    "\n",
    "    date_match_array_players = []\n",
    "\n",
    "    for i in range(0, len(filtered_names)):\n",
    "        #print(filtered_names[i], players_pagesoup_dictionary[filtered_names[i]])\n",
    "        if(find_match_date_in_player_history(input_year_test, players_pagesoup_dictionary[filtered_names[i]]) == True):\n",
    "            print('true', filtered_names[i])\n",
    "            ###when it works###\n",
    "            date_match_array_players.append(filtered_names[i])\n",
    "        else:\n",
    "            print('match date didn\\'t match for', filtered_names[i])\n",
    "            \n",
    "\n",
    "    if(len(date_match_array_players) == 1):\n",
    "        candidate_name_r6 = date_match_array_players[0]\n",
    "        print('match found. you never accounted for this!!')\n",
    "        #you have to return this guy's profile URL \n",
    "\n",
    "    elif(len(date_match_array_players) >= 2):\n",
    "        print('multiple matches', date_match_array_players)\n",
    "    else:\n",
    "        print('0 matches', filtered_names)\n",
    "        url_tosplit = \"\"\n",
    "        #wtf is going on here \n",
    "        ###IMPUTE HERE - VALUES###\n",
    "else:\n",
    "    0==0\n",
    "#return url_tosplit, url_is_transfermarkt\n",
    "        \n",
    "\n",
    "#Zelarayan case\n",
    "#url_tosplit = 'https://www.transfermarkt.us/lucas-zelarayan/marktwertverlauf/spieler/230030'\n",
    "\n",
    "#NDiaye case\n",
    "#url_tosplit = \"https://www.transfermarkt.us/mamoutou-ndiaye/marktwertverlauf/spieler/94298\"\n",
    "\n",
    "#names_on_page\n",
    "url_tosplit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find names on page, using soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEBUG findNamesOnPageUsingSoup\n",
    "\n",
    "search_results = soup.find_all('a')\n",
    "\n",
    "transfermarkt_links = []\n",
    "\n",
    "for result in search_results:\n",
    "    link = result.get('href')\n",
    "    if link:\n",
    "        if ('transfermarkt' in link) and ('spieler' in link):\n",
    "            transfermarkt_links.append(link)\n",
    "\n",
    "#def extract_domain_substring(url):\n",
    "phrases = ['.us/', '.tr/', '.in/', '.com/', '.uk/', '.de/', '.fr/']\n",
    "\n",
    "#url = transfermarkt_links[3]\n",
    "\n",
    "\n",
    "#parsed_url = urlparse(url)\n",
    "#path = parsed_url.path\n",
    "\n",
    "names_in_links = []\n",
    "link_indexes = []\n",
    "original_links = []\n",
    "\n",
    "names_dataframe = pd.DataFrame()\n",
    "\n",
    "rows_counter = 0\n",
    "\n",
    "for i in range(0, len(transfermarkt_links)):\n",
    "    url = transfermarkt_links[i]\n",
    "\n",
    "    for phrase in phrases:\n",
    "        if phrase in url:\n",
    "\n",
    "            #print(i, url)\n",
    "            #original_links.append(url)\n",
    "            #link_indexes.append(i)\n",
    "            start_index = url.find(phrase) + len(phrase)\n",
    "            end_index = url.find('/', start_index)\n",
    "            if end_index != -1:\n",
    "                correct_name_from_url = url[start_index:end_index]\n",
    "                #names_in_links.append(correct_name_from_url)\n",
    "                #print(i, correct_name_from_url)\n",
    "                new_row = {'Link': url, 'Name': correct_name_from_url}\n",
    "\n",
    "                new_row  = pd.DataFrame(new_row, index=[rows_counter])\n",
    "\n",
    "                # Add the new row to the DataFrame\n",
    "                names_dataframe = pd.concat([names_dataframe, new_row], ignore_index=True) \n",
    "\n",
    "                rows_counter += 1\n",
    "                \n",
    "            else:\n",
    "                print('2', url[start_index:])\n",
    "\n",
    "filtered_dataframe_names = names_dataframe[names_dataframe['Name'].apply(lambda x: bool(re.match(\"^[a-zA-Z-]+$\", x)))].reset_index().drop(columns='index', axis=1)\n",
    "\n",
    "filtered_dataframe_names['Name'] = filtered_dataframe_names['Name'].apply(lambda x: x.replace('-', ' '))\n",
    "\n",
    "filtered_dataframe_names['Name'] = filtered_dataframe_names['Name'].apply(lambda x: x.title())\n",
    "\n",
    "for i in range(len(filtered_dataframe_names['Link'])):\n",
    "    link = filtered_dataframe_names.at[i, 'Link']\n",
    "    occurrences = link.count('https:')\n",
    "    \n",
    "    # If there are more than 1 \"https:\", keep only the part starting from the second occurrence\n",
    "    if occurrences > 1:\n",
    "        second_occurrence_index = link.find('https:', link.find('https:') + 1)\n",
    "        filtered_dataframe_names.at[i, 'Link'] = link[second_occurrence_index:]\n",
    "\n",
    "filtered_dataframe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Market value graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug new_seleniumFindMarketValueGraph\n",
    "\n",
    "max_retries = 5\n",
    "for retry in range(max_retries):\n",
    "    service = Service()\n",
    "    options = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    try:\n",
    "        ID = \"id\"\n",
    "        NAME = \"name\"\n",
    "        XPATH = \"xpath\"\n",
    "        LINK_TEXT = \"link text\"\n",
    "        PARTIAL_LINK_TEXT = \"partial link text\"\n",
    "        TAG_NAME = \"tag name\"\n",
    "        CLASS_NAME = \"class name\"\n",
    "        CSS_SELECTOR = \"css selector\"\n",
    "\n",
    "        driver.get(url_tosplit)\n",
    "\n",
    "        elements = driver.find_elements(By.XPATH, '/html/body/div/main/div[3]/div[1]/div/tm-market-value-development-graph-extended/div/div')\n",
    "\n",
    "        # Check if elements were found\n",
    "        if elements:\n",
    "            # Access the first element in the list\n",
    "            first_element = elements[0]\n",
    "\n",
    "            # Get the text content of the element\n",
    "            element_text = first_element.text\n",
    "            #print(f\"Text Content: {element_text}\")\n",
    "\n",
    "            # Alternatively, get the outer HTML of the element\n",
    "            element_html = first_element.get_attribute('outerHTML')\n",
    "\n",
    "            if element_html == '<div class=\"content-box-headline\">Loading...</div>':\n",
    "                print(\"Retrying...\")\n",
    "                raise ValueError(\"Loading... message detected\")\n",
    "            else:\n",
    "                driver.quit()\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            elements = driver.find_elements(By.XPATH, '/html/body/div/main/div[2]/div[1]/div/tm-market-value-development-graph-extended/div/div')\n",
    "            if elements:\n",
    "                # Access the first element in the list\n",
    "                first_element = elements[0]\n",
    "\n",
    "                # Get the text content of the element\n",
    "                element_text = first_element.text\n",
    "                #print(f\"Text Content: {element_text}\")\n",
    "\n",
    "                # Alternatively, get the outer HTML of the element\n",
    "                element_html = first_element.get_attribute('outerHTML')\n",
    "\n",
    "                if element_html == '<div class=\"content-box-headline\">Loading...</div>':\n",
    "                    print(\"Retrying...\")\n",
    "                    raise ValueError(\"Loading... message detected\")\n",
    "                else:\n",
    "                    driver.quit()\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                print('nayem case')\n",
    "                element_html = \"\"\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        driver.quit()\n",
    "        continue\n",
    "    else:\n",
    "        # Close the WebDriver if successful\n",
    "        driver.quit()\n",
    "        if len(element_html) == 0:\n",
    "            # It didn't return anything\n",
    "            print (\"\")\n",
    "        else:\n",
    "            # print(f\"Outer HTML: {element_html}\")\n",
    "            print( element_html)\n",
    "\n",
    "# If max retries reached and still not successful, return an empty string\n",
    "print (\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Axis Points (from graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug extract_axis_points  \n",
    "outer_html = element_html #found the -1 error with \"<div class=\\\"content-box-headline\\\">Loading...</div>\"\n",
    "\n",
    "soup = BeautifulSoup(outer_html, 'html.parser')\n",
    "\n",
    "# Extract X axis points\n",
    "x_axis_points = []\n",
    "x_axis_elements = soup.select('.axis.svelte-oklk3z text')\n",
    "\n",
    "# Add the origin point\n",
    "x_axis_points.append({'value': '0', 'coordinates': (0, 320)})\n",
    "\n",
    "# Extract other X axis points\n",
    "for i, element in enumerate(x_axis_elements):\n",
    "    value = element.get_text().strip()\n",
    "    transform_attribute = element.find_parent('g')['transform']\n",
    "    x_coordinate = float(transform_attribute.split('(')[1].split(',')[0])\n",
    "    y_coordinate = float(transform_attribute.split(',')[1].split(')')[0])\n",
    "    x_axis_points.append({'value': value, 'coordinates': (x_coordinate, y_coordinate)})\n",
    "\n",
    "# Adjust the last X axis point\n",
    "#CHECK BEFORE U DO THIS\n",
    "#x_axis_points[-1]['coordinates'] = (0, 42.727272727272734)\n",
    "\n",
    "# Calculate coordinates for other X axis points\n",
    "# x_interval = 55.4545454545\n",
    "# for i in range(len(x_axis_points) - 2, 0, -1):\n",
    "#     x_axis_points[i]['coordinates'] = (0, x_axis_points[i+1]['coordinates'][1] + x_interval)\n",
    "\n",
    "# Extract Y axis points\n",
    "y_axis_points = []\n",
    "y_axis_elements = soup.select('.axis.svelte-3ta12v text')\n",
    "for element in y_axis_elements:\n",
    "    value = element.get_text().strip()\n",
    "    transform_attribute = element.find_parent('g')['transform']\n",
    "    x_coordinate = float(transform_attribute.split('(')[1].split(',')[0])\n",
    "    y_coordinate = float(transform_attribute.split(',')[1].split(')')[0])\n",
    "    y_axis_points.append({'value': value, 'coordinates': (x_coordinate, y_coordinate)})\n",
    "\n",
    "###Y AXIS THING TO COMMENT OUT###\n",
    "#y_axis_points.append({'value': 2023, 'coordinates': (701.634619143, 0.0)})\n",
    "\n",
    "\n",
    "if str(x_axis_points[-1]['coordinates'][1]).startswith('88'):\n",
    "    #print(\"The value starts with '88'\")\n",
    "    for i in range(0, len(x_axis_points)):\n",
    "        differencearray = []\n",
    "        if(i != 0):\n",
    "            currnumber = x_axis_points[i]['coordinates'][1]\n",
    "            prevnumber = x_axis_points[i-1]['coordinates'][1]\n",
    "            difference = prevnumber - currnumber\n",
    "            differencearray.append(difference)\n",
    "            #print(i, currnumber, difference)\n",
    "\n",
    "    coordinate_tickmark_difference = np.average(differencearray)\n",
    "\n",
    "    x_axis_points.append({'value': '0', 'coordinates': (0, x_axis_points[-1]['coordinates'][1] - coordinate_tickmark_difference)})\n",
    "else:\n",
    "    print('no 88 problem')\n",
    "x_axis_points #y_axis_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST extract_axis_points\n",
    "\n",
    "# Example usage:\n",
    "outer_html = element_html #found the -1 error with \"<div class=\\\"content-box-headline\\\">Loading...</div>\"\n",
    "x_points, y_points = extract_axis_points(outer_html)\n",
    "\n",
    "print(\"X axis points:\")\n",
    "for point in x_points:\n",
    "    print(f\"{point['value']}: {point['coordinates']}\")\n",
    "\n",
    "print(\"\\nY axis points:\")\n",
    "for point in y_points:\n",
    "    print(f\"{point['value']}: {point['coordinates']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract DATA Points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST extract_data_points\n",
    "\n",
    "# Your HTML string\n",
    "html_string = outer_html #element_html IS FOR when u don't run outer_html = element_html #WAS outer_html BEFORE\n",
    "\n",
    "# Call the function\n",
    "points = extract_data_points(html_string)\n",
    "\n",
    "# Print the result\n",
    "for point in points:\n",
    "    print(point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create X Points Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST create_x_points_dataframe\n",
    "\n",
    "# Your data\n",
    "data = x_points #x_axis_points for debugging  #x_points otherwise\n",
    "\n",
    "# Call the function with your data\n",
    "x_points_dataframe = create_x_points_dataframe(data)\n",
    "if(x_points_dataframe.iloc[len(x_points_dataframe) - 1]['Value'] == 0):\n",
    "    row_value_difference_array = []\n",
    "    for i in range(len(x_points_dataframe)):\n",
    "        this_row_value = x_points_dataframe.at[i, 'Value']\n",
    "        if(i == len(x_points_dataframe) - 1):\n",
    "            0==0\n",
    "        elif(i != 0):\n",
    "            difference_row_to_previous = this_row_value - x_points_dataframe.at[i-1, 'Value']\n",
    "            row_value_difference_array.append(difference_row_to_previous)\n",
    "\n",
    "    value_difference_between_rows = np.average(row_value_difference_array)\n",
    "\n",
    "    x_points_dataframe.at[len(x_points_dataframe)-1, 'Value'] = x_points_dataframe.at[len(x_points_dataframe)-2, 'Value'] + value_difference_between_rows\n",
    "\n",
    "x_points_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Y points dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug create_y_points_dataframe \n",
    "\n",
    "\n",
    "data = y_points #use y_points_data during a debug ???\n",
    "\n",
    "x_list, y_list, date_list = [], [], []\n",
    "\n",
    "# Iterate through data and fill lists\n",
    "for entry in data:\n",
    "    print(entry)\n",
    "    x, y = entry['coordinates']\n",
    "    x_list.append(x)\n",
    "    y_list.append(y)\n",
    "    # Example usage:\n",
    "    year, month, has_month = parse_date(entry['value'])\n",
    "\n",
    "    if(has_month):\n",
    "        last_day = calendar.monthrange(int(f'20{year}'), list(calendar.month_abbr).index(month))[1]\n",
    "\n",
    "        \n",
    "        date_str_withmonth = f\"{month} {last_day}, {int(year):02d}\"\n",
    "        date_month_included = datetime.strptime(date_str_withmonth, \"%b %d, %y\")\n",
    "        #print(date_month_included)\n",
    "        date_list.append(date_month_included)\n",
    "    else:\n",
    "\n",
    "        \n",
    "        date_str = f\"Dec 31, {int(year):02d}\"\n",
    "        date = datetime.strptime(date_str, \"%b %d, %y\")\n",
    "        \n",
    "        date_list.append(date)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'X': x_list, 'Y': y_list, 'Date': date_list})\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "\n",
    "###UNFORESEEN 4###\n",
    "result = calculate_differences_date_X_coordinate(df)\n",
    "\n",
    "last_row_length = df.at[len(df)-1, 'X'] + result[0]\n",
    "\n",
    "original_timestamp = df.at[len(df)-1, 'Date']\n",
    "days_to_add = result[1] \n",
    "\n",
    "new_timestamp = original_timestamp + pd.to_timedelta(days_to_add, unit='D')\n",
    "\n",
    "# New row data\n",
    "new_row = {'X': last_row_length, 'Y': 0, 'Date': new_timestamp}\n",
    "\n",
    "new_row  = pd.DataFrame(new_row, index=[len(df)])\n",
    "\n",
    "# Add the new row to the DataFrame\n",
    "df = pd.concat([df, new_row], ignore_index=True) \n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST create_y_points_dataframe\n",
    "\n",
    "# Your data\n",
    "y_points_data = y_points #y_points when not doing ndiaye debug #y_axis_points otherwise\n",
    "\n",
    "# Call the function with your data\n",
    "y_points_dataframe = create_y_points_dataframe(y_points_data)\n",
    "\n",
    "#put this below line inside the function (see above)\n",
    "#y_points_dataframe['Date'] = pd.to_datetime(y_points_dataframe['Date'], errors='coerce')\n",
    "\n",
    "y_points_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Data Points Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST create_data_points_table\n",
    "\n",
    "# Your data\n",
    "data_points = points\n",
    "\n",
    "# Call the function with your data\n",
    "data_points_table = create_data_points_table(data_points)\n",
    "data_points_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate Market Value (using axis points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST estimate_value\n",
    "\n",
    "# Example usage:\n",
    "input_y_mv = 306.136364\t\n",
    "\n",
    "\n",
    "estimated_value = estimate_value(input_y_mv, x_points_dataframe)\n",
    "estimated_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate date (using axis points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST estimate_date\n",
    "\n",
    "# Example usage:\n",
    "input_x_date = 131.3802231 #531.0777369356792 for zelarayan\n",
    "#y_points_dataframe\n",
    "\n",
    "estimated_date = estimate_date(input_x_date, y_points_dataframe)\n",
    "estimated_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add date difference column (from match date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST add_date_difference\n",
    "for i in range(0, len(data_points_table)):\n",
    "    # Handling MV\n",
    "    this_row_mv_coordinate = data_points_table.at[i, 'Y']\n",
    "    this_row_mv = estimate_value(this_row_mv_coordinate, x_points_dataframe)\n",
    "    data_points_table.at[i, 'Market Value'] = this_row_mv\n",
    "\n",
    "    # Handling Date\n",
    "    this_row_date_coordinate = data_points_table.at[i, 'X']\n",
    "    this_row_date = estimate_date(this_row_date_coordinate, y_points_dataframe)\n",
    "    data_points_table.at[i, 'Date'] = pd.Timestamp(this_row_date)\n",
    "\n",
    "# Set appropriate data types\n",
    "data_points_table['Market Value'] = data_points_table['Market Value'].astype(float)\n",
    "data_points_table['Date'] = pd.to_datetime(data_points_table['Date'], errors='coerce')\n",
    "\n",
    "data_points_table\n",
    "\n",
    "#this is for Ndiaye\n",
    "#do it for zelarayan \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find closest market value (from table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST findMarketValueFromTable\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "result = findMarketValueFromTable(data_points_table, input_year_test)\n",
    "print(f\"The market value closest to {input_year_test} is: {result}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Name Match Date Lookup (Dejan Lovren)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start\n",
    "\n",
    "#search = f\"{vet_for_match_season[0]} Transfermarkt\"\n",
    "\n",
    "search = f\"Dejan Lovren Transfermarkt\"\n",
    "input_year_test = '08.06.19'\n",
    "\n",
    "url = 'https://www.google.com/search'\n",
    "\n",
    "headers = {'Accept' : '*/*', 'Accept-Language': 'en-US,en;q=0.5','User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.82',\n",
    "}\n",
    "parameters = {'q': search}\n",
    "\n",
    "content = requests.get(url, headers = headers, params = parameters).text\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "search = soup.find(id = 'search')\n",
    "first_link = search.find('a')\n",
    "\n",
    "url_tosplit = (first_link['href'])\n",
    "\n",
    "player_name_string = url_tosplit.split('/')[3]\n",
    "player_code = url_tosplit.split('/')[6]\n",
    "\n",
    "find_national_team_history_URL = f\"https://www.transfermarkt.us/{player_name_string}/nationalmannschaft/spieler/{player_code}\"\n",
    "\n",
    "page_soup_history_pg = grab_transfer_pagesoup(find_national_team_history_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another example - Server Djeparov?\n",
    "#djeparov_soup = players_pagesoup_dictionary[filtered_names[i]]\n",
    "# input_year_test = '03-09-2015' #Djeparov match date\n",
    "# page_soup_history_pg = djeparov_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find name in national team history\n",
    "boxes = page_soup_history_pg.find_all(\"div\", {\"class\": \"box\"})\n",
    "\n",
    "nationalteamcareer_table = boxes[0].find(\"table\")\n",
    "\n",
    "rows = nationalteamcareer_table.find_all('tr')\n",
    "\n",
    "teams_played_for_array = []\n",
    "\n",
    "for i in range(0, len(rows)): \n",
    "    if(i == 0):\n",
    "        0==0\n",
    "    elif(i % 2 != 0):\n",
    "        0==0\n",
    "    else:\n",
    "        this_row =  rows[i]\n",
    "\n",
    "        nat_team_name = this_row.find_all(\"td\", {\"class\": \"hauptlink no-border-links hide-for-small\"})[0].text.strip().encode().decode(\"utf-8\")\n",
    "        teams_played_for_array.append(nat_team_name)\n",
    "\n",
    "teams_played_for_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINDING MATCH DATE IN THEIR HISTORY\n",
    "#datestring_lookup = input_year_test.replace(\".\", \"/\")\n",
    "\n",
    "#month, day, year = datestring_lookup.split('/')\n",
    "# Swap the month and day\n",
    "#swapped_date = f'{day}/{month}/{year}'\n",
    "\n",
    "#swapped_date\n",
    "\n",
    "correctly_formatted_date = process_date_format_for_transfermarkt_lookup(input_year_test)\n",
    "\n",
    "#boxes[2].find(\"table\")#.find_all(\"tr\", {\"class\": \"bg_gelb_20\"})\n",
    "\n",
    "#.find_all(\"div\", {\"class\": \"responsive-table\"})\n",
    "table_test = page_soup_history_pg.find_all(\"div\", {\"class\": \"responsive-table\"})[1].find_all(\"tbody\")[0]\n",
    "\n",
    "for i in range(0, len(table_test.find_all('tr'))):\n",
    "    this_tr_row = table_test.find_all('tr')[i]\n",
    "\n",
    "    data_row = this_tr_row.find_all(\"td\", {\"class\": \"zentriert\"})\n",
    "\n",
    "    if(len(data_row) == 1):\n",
    "        0==0\n",
    "    elif(len(data_row) == 7):\n",
    "        0==0\n",
    "        #wasn't in the squad\n",
    "    elif(len(data_row) == 12):\n",
    "\n",
    "        match_date_row = data_row[1].text.strip()\n",
    "        #print(match_date_row)\n",
    "        if((match_date_row == correctly_formatted_date)):  #remember this was swapped_date\n",
    "            print(i, match_date_row)\n",
    "        elif(add_leading_zeros(match_date_row) == correctly_formatted_date): #remember this was swapped_date\n",
    "            print(i, add_leading_zeros(match_date_row))\n",
    "    else:\n",
    "        print(i, len(data_row))\n",
    "\n",
    "switched_date = datetime.strptime(correctly_formatted_date, \"%m/%d/%y\").strftime(\"%d/%m/%y\")\n",
    "\n",
    "for i in range(0, len(table_test.find_all('tr'))):\n",
    "    this_tr_row = table_test.find_all('tr')[i]\n",
    "\n",
    "    data_row = this_tr_row.find_all(\"td\", {\"class\": \"zentriert\"})\n",
    "\n",
    "    if(len(data_row) == 1):\n",
    "        0==0\n",
    "    elif(len(data_row) == 7):\n",
    "        0==0\n",
    "        #wasn't in the squad\n",
    "    elif(len(data_row) == 12):\n",
    "\n",
    "        match_date_row = data_row[1].text.strip()\n",
    "        #print(match_date_row)\n",
    "        if((match_date_row == switched_date)):  #remember this was swapped_date\n",
    "            print(i, match_date_row)\n",
    "        elif(add_leading_zeros(match_date_row) == switched_date): #remember this was swapped_date\n",
    "            print(i, add_leading_zeros(match_date_row))\n",
    "    else:\n",
    "        print(i, len(data_row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookup Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lookup name\n",
    "# example_problem = '6. Lovren'\n",
    "# natl_test = 'Croatia'\n",
    "# input_year_test = '08.06.19'\n",
    "\n",
    "example_problem = '19 B. Traoré'\n",
    "natl_test = 'Burkina Faso'\n",
    "input_year_test = \"07.10.16\"\n",
    "salaries_or_values = 'salary'\n",
    "\n",
    "\n",
    "if(salaries_or_values == \"salary\"):\n",
    "    database_name = leagues_salary\n",
    "    money_column_name = \"Inflation-Adjusted Yearly Salary\"\n",
    "elif(salaries_or_values == \"value\"):\n",
    "    database_name = leagues_value\n",
    "    money_column_name = \"Market Value\"\n",
    "\n",
    "    \n",
    "candidate_name = \"\"\n",
    "candidates_set = []\n",
    "match_type = \"\"\n",
    "\n",
    "\n",
    "#players from their country \n",
    "dataset_nationality = database_name[database_name['Nationality'] == f\"{natl_test}\"]['Name'].unique()\n",
    "\n",
    "if(is_cyrillic(example_problem)):\n",
    "    #change from cyrillic to english\n",
    "    example_problem = cyrillic_to_latin(example_problem)\n",
    "\n",
    "#remove jersey Nums and order initials correctly. \n",
    "search_name, final_tokens_name = process_string_newest_ii(example_problem) \n",
    "\n",
    "#look their name up in the list of names from their nationality. \n",
    "# result = find_closest_string(search_name, dataset_nationality)\n",
    "result = find_closest_string_newEST(search_name, dataset_nationality, final_tokens_name, example_problem)\n",
    "if((type(result) == list) & (len(result) >= 2)):\n",
    "    candidates_set = result\n",
    "elif(result[0] in dataset_nationality):\n",
    "    candidate_name = result[0]\n",
    "elif(result in dataset_nationality):\n",
    "    print(result)\n",
    "    #RETURN \n",
    "    candidate_name = result\n",
    "else:\n",
    "    #no match found after first call \n",
    "    print('no initial match found: ', search_name)\n",
    "    nationality_names_accents_removed = remove_accents_from_strings(dataset_nationality)\n",
    "    match_accent_accounted = find_closest_string_newEST(search_name, nationality_names_accents_removed,final_tokens_name, example_problem)\n",
    "    if(match_accent_accounted in nationality_names_accents_removed):\n",
    "        #print(match_accent_accounted)\n",
    "        matching_names_with_accents = find_names_with_accents(match_accent_accounted, dataset_nationality)\n",
    "        if(type(matching_names_with_accents) == str):\n",
    "            print(matching_names_with_accents)\n",
    "            #RETURN\n",
    "            candidate_name = matching_names_with_accents\n",
    "        elif(len(matching_names_with_accents) == 0):\n",
    "            print(f'accent-less name found: {match_accent_accounted}. But name not in original dataset')\n",
    "        else:\n",
    "            print(f'multiple names found after adding accents: {matching_names_with_accents}')\n",
    "            candidates_set = matching_names_with_accents\n",
    "\n",
    "\n",
    "        #MAKE SURE THE NAME WITH ACCENTS IS IN DATASET NATIONALITY \n",
    "    else:\n",
    "        print('no accent match found:', search_name)\n",
    "\n",
    "        dataset_nationality_backticks = remove_apostrophes_backticks(dataset_nationality) #dataset_nationality_updated\n",
    "        match_apostrophes_accounted = find_closest_string_newEST(search_name, dataset_nationality_backticks,final_tokens_name, example_problem)\n",
    "    \n",
    "        if(match_apostrophes_accounted in dataset_nationality_backticks):\n",
    "\n",
    "            lastname_match = match_apostrophes_accounted.split()[-1] \n",
    "            original_string_nojersey = re.sub(r'^\\d+(\\.)?\\s*', '', example_problem)\n",
    "            correct_lastname = add_backticks(lastname_match, original_string_nojersey)\n",
    "            correct_firstname = extract_first_name(match_apostrophes_accounted, lastname_match)\n",
    "            \n",
    "            correct_name_full = correct_firstname + ' ' + correct_lastname\n",
    "\n",
    "            if(correct_name_full in dataset_nationality):\n",
    "                print(correct_name_full)\n",
    "                #RETURN\n",
    "                candidate_name = correct_name_full\n",
    "            elif(correct_name_full.replace('`', \"'\") in dataset_nationality):\n",
    "                print(correct_name_full.replace('`', \"'\"))\n",
    "                #RETURN\n",
    "                candidate_name = correct_name_full.replace('`', \"'\")\n",
    "            elif(type(match_apostrophes_accounted) != str):\n",
    "                print(f'multiple names found after adding backticks: {match_apostrophes_accounted}')\n",
    "                candidates_set = match_apostrophes_accounted\n",
    "            else:\n",
    "                print(f'backtick-less name found: {match_apostrophes_accounted}. But name not in original dataset')\n",
    "        \n",
    "        else:\n",
    "            print('no backtick match found:', search_name)\n",
    "\n",
    "if(candidate_name != \"\"):\n",
    "    match_type = \"single\"\n",
    "    print(candidate_name, match_type, search_name)\n",
    "    #return candidate_name, match_type, search_name\n",
    "else:\n",
    "    if(candidates_set != []):\n",
    "        match_type = \"multiple\"\n",
    "        print(candidates_set, match_type, search_name)\n",
    "        #return candidates_set, match_type, search_name\n",
    "    else:\n",
    "        match_type = \"none\"\n",
    "        print(0, match_type, search_name)\n",
    "        #return 0, match_type, search_name\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find closest string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find closest string newest\n",
    "\n",
    "\n",
    "#replace nationality name list with string_list\n",
    "#replace string_for_search with input_string \n",
    "#replace final_tokens with input_final_tokens\n",
    "#replace input_string with ORIGINAL_NAME_STRING\n",
    "\n",
    "string_list = dataset_nationality\n",
    "input_string = search_name\n",
    "input_final_tokens = final_tokens_name\n",
    "ORIGINAL_NAME_STRING = example_problem\n",
    "\n",
    "\n",
    "closest_match = get_close_matches(input_string, string_list, n=1, cutoff=0.8)\n",
    "\n",
    "closest_match_4 = []\n",
    "closest_match_3 = []\n",
    "closest_match_2 = []\n",
    "index_match = \"\"\n",
    "matching_indices = []\n",
    "\n",
    "if closest_match:\n",
    "    #TIGHT CLOSE MATCH FUNCTION RETURNS A NAME\n",
    "    #print('0', closest_match[0], closest_match in string_list)\n",
    "    #RETURN HERE\n",
    "    #return closest_match[0]\n",
    "    print(closest_match[0])\n",
    "else:\n",
    "    #Reduce match constraints\n",
    "    closest_match_ii = get_close_matches(input_string, string_list)\n",
    "    #Produces a match\n",
    "    if closest_match_ii:\n",
    "        if((type(closest_match_ii) == list) & (len(closest_match_ii) >= 2)):\n",
    "            #Closest match II returns 1 name\n",
    "            original_string_nojersey = re.sub(r'^\\d+(\\.)?\\s*', '', ORIGINAL_NAME_STRING)\n",
    "            #Find best match from set of names\n",
    "            result_1 = find_best_match(closest_match_ii, input_final_tokens, original_string_nojersey)\n",
    "            if(type(result_1) == list):\n",
    "                closest_match_ii = result_1\n",
    "                print(closest_match_ii)\n",
    "            elif(pd.isna(result_1)):\n",
    "                #none of the names from closest match ii were a good match\n",
    "                0==0\n",
    "            else:\n",
    "                #1 of the names from closest match ii were a good match\n",
    "                #print('closest match ii best match: ' + result_1)\n",
    "                closest_match_ii = result_1\n",
    "                #RETURN HERE\n",
    "                #return closest_match_ii\n",
    "                print(closest_match_ii)\n",
    "        else:\n",
    "            #Closest match II returns 1 name\n",
    "            #print('1 match ' + closest_match_ii[0])\n",
    "            #RETURN HERE\n",
    "            #return closest_match_ii[0]\n",
    "            print(closest_match_ii[0])\n",
    "    else:\n",
    "        # no close matches\n",
    "        last_word = input_string.split()[-1]\n",
    "        #KREJCI CASE \n",
    "        if(last_word == 'Krejčí'):\n",
    "            match_krejci = get_close_matches(last_word, string_list, n=1, cutoff=0.380952)\n",
    "            if(type(match_krejci) == list):\n",
    "                if(len(match_krejci) == 1):\n",
    "                    match_krejci = match_krejci[0]\n",
    "                    print(match_krejci)\n",
    "                    #return match_krejci\n",
    "                \n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "        #if this is an initial you need to save it as an initial or a word start \n",
    "        #if(last_word)\n",
    "\n",
    "        # Return strings from the list if the last word is in those strings\n",
    "        matching_strings = [s for s in string_list if last_word in s]\n",
    "\n",
    "        if matching_strings:\n",
    "            if(len(matching_strings) == 1):\n",
    "                print('match string ' + matching_strings[0])\n",
    "                #RETURN HERE\n",
    "                #return matching_strings[0]\n",
    "\n",
    "            else:\n",
    "                print(matching_strings)\n",
    "                setofmatches = matching_strings\n",
    "                \n",
    "    #elif(closest_match):       \n",
    "    if(closest_match):\n",
    "        #RETURN\n",
    "        print('1', closest_match[0], closest_match[0] in string_list)\n",
    "    elif(closest_match_ii):\n",
    "        if(type(closest_match_ii) == str):\n",
    "            print(closest_match_ii, closest_match_ii in string_list)\n",
    "        #RETURN\n",
    "        else:\n",
    "            print('ii', closest_match_ii)#closest_match_ii[0] in string_list\n",
    "    elif(closest_match_3):\n",
    "        #RETURN\n",
    "        print('3', closest_match_3[0], closest_match_3[0] in string_list)\n",
    "    elif(closest_match_2):\n",
    "        #RETURN\n",
    "        print('2', closest_match_2[0], closest_match_2[0] in string_list)\n",
    "    elif(closest_match_4):\n",
    "        #RETURN\n",
    "        print('4', closest_match_4[0], closest_match_4[0] in string_list)\n",
    "    elif(index_match != \"\"):\n",
    "        #RETURN\n",
    "        print('end ' + index_match + ORIGINAL_NAME_STRING, matching_indices)\n",
    "        #return(index_match)\n",
    "    else:\n",
    "        #RETURN\n",
    "        #return(\"No close match found.\")\n",
    "        print(\"No close match found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best match\n",
    "closest_match_ii = ['Dejan Lovren', 'Davor Lovren', 'Lovre Kalinic']\n",
    "input_final_tokens = ['Lovren']\n",
    "original_string_nojersey = 'Lovren'\n",
    "\n",
    "#closest_match_ii, input_final_tokens, original_string_nojersey\n",
    "array = closest_match_ii\n",
    "final_tokens = input_final_tokens\n",
    "ORIGINAL_STRING = original_string_nojersey\n",
    "\n",
    "\n",
    "# Concatenate final tokens to form the expected full name\n",
    "expected_name = ' '.join(final_tokens)\n",
    "\n",
    "# Filter names that start with the initial letter\n",
    "filtered_names = [name for name in array if name.startswith(final_tokens[0])]\n",
    "\n",
    "if not filtered_names:\n",
    "    if(len(final_tokens) == 1):\n",
    "        filtered_names = [name for name in array if name.endswith(final_tokens[-1])]\n",
    "        final_tokens = [final_tokens[-1]]\n",
    "    # Try switching the order of final tokens\n",
    "    else:\n",
    "        filtered_names = [name for name in array if name.startswith(final_tokens[1])]\n",
    "        final_tokens = [final_tokens[1], final_tokens[0]]\n",
    "\n",
    "if not filtered_names:\n",
    "    print('no matching names found 1')\n",
    "    #return None  # No matching names found\n",
    "\n",
    "# Check if the ORIGINAL_STRING contains a backtick/apostrophe\n",
    "has_backtick_apostrophe = \"'\" in ORIGINAL_STRING or \"`\" in ORIGINAL_STRING\n",
    "\n",
    "# Filter names based on the presence of a backtick/apostrophe\n",
    "filtered_names = [name for name in filtered_names if \"'\" in name or \"`\" in name] if has_backtick_apostrophe else filtered_names\n",
    "\n",
    "if not filtered_names:\n",
    "    print('no matching names found 2')\n",
    "    #return None  # No matching names found\n",
    "\n",
    "# Calculate Levenshtein distance between the expected name and each remaining name\n",
    "distances = [Levenshtein.distance(unidecode(expected_name), unidecode(name.replace(\" \", \"\"))) for name in filtered_names]\n",
    "\n",
    "# Find the minimum distance\n",
    "min_distance = min(distances)\n",
    "\n",
    "# Find the indices of names with the minimum distance\n",
    "min_distance_indices = [i for i, distance in enumerate(distances) if distance == min_distance]\n",
    "\n",
    "# Return all names with the minimum distance\n",
    "result_names = [filtered_names[i] for i in min_distance_indices]\n",
    "\n",
    "# Print or return the result based on your requirement\n",
    "print(result_names)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
