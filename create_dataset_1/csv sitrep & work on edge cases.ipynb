{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/finneganlaister-smith/Downloads/DEV ENVIRONMENT/data-science-jupyter-template-main/footy_project_market_value_rankings/create_dataset_1/AssemblyHelpers.py:54: DtypeWarning: Columns (3,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(file_path)\n",
      "/var/folders/vw/z2xv57_n70l_szw_fr80j5km0000gn/T/ipykernel_98335/3382385496.py:49: DtypeWarning: Columns (3,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from countries_languages import country_to_language\n",
    "import pandas as pd\n",
    "from difflib import get_close_matches\n",
    "import numpy as np\n",
    "import re\n",
    "from transliterate import translit\n",
    "from unidecode import unidecode\n",
    "import Levenshtein\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "import wikipediaapi\n",
    "\n",
    "import wikipedia\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import difflib\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import calendar\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import AssemblyHelpers\n",
    "#from AssemblyHelpers import find_money_info_from_name\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def load_csv_dataset(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "countries_codes = load_csv_dataset('/Users/finneganlaister-smith/Downloads/DEV ENVIRONMENT/data-science-jupyter-template-main/footy_project_market_value_rankings/create_dataset_1/countries_and_codes.csv')\n",
    "\n",
    "#leagues value\n",
    "leagues_value = load_csv_dataset('/Users/finneganlaister-smith/Downloads/DEV ENVIRONMENT/data-science-jupyter-template-main/footy_project_market_value_rankings/create_dataset_1/Most Updated Edited Transfermarkt Dataset.csv')\n",
    "#leagues value large \n",
    "leagues_value_large = load_csv_dataset('/Users/finneganlaister-smith/Downloads/DEV ENVIRONMENT/data-science-jupyter-template-main/footy_project_market_value_rankings/create_dataset_1/most_updated_transfermarkt_dataset.csv')\n",
    "\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "def find_close_matches_variable(this_jersey, dataset_nationality, threshold):\n",
    "    \"\"\"\n",
    "    Find close matches of `this_jersey` in `dataset_nationality` using Levenshtein distance.\n",
    "\n",
    "    Args:\n",
    "    - this_jersey (str): The string to find close matches for.\n",
    "    - dataset_nationality (list): List of strings to search for close matches in.\n",
    "    - threshold (int): Minimum similarity score required for a match (default is 90).\n",
    "\n",
    "    Returns:\n",
    "    - List of strings from `dataset_nationality` that are close matches to `this_jersey`.\n",
    "    \"\"\"\n",
    "    close_matches = process.extract(this_jersey, dataset_nationality, limit=None)\n",
    "    return [match[0] for match in close_matches if match[1] >= threshold]\n",
    "\n",
    "#threshold match one \n",
    "def threshold_player_match(this_jersey, dataset_nationality):\n",
    "    THRESHOLD_NUM = 89\n",
    "    #stops when it returns a name.\n",
    "    #if it doesnt find a match keep lowering the threshold until you find a match\n",
    "    #but if you get to threshold of like 50 first you stop and just abandon ship    \n",
    "\n",
    "    # Loop until someone is found or threshold goes below 50\n",
    "    while THRESHOLD_NUM >= 50:\n",
    "        matches = find_close_matches_variable(this_jersey, dataset_nationality, THRESHOLD_NUM)\n",
    "        if matches:\n",
    "            #print(f\"Player is {this_jersey}. Found matches: {matches}. threshold is {THRESHOLD_NUM}\")\n",
    "            return matches, THRESHOLD_NUM\n",
    "            #break\n",
    "        else:\n",
    "            THRESHOLD_NUM -= 1\n",
    "\n",
    "    # If threshold reaches below 50 without finding any matches\n",
    "    if THRESHOLD_NUM < 50:\n",
    "        return [f\"No matches found even with the lowest threshold.\"], this_jersey #jersey was {this_jersey}\n",
    "\n",
    "\n",
    "def filter_using_date_of_match(candidate_list_of_names_input, nationality_input, matchdate_Input, SALARY_BOOLEAN): \n",
    "\n",
    "    FILTERED_NAMES_USING_MATCH_DATE = AssemblyHelpers.multiNameMatchDateLookup(candidate_list_of_names_input, nationality_input, matchdate_Input) \n",
    "\n",
    "    #print('in fudom', type(FILTERED_NAMES_USING_MATCH_DATE))\n",
    "    #1 MATCH\n",
    "    if isinstance(FILTERED_NAMES_USING_MATCH_DATE, str) or isinstance(FILTERED_NAMES_USING_MATCH_DATE, np.str_): #YELLOW 3\n",
    "        #print('were here', FILTERED_NAMES_USING_MATCH_DATE)\n",
    "        #print('were in fudom in the right place')\n",
    "        #NAME_FOR_SELENIUM_SEARCH = FILTERED_NAMES_USING_MATCH_DATE\n",
    "        # MONEY_FOUND_ONLINE = useSeleniumToFindMoney(NAME_FOR_SELENIUM_SEARCH, nationality_input, matchdate_Input, SALARY_BOOLEAN)\n",
    "        # print('made it out of using selenium')\n",
    "        # money_thisplayer = [MONEY_FOUND_ONLINE, f'Online WAS Lookup Required - salary boolean is {SALARY_BOOLEAN}', True, False]\n",
    "        money_thisplayer = [0, 'Lookup Required', True, False]\n",
    "        return money_thisplayer, FILTERED_NAMES_USING_MATCH_DATE\n",
    "    #MULTIPLE MATCHES\n",
    "    elif(len(FILTERED_NAMES_USING_MATCH_DATE) >= 2):\n",
    "        print(FILTERED_NAMES_USING_MATCH_DATE, type(FILTERED_NAMES_USING_MATCH_DATE), type(FILTERED_NAMES_USING_MATCH_DATE) == str)\n",
    "        money_thisplayer = [0, F'FILTERING USING MATCH DATE LEFT MULTIPLE RESULTS: {FILTERED_NAMES_USING_MATCH_DATE}', True, False]\n",
    "        return money_thisplayer, FILTERED_NAMES_USING_MATCH_DATE\n",
    "    #0 MATCHES\n",
    "    elif(len(FILTERED_NAMES_USING_MATCH_DATE) == 0):\n",
    "        money_thisplayer = [0, F'FILTERING USING MATCH DATE LEFT 0 RESULTS. BEFORE FILTERING, CANDIDATE NAMES WERE {candidate_list_of_names_input}', True, False]\n",
    "        return money_thisplayer, ''\n",
    "    \n",
    "\n",
    "def check_tokens(this_jersey, match):\n",
    "    jersey_tokens = this_jersey.lower().split()\n",
    "    match_tokens = match.lower().split()\n",
    "    \n",
    "    if all(token in match_tokens for token in jersey_tokens):\n",
    "        return True\n",
    "    elif all(token in jersey_tokens for token in match_tokens) and len(jersey_tokens) == len(match_tokens) + 1:\n",
    "        return True\n",
    "    elif all(token in match_tokens for token in jersey_tokens) and len(jersey_tokens) > len(match_tokens):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def filter_multi_word_matches_by_jersey_tokens(jersey_tokens, potential_matches):\n",
    "    filtered_matches = []\n",
    "\n",
    "    for match in potential_matches:\n",
    "        # Split each match into tokens\n",
    "        match_tokens = match.split()\n",
    "\n",
    "        # Count the number of tokens that start with each character in jersey_tokens\n",
    "        match_token_start_chars = {token[0] for token in match_tokens}\n",
    "        #print(match_token_start_chars, f\"*{jersey_tokens}\")\n",
    "\n",
    "\n",
    "        #for token in jersey_tokens:\n",
    "        #   print(token, token[0] in match_token_start_chars)\n",
    "\n",
    "        # Check if the match contains at least one token for each character in jersey_tokens\n",
    "        if all(token[0] in match_token_start_chars for token in jersey_tokens):\n",
    "            filtered_matches.append(match)\n",
    "\n",
    "    return filtered_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV SITREP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BIG DATASET\n",
    "\n",
    "large_df_combined = load_csv_dataset('updated COMBINED DF_WORKING.csv')\n",
    "\n",
    "\n",
    "terrell_new_df = load_csv_dataset('CSVs of edge cases - for T/updated Terrell new DF.csv')\n",
    "len(terrell_new_df), len(large_df_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group 1 \n",
    "\n",
    "group_1_updated = load_csv_dataset('updated_Group1_fixes_complete_ii.csv')\n",
    "len(group_1_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group 2 \n",
    "\n",
    "online_lookup_group_2 = load_csv_dataset('Group 2 (for T) - Find their $ Info/Online lookup required - 1 name found.csv')\n",
    "\n",
    "lookup_group_2 = load_csv_dataset('Group 2 (for T) - Find their $ Info/lookup_required_matches.csv')\n",
    "\n",
    "cyrillic_group_2 = load_csv_dataset('Group 2 (for T) - Find their $ Info/Cyrillic_Found_Guys.csv')\n",
    "len(online_lookup_group_2), len(lookup_group_2), len(cyrillic_group_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group 3 \n",
    "\n",
    "cyrillic_group_3 = load_csv_dataset('CSVs of edge cases - for F/Group 3 - Nobody Found/Cyrillic_Guys_Nobody_Found.csv')\n",
    "\n",
    "zeros_group_3 = load_csv_dataset('CSVs of edge cases - for F/Group 3 - Nobody Found/odd_name_found_0_cases.csv')\n",
    "\n",
    "online_lookup_group_3 = load_csv_dataset('CSVs of edge cases - for F/Group 3 - Nobody Found/Online lookup required - 0 names found.csv')\n",
    "\n",
    "len(cyrillic_group_3), len(zeros_group_3), len(online_lookup_group_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leagues_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other group\n",
    "\n",
    "wrong_df = load_csv_dataset('CSVs of edge cases - for F/\"other\" group/updated Terrell lookup cases - wrong DF.csv')\n",
    "wrong_df = wrong_df.drop(columns={'Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.2'})\n",
    "lookup_required_dudes = load_csv_dataset('CSVs of edge cases - for F/\"other\" group/lookup_required_dudes.csv')\n",
    "\n",
    "match_date_filter_said_0_guys = load_csv_dataset('CSVs of edge cases - for F/\"other\" group/updated match date said zero guys.csv')\n",
    "\n",
    "len(wrong_df), len(lookup_required_dudes), len(large_dataset_said_0), len(match_date_filter_said_0_guys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORKING WITH WRONG DF (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_df['New Found Name'] = ''\n",
    "wrong_df['Status II'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_df[wrong_df['Status'] == 'SUCCESS']['Lookup Return Case'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_status_counter = 0\n",
    "for i in range(0, 1063):\n",
    "    names_found = []\n",
    "    num_identical_rows = 0\n",
    "    num_names_this = 0\n",
    "    index = i\n",
    "    row = wrong_df.loc[index]\n",
    "    # Extract relevant columns\n",
    "    match = row['Match']\n",
    "    date = row['Date']\n",
    "\n",
    "    if type(row['Found Name']) == str:\n",
    "        if row['Found Name'].startswith('['):\n",
    "            FOUND_NAME = eval(row['Found Name']) \n",
    "        else:\n",
    "            FOUND_NAME = row['Found Name']\n",
    "    \n",
    "    if row['Status II'] != 'Done':\n",
    "    \n",
    "        if row['Status'] == 'SUCCESS':\n",
    "            #for all the success guys. \n",
    "            #these are all people that eventually T needs to use imputing methods.   \n",
    "            NAMES_FOUND = row['Name(s) Found']\n",
    "\n",
    "            ORIGINAL_JERSEY = row['ORIGINAL JERSEY']\n",
    "            if ORIGINAL_JERSEY == 'evčík':\n",
    "                ORIGINAL_JERSEY = 'P Ševčík'\n",
    "            this_country_code = row['Team Country Code']\n",
    "            if pd.isna(this_country_code):\n",
    "                country_name = 'Namibia'\n",
    "            else:\n",
    "                country_name = countries_codes[countries_codes[' Code'] == this_country_code]['Country'].unique()[0]\n",
    "            dataset_nationality = leagues_value[leagues_value['Team 1 Code'] == this_country_code]['Name'].unique()\n",
    "            if 'Mohd' in ORIGINAL_JERSEY:\n",
    "                ORIGINAL_JERSEY = ORIGINAL_JERSEY.replace('Mohd', 'Mohammed')\n",
    "            if type(FOUND_NAME) == list:\n",
    "                if NAMES_FOUND.startswith('['):\n",
    "                    for name in eval(NAMES_FOUND):\n",
    "                        FOUND_NAME.append(name)\n",
    "                else:\n",
    "                    FOUND_NAME.append(NAMES_FOUND)\n",
    "                found_names = FOUND_NAME\n",
    "            else:\n",
    "                found_names = [FOUND_NAME, NAMES_FOUND]\n",
    "            closest_name = filter_multi_word_matches_by_jersey_tokens([token[0] for token in ORIGINAL_JERSEY.split(' ')], found_names)\n",
    "\n",
    "            threshold_last_name = threshold_player_match(ORIGINAL_JERSEY.split(' ')[-1], dataset_nationality)[0]\n",
    "            if closest_name != []:\n",
    "                intersection = [name for name in closest_name if name in found_names]\n",
    "            else:\n",
    "                intersection = [name for name in found_names if name in threshold_last_name]\n",
    "\n",
    "            \n",
    "            if list(set(intersection)) != []:\n",
    "                if len(list(set(intersection))) == 1 and list(set(intersection)) != ['No matches found even with the lowest threshold.']:\n",
    "                    if ORIGINAL_JERSEY == 'Mudir Al Radaei':\n",
    "                        intersection = ['Mudir Abdurabu']\n",
    "                    elif ORIGINAL_JERSEY == 'Noel-Mc Leod':\n",
    "                        intersection = ['Kraig Noel-McLeod']\n",
    "                    elif ORIGINAL_JERSEY == 'Nguyn Tin Duy':\n",
    "                        intersection = ['Tien Duy Nguyen']\n",
    "                    elif ORIGINAL_JERSEY == 'Abbas Al Hassan':\n",
    "                        intersection = ['Abbas Al-Hassan']\n",
    "                    elif ORIGINAL_JERSEY == 'Angel':\n",
    "                        intersection = ['Wilker Ángel']\n",
    "\n",
    "                    print(i, ORIGINAL_JERSEY, closest_name, found_names)\n",
    "                    print(i, f'intersection of lists is says{list(set(intersection))}')\n",
    "\n",
    "                    success_status_counter += 1\n",
    "                    wrong_df.at[i, 'New Found Name'] = list(set(intersection))[0]\n",
    "                    wrong_df.at[i, 'Status II'] = 'Done - New'\n",
    "                else:\n",
    "                    #16 cases here left\n",
    "                    filtered_names_match_date = filter_using_date_of_match(list(set(intersection)), row['Nationality'], date, False)[1]\n",
    "                    # if type(filtered_names_match_date) == str:\n",
    "                    print(i, ORIGINAL_JERSEY, closest_name, found_names)\n",
    "                    print(i, f'FILTERED intersection of lists is says{filtered_names_match_date}')\n",
    "                    wrong_df.at[i, 'New Found Name'] = filtered_names_match_date#[name for name in closest_name if name in found_names][0]\n",
    "                    wrong_df.at[i, 'Status II'] = 'Done - Match Date Filtered'\n",
    "                    success_status_counter += 1\n",
    "            else:\n",
    "                0==0 #18 cases here\n",
    "                success_status_counter += 1\n",
    "                print(i, wrong_df.at[i, 'Date'], wrong_df.at[i, 'Nationality'], wrong_df.at[i, 'Match'], ORIGINAL_JERSEY, closest_name, found_names)\n",
    "                print(i, f'lists is says{found_names}')\n",
    "            \n",
    "        else:\n",
    "        \n",
    "                      \n",
    "            NAMES_FOUND = row['Name(s) Found']\n",
    "\n",
    "            ORIGINAL_JERSEY = row['ORIGINAL JERSEY']\n",
    "            if ORIGINAL_JERSEY == 'evčík':\n",
    "                ORIGINAL_JERSEY = 'P Ševčík'\n",
    "            this_country_code = row['Team Country Code']\n",
    "            if pd.isna(this_country_code):\n",
    "                country_name = 'Namibia'\n",
    "            else:\n",
    "                country_name = countries_codes[countries_codes[' Code'] == this_country_code]['Country'].unique()[0]\n",
    "            dataset_nationality = leagues_value[leagues_value['Team 1 Code'] == this_country_code]['Name'].unique()\n",
    "            if 'Mohd' in ORIGINAL_JERSEY:\n",
    "                ORIGINAL_JERSEY = ORIGINAL_JERSEY.replace('Mohd', 'Mohammed')\n",
    "            if type(FOUND_NAME) == list:\n",
    "                if NAMES_FOUND.startswith('['):\n",
    "                    for name in eval(NAMES_FOUND):\n",
    "                        FOUND_NAME.append(name)\n",
    "                else:\n",
    "                    FOUND_NAME.append(NAMES_FOUND)\n",
    "                found_names = FOUND_NAME\n",
    "            else:\n",
    "                found_names = [FOUND_NAME, NAMES_FOUND]\n",
    "            closest_name = filter_multi_word_matches_by_jersey_tokens([token[0] for token in ORIGINAL_JERSEY.split(' ')], found_names)\n",
    "     \n",
    "            if len(closest_name) == 1:\n",
    "                print(i, ORIGINAL_JERSEY, [name for name in closest_name if name in found_names]) #closest_name, found_names\n",
    "                success_status_counter += 1\n",
    "                \n",
    "                #wrong_df['Status 2'] MAKE A NEW STATUS TO MARK THEM OFF ONCE DONE  \n",
    "            else:\n",
    "                print(i)\n",
    "                \n",
    "                threshold_last_name = threshold_player_match(ORIGINAL_JERSEY.split(' ')[-1], dataset_nationality)[0]\n",
    "                if closest_name != []:\n",
    "                    intersection = [name for name in closest_name if name in found_names and name in threshold_last_name]\n",
    "                else:\n",
    "                    intersection = [name for name in found_names if name in threshold_last_name]\n",
    "                if len(list(set(intersection))) == 2:\n",
    "                    \n",
    "                    filtered_names_match_date = filter_using_date_of_match(list(set(intersection)), row['Nationality'], date, False)[1]\n",
    "                    if type(filtered_names_match_date) == str:\n",
    "                        print(i, ORIGINAL_JERSEY, closest_name, found_names)\n",
    "                        print(i, f'SINGLE - filtered bymatch date - intersection of lists is says{filtered_names_match_date}')\n",
    "                        wrong_df.at[i, 'New Found Name'] = filtered_names_match_date#[name for name in closest_name if name in found_names][0]\n",
    "                        wrong_df.at[i, 'Status II'] = 'Done - Match Date Filtered'\n",
    "                    \n",
    "                        success_status_counter += 1\n",
    "                    elif type(filtered_names_match_date) == list:\n",
    "                        0==0\n",
    "                        print(i, ORIGINAL_JERSEY, closest_name, found_names)\n",
    "                        print(i, f'MULTI - filtered bymatch date - intersection of lists is says{filtered_names_match_date}')\n",
    "                        wrong_df.at[i, 'New Found Name'] = filtered_names_match_date#[name for name in closest_name if name in found_names][0]\n",
    "                        wrong_df.at[i, 'Status II'] = 'Done - Match Date Filtered'\n",
    "                        success_status_counter += 1\n",
    "                else:\n",
    "                    filtered_names_match_date = filter_using_date_of_match(list(set(intersection)), row['Nationality'], date, False)[1]\n",
    "                    print(i, ORIGINAL_JERSEY, closest_name, found_names)\n",
    "                    print(i, f'FILTERED intersection of lists is says{filtered_names_match_date}')\n",
    "                    success_status_counter += 1\n",
    "                    #wrong_df.at[i, 'New Found Name'] = filtered_names_match_date#[name for name in closest_name if name in found_names][0]\n",
    "                    #wrong_df.at[i, 'Status II'] = 'Done - Match Date Filtered'\n",
    "\n",
    "\n",
    "                #print(i, f\"threshold match says {threshold_player_match(ORIGINAL_JERSEY, found_names)}\")\n",
    "print(success_status_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_df.to_csv('wrong_df_corrected.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match date said zero dudes (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_date_filter_said_0_guys['Status New'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTIONS\n",
    "\n",
    "def find_transfermarkt_links(html_content):\n",
    "    # Regular expression patterns for transfermarkt URLs\n",
    "    transfermarkt_patterns = [\n",
    "        r\"transfermarkt\\.de\",\n",
    "        r\"transfermarkt\\.us\",\n",
    "        r\"transfermarkt\\.world\"\n",
    "    ]\n",
    "    transfermarkt_links = []\n",
    "\n",
    "    # Find all 'a' tags in the HTML content\n",
    "    links = html_content.find_all('a', href=True)\n",
    "\n",
    "    # Filter links that match transfermarkt patterns\n",
    "    for link in links:\n",
    "        for pattern in transfermarkt_patterns:\n",
    "            if re.search(pattern, link['href']):\n",
    "                transfermarkt_links.append(link['href'])\n",
    "                break\n",
    "\n",
    "    return transfermarkt_links\n",
    "\n",
    "\n",
    "def query_google_1(input_string):\n",
    "    # Replace spaces with plus signs to format the query string\n",
    "    query = '+'.join(input_string.split())\n",
    "\n",
    "    # URL for Google search\n",
    "    url = f\"https://www.google.com/search?q={query}\"\n",
    "\n",
    "    # Send GET request to Google\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Return the parsed HTML content\n",
    "        return soup\n",
    "    else:\n",
    "        # If request fails, print error message\n",
    "        print(\"Failed to fetch results from Google.\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def extract_name_from_link(link):\n",
    "        \n",
    "    name_in_link = link.split('/')[3]\n",
    "    name_in_link_tokens = name_in_link.split('-')\n",
    "    capitalized_tokens = []\n",
    "\n",
    "    for token in name_in_link_tokens:\n",
    "        capitalized_tokens.append(token.capitalize())\n",
    "\n",
    "    combined_string = ' '.join(capitalized_tokens)\n",
    "\n",
    "    return combined_string\n",
    "\n",
    "\n",
    "def process_google_query(input_text):\n",
    "    html_result = query_google_1(input_text)\n",
    "    if html_result:\n",
    "        transfermarkt_links = find_transfermarkt_links(html_result)\n",
    "        if transfermarkt_links:\n",
    "            names_links = []\n",
    "            for link in transfermarkt_links:\n",
    "                if link.startswith('/url?q='):\n",
    "                    link = link.split('/url?q=')[1]\n",
    "                name_from_link = extract_name_from_link(link)\n",
    "                names_links.append(name_from_link)\n",
    "            if len(names_links) == 1:\n",
    "                name = names_links[0]\n",
    "                return name\n",
    "            elif len(names_links) >= 2:\n",
    "                name_list_matches = list(set(names_links))\n",
    "                return name_list_matches\n",
    "            else:\n",
    "                return 0  # Nobody found\n",
    "        else:\n",
    "            return 0  # No links found\n",
    "    else:\n",
    "        return 0  # No HTML result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "leagues_value_old = load_csv_dataset('MarketValues.csv')\n",
    "leagues_value_old = leagues_value_old[leagues_value_old['Season'] <= 2013]\n",
    "match_date_filter_said_0_guys = load_csv_dataset('CSVs of edge cases - for F/\"other\" group/updated_match_date_filter_said_0_guys_ii.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 WORKING Saucedo Bolivia Uruguay vs Bolivia 07-Oct-11\n",
      "https://www.transfermarkt.com/carlos-saucedo/profil/spieler/77940\n",
      "https://www.transfermarkt.us/mauricio-saucedo/nationalmannschaft/spieler/73298/wettbewerb_id/CA11\n",
      "there was no national team history cannot access local variable 'match_date_row' where it is not associated with a value\n",
      "there was no national team history cannot access local variable 'match_date_row' where it is not associated with a value\n",
      "\n",
      "23 WORKING Vargas Bolivia Venezuela vs Bolivia 15-Nov-11\n",
      "https://www.transfermarkt.com/christian-vargas/nationalmannschaft/spieler/89537\n",
      "https://www.transfermarkt.us/rodrigo-vargas/profil/spieler/177676\n",
      "there was no national team history cannot access local variable 'match_date_row' where it is not associated with a value\n",
      "reverse loop. there was no national team history time data '15/11/11' does not match format '%m/%d/%y'\n",
      "reverse loop. there was no national team history time data '15/11/11' does not match format '%m/%d/%y'\n",
      "\n",
      "24 WORKING Vargas Bolivia Bolivia vs Chile 02-Jun-12\n",
      "https://www.transfermarkt.com/christian-vargas/nationalmannschaft/spieler/89537\n",
      "https://www.transfermarkt.us/rodrigo-vargas/profil/spieler/177676\n",
      "there was no national team history cannot access local variable 'match_date_row' where it is not associated with a value\n",
      "\n",
      "27 WORKING Vargas Bolivia Ecuador vs Bolivia 07-Sep-12\n",
      "https://www.transfermarkt.com/christian-vargas/nationalmannschaft/spieler/89537\n",
      "https://www.transfermarkt.us/rodrigo-vargas/profil/spieler/177676\n",
      "there was no national team history cannot access local variable 'match_date_row' where it is not associated with a value\n",
      "\n",
      "28 WORKING Saucedo Bolivia Ecuador vs Bolivia 07-Sep-12\n",
      "https://www.transfermarkt.com/carlos-saucedo/profil/spieler/77940\n",
      "https://www.transfermarkt.us/mauricio-saucedo/nationalmannschaft/spieler/73298/wettbewerb_id/CA11\n",
      "there was no national team history cannot access local variable 'match_date_row' where it is not associated with a value\n",
      "there was no national team history cannot access local variable 'match_date_row' where it is not associated with a value\n",
      "\n",
      "30 WORKING Vargas Bolivia Bolivia vs Peru 12-Oct-12\n",
      "https://www.transfermarkt.com/christian-vargas/nationalmannschaft/spieler/89537\n",
      "https://www.transfermarkt.com/rodrigo-vargas/profil/spieler/234820\n",
      "there was no national team history cannot access local variable 'match_date_row' where it is not associated with a value\n",
      "there was no national team history cannot access local variable 'match_date_row' where it is not associated with a value\n",
      "\n",
      "242 WORKING Fernandez Cuba Cuba vs Canada 08-Jun-12\n",
      "https://www.transfermarkt.us/reysander-fernandez/nationalmannschaft/spieler/62817\n",
      "https://www.transfermarkt.us/sander-fernandez/profil/spieler/169707\n",
      "there was no national team history cannot access local variable 'match_date_row' where it is not associated with a value\n",
      "there was no national team history cannot access local variable 'match_date_row' where it is not associated with a value\n",
      "\n",
      "243 WORKING Torres Mexico Mexico vs Guyana 08-Jun-12\n",
      "https://www.transfermarkt.us/jorge-torres-nilo/profil/spieler/58642\n",
      "https://www.transfermarkt.us/erick-torres/profil/spieler/170016\n",
      "there was no national team history cannot access local variable 'match_date_row' where it is not associated with a value\n",
      "there was no national team history cannot access local variable 'match_date_row' where it is not associated with a value\n",
      "\n",
      "249 WORKING Stewart Jamaica Jamaica vs Guatemala 08-Jun-12\n",
      "https://www.transfermarkt.us/damion-stewart/profil/spieler/41185\n",
      "https://www.transfermarkt.com/tremaine-stewart/profil/spieler/217290\n",
      "there was no national team history cannot access local variable 'match_date_row' where it is not associated with a value\n",
      "there was no national team history cannot access local variable 'match_date_row' where it is not associated with a value\n",
      "\n",
      "253 WORKING Morales Guatemala Guatemala vs USA 12-Jun-12\n",
      "https://www.transfermarkt.us/ruben-morales/nationalmannschaft/spieler/242387\n",
      "https://www.transfermarkt.us/rafael-morales/profil/spieler/144316\n",
      "https://www.transfermarkt.us/erwin-morales/profil/spieler/82235\n",
      "there was no national team history cannot access local variable 'match_date_row' where it is not associated with a value\n",
      "there was no national team history cannot access local variable 'match_date_row' where it is not associated with a value\n",
      "there was no national team history cannot access local variable 'match_date_row' where it is not associated with a value\n",
      "\n",
      "254 WORKING Johnson USA Guatemala vs USA 12-Jun-12\n",
      "https://www.transfermarkt.us/sean-johnson/profil/spieler/126630\n",
      "https://www.transfermarkt.us/fabian-johnson/profil/spieler/31041\n",
      "https://www.transfermarkt.us/eddie-johnson/profil/spieler/38773\n",
      "\n",
      "257 WORKING Rodrigues Costa Rica Guyana vs Costa Rica 12-Jun-12\n",
      "https://www.transfermarkt.us/ariel-rodriguez/profil/spieler/195831\n",
      "https://www.transfermarkt.com/osvaldo-rodriguez/profil/spieler/195285\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWORKING\u001b[39m\u001b[38;5;124m'\u001b[39m, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mORIGINAL JERSEY\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNationality\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatch\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#print(i, threshold_player_match(row['ORIGINAL JERSEY'], dataset_nationality))\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m filtered_names_match_date \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_using_date_of_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthreshold_player_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mORIGINAL JERSEY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_nationality\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNationality\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# if type(filtered_names_match_date) == str:\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filtered_names_match_date \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m filtered_names_match_date \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[0;32mIn[2], line 42\u001b[0m, in \u001b[0;36mfilter_using_date_of_match\u001b[0;34m(candidate_list_of_names_input, nationality_input, matchdate_Input, SALARY_BOOLEAN)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter_using_date_of_match\u001b[39m(candidate_list_of_names_input, nationality_input, matchdate_Input, SALARY_BOOLEAN): \n\u001b[0;32m---> 42\u001b[0m     FILTERED_NAMES_USING_MATCH_DATE \u001b[38;5;241m=\u001b[39m \u001b[43mAssemblyHelpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiNameMatchDateLookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate_list_of_names_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnationality_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatchdate_Input\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m#print('in fudom', type(FILTERED_NAMES_USING_MATCH_DATE))\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m#1 MATCH\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(FILTERED_NAMES_USING_MATCH_DATE, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(FILTERED_NAMES_USING_MATCH_DATE, np\u001b[38;5;241m.\u001b[39mstr_): \u001b[38;5;66;03m#YELLOW 3\u001b[39;00m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;66;03m#print('were here', FILTERED_NAMES_USING_MATCH_DATE)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;66;03m#print('were in fudom in the right place')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;66;03m# print('made it out of using selenium')\u001b[39;00m\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;66;03m# money_thisplayer = [MONEY_FOUND_ONLINE, f'Online WAS Lookup Required - salary boolean is {SALARY_BOOLEAN}', True, False]\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/DEV ENVIRONMENT/data-science-jupyter-template-main/footy_project_market_value_rankings/create_dataset_1/AssemblyHelpers.py:1361\u001b[0m, in \u001b[0;36mmultiNameMatchDateLookup\u001b[0;34m(input_list_of_names, input_nationality, input_year_of_match)\u001b[0m\n\u001b[1;32m   1359\u001b[0m currplayer \u001b[38;5;241m=\u001b[39m playernames_testing[j]\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;66;03m#print(f'multiNameMatchDateLookup. about to find page soup. were doing {currplayer}')\u001b[39;00m\n\u001b[0;32m-> 1361\u001b[0m transfermarkt_page_soup \u001b[38;5;241m=\u001b[39m \u001b[43mfind_transfermarkt_pagesoup_player\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrplayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnatl_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[38;5;66;03m#print('this player is ', currplayer, transfermarkt_page_soup, natl_test)\u001b[39;00m\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(find_national_team_in_player_history(transfermarkt_page_soup, natl_test) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/Downloads/DEV ENVIRONMENT/data-science-jupyter-template-main/footy_project_market_value_rankings/create_dataset_1/AssemblyHelpers.py:1151\u001b[0m, in \u001b[0;36mfind_transfermarkt_pagesoup_player\u001b[0;34m(input_playername, input_nationality_string)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;66;03m#print(f\"this player is {input_playername}, URL is {find_national_team_history_URL}. lowercase is {input_playername.lower()}. transformed with my new change it's {transform_name(unidecode(input_playername.lower()))}\")\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m((input_playername\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m find_national_team_history_URL) \u001b[38;5;129;01mor\u001b[39;00m (input_playername\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m find_national_team_history_URL) \u001b[38;5;129;01mor\u001b[39;00m (transform_name(input_playername\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;129;01min\u001b[39;00m find_national_team_history_URL) \u001b[38;5;129;01mor\u001b[39;00m (transform_name(unidecode(input_playername\u001b[38;5;241m.\u001b[39mlower())) \u001b[38;5;129;01min\u001b[39;00m find_national_team_history_URL)):\n\u001b[0;32m-> 1151\u001b[0m     page_soup_history_pg \u001b[38;5;241m=\u001b[39m \u001b[43mgrab_transfer_pagesoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_national_team_history_URL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1153\u001b[0m     page_soup_history_pg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/DEV ENVIRONMENT/data-science-jupyter-template-main/footy_project_market_value_rankings/create_dataset_1/AssemblyHelpers.py:1093\u001b[0m, in \u001b[0;36mgrab_transfer_pagesoup\u001b[0;34m(input_url)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrab_transfer_pagesoup\u001b[39m(input_url):\n\u001b[1;32m   1092\u001b[0m     headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m-> 1093\u001b[0m     pageTree \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1094\u001b[0m     pageSoup_club \u001b[38;5;241m=\u001b[39m BeautifulSoup (pageTree\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pageSoup_club\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1411\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1410\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1411\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1412\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1413\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:324\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:285\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 285\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1249\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1246\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1247\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1248\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1105\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1107\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "success_status_counter = 0\n",
    "cases_2011_12 = 0\n",
    "done_cases = 0\n",
    "zero_cases = 0\n",
    "multi_cases = 0\n",
    "for i in range(0, 438):\n",
    "    index = i\n",
    "    row = match_date_filter_said_0_guys.loc[index]\n",
    "    if row['Status New'] == 'Done' or row['Status New'] == 'Done-OLD':\n",
    "        done_cases += 1\n",
    "    elif row['Season'] <= 2012:\n",
    "        cases_2011_12 += 1\n",
    "        nationality = row['Nationality']\n",
    "        this_country_code = row['Team Country Code']\n",
    "        if pd.isna(this_country_code):\n",
    "            country_name = 'Namibia'\n",
    "        else:\n",
    "            if this_country_code == 'BVI':\n",
    "                country_name = 'British Virgin Islands'\n",
    "            else:\n",
    "                country_name = countries_codes[countries_codes[' Code'] == this_country_code]['Country'].unique()[0]\n",
    "        dataset_nationality = leagues_value_old[leagues_value_old['Team 1 Code'] == this_country_code]['Name'].unique()\n",
    "        print(i, 'WORKING', row['ORIGINAL JERSEY'], row['Nationality'], row['Match'], row['Date'])\n",
    "        #print(i, threshold_player_match(row['ORIGINAL JERSEY'], dataset_nationality))\n",
    "\n",
    "        filtered_names_match_date = filter_using_date_of_match(threshold_player_match(row['ORIGINAL JERSEY'], dataset_nationality)[0], row['Nationality'], row['Date'], False)[1]\n",
    "        # if type(filtered_names_match_date) == str:\n",
    "        if filtered_names_match_date == '' or filtered_names_match_date == \" \":\n",
    "            if len(threshold_player_match(row['ORIGINAL JERSEY'], dataset_nationality)[0]) == 1:\n",
    "                print(i, f'noboddy found, list was{threshold_player_match(row['ORIGINAL JERSEY'], dataset_nationality)[0]}')\n",
    "                match_date_filter_said_0_guys.at[i, 'New Found Name'] = threshold_player_match(row['ORIGINAL JERSEY'], dataset_nationality)[0]\n",
    "                match_date_filter_said_0_guys.at[i, 'Status New'] = 'Done-OLD'\n",
    "            else:\n",
    "                print()\n",
    "        else:\n",
    "            print(i, f'FILTERED intersection of lists is says{filtered_names_match_date}')\n",
    "            match_date_filter_said_0_guys.at[i, 'New Found Name'] = filtered_names_match_date\n",
    "            match_date_filter_said_0_guys.at[i, 'Status New'] = 'Done-OLD'\n",
    "    else:\n",
    "        match = row['Match']\n",
    "        date = row['Date']\n",
    "\n",
    "        ORIGINAL_JERSEY = row['ORIGINAL JERSEY']\n",
    "        if ORIGINAL_JERSEY == 'Ahmad Almahaijri':\n",
    "            ORIGINAL_JERSEY = 'Ahmed Kano'\n",
    "        elif ORIGINAL_JERSEY == 'Abdullah Ammar':\n",
    "            ORIGINAL_JERSEY = 'Abdullah Al-Ammar'\n",
    "        elif ORIGINAL_JERSEY == 'Johnson':\n",
    "            if row['Nationality'] == 'USA':\n",
    "                ORIGINAL_JERSEY = 'Fabian Johnson'\n",
    "        elif ORIGINAL_JERSEY == 'Jones':\n",
    "            ORIGINAL_JERSEY = 'Jermaine Jones'\n",
    "        elif ORIGINAL_JERSEY == 'Gonzalez':\n",
    "            ORIGINAL_JERSEY = 'Omar Gonzalez'\n",
    "        elif ORIGINAL_JERSEY == 'inh Tin Thành':\n",
    "            ORIGINAL_JERSEY = 'Thanh Tin Than'\n",
    "        elif ORIGINAL_JERSEY == 'A Traoré':\n",
    "            if i == 55 or i == 56:\n",
    "                ORIGINAL_JERSEY = 'Adama Traoré'\n",
    "        elif ORIGINAL_JERSEY == 'C Lei':\n",
    "            ORIGINAL_JERSEY = 'Cheng Lam Lei'\n",
    "        elif ORIGINAL_JERSEY == 'E García':\n",
    "            ORIGINAL_JERSEY = 'Gabriel García'\n",
    "        elif ORIGINAL_JERSEY == 'O`Shea':\n",
    "            ORIGINAL_JERSEY = 'John O\\'Shea'\n",
    "        elif ORIGINAL_JERSEY == 'Lobjanidze':\n",
    "            ORIGINAL_JERSEY = 'Ucha Lobjanidze'\n",
    "        elif ORIGINAL_JERSEY == 'Alfonso González':\n",
    "            ORIGINAL_JERSEY = 'Arturo González'\n",
    "        elif ORIGINAL_JERSEY == 'Robinson':\n",
    "            ORIGINAL_JERSEY = 'Antonee Robinson'\n",
    "        elif ORIGINAL_JERSEY == 'Mc Kennie':\n",
    "            ORIGINAL_JERSEY = 'Weston Mckennie'\n",
    "        elif ORIGINAL_JERSEY == 'Aaronson':\n",
    "            ORIGINAL_JERSEY = 'Brendan Aaronson'\n",
    "        nationality = row['Nationality']\n",
    "        this_country_code = row['Team Country Code']\n",
    "        if pd.isna(this_country_code):\n",
    "            country_name = 'Namibia'\n",
    "        else:\n",
    "            if this_country_code == 'BVI':\n",
    "                country_name = 'British Virgin Islands'\n",
    "            else:\n",
    "                country_name = countries_codes[countries_codes[' Code'] == this_country_code]['Country'].unique()[0]\n",
    "        dataset_nationality = leagues_value[leagues_value['Team 1 Code'] == this_country_code]['Name'].unique()\n",
    "\n",
    "        query_text = f\"{ORIGINAL_JERSEY} {nationality} transfermarkt\"\n",
    "        names_transfermarkt_search = process_google_query(query_text)\n",
    "        correct_names = []\n",
    "        for name_option in names_transfermarkt_search:\n",
    "            if 'Club' in name_option or 'League' in name_option:\n",
    "                names_transfermarkt_search.remove(name_option)\n",
    "        for name_option in names_transfermarkt_search:\n",
    "            if (nationality in leagues_value[leagues_value['Name'] == name_option]['Nationality'].unique()):\n",
    "                correct_names.append(name_option)\n",
    "        if len(correct_names) == 1:\n",
    "            match_date_filter_said_0_guys.at[i, 'Status New'] = 'Done'\n",
    "            match_date_filter_said_0_guys.at[i, 'Name(s) Found'] = correct_names[0]\n",
    "            \n",
    "            print(i, 'WORKING', ORIGINAL_JERSEY, correct_names, names_transfermarkt_search)\n",
    "            success_status_counter += 1\n",
    "        else:\n",
    "            if correct_names != []:\n",
    "                #remaining_match = threshold_player_match(ORIGINAL_JERSEY, correct_names)[0]\n",
    "                closest_name = filter_multi_word_matches_by_jersey_tokens([token[0] for token in ORIGINAL_JERSEY.split(' ')], correct_names)\n",
    "            else:\n",
    "                closest_name = filter_multi_word_matches_by_jersey_tokens([token[0] for token in ORIGINAL_JERSEY.split(' ')], names_transfermarkt_search)\n",
    "            if len(closest_name) == 1:\n",
    "                match_date_filter_said_0_guys.at[i, 'Status New'] = 'Done'\n",
    "                match_date_filter_said_0_guys.at[i, 'Name(s) Found'] = closest_name[0]\n",
    "                print(i, 'WORKING', ORIGINAL_JERSEY, closest_name, correct_names, names_transfermarkt_search)\n",
    "                success_status_counter += 1\n",
    "            else:\n",
    "                filtered_names_match_date = filter_using_date_of_match(closest_name, row['Nationality'], date, False)[1]\n",
    "                if type(filtered_names_match_date) == str:\n",
    "                    if filtered_names_match_date == '':\n",
    "                        \n",
    "                        dataset_nationality_large = leagues_value_large[leagues_value_large['Team 1 Code'] == this_country_code]['Name'].unique()\n",
    "                        list_large_set = threshold_player_match(ORIGINAL_JERSEY, dataset_nationality_large)[0]\n",
    "                        if len(list_large_set) >= 1:\n",
    "                            filtered_names_match_date_ii = filter_using_date_of_match(list_large_set, row['Nationality'], date, False)[1]\n",
    "                            if type(filtered_names_match_date_ii) == str:\n",
    "                                if filtered_names_match_date_ii == '':\n",
    "                                    print(i, 'NOBODY REMAINING', nationality, match, date, ORIGINAL_JERSEY, filtered_names_match_date_ii, correct_names, names_transfermarkt_search)\n",
    "                                    zero_cases += 1\n",
    "                                elif filtered_names_match_date_ii.startswith('['):\n",
    "                                    print(i, 'MULTIPLE REMAINING', ORIGINAL_JERSEY, filtered_names_match_date_ii, correct_names, names_transfermarkt_search)\n",
    "                                    multi_cases += 1\n",
    "                                else:\n",
    "                                    print(i, 'ONE REMAINING', ORIGINAL_JERSEY, filtered_names_match_date_ii, correct_names, names_transfermarkt_search)\n",
    "                                    match_date_filter_said_0_guys.at[i, 'Status New'] = 'Done'\n",
    "                                    match_date_filter_said_0_guys.at[i, 'Name(s) Found'] = filtered_names_match_date_ii\n",
    "                                    success_status_counter += 1\n",
    "                                                            \n",
    "                    elif filtered_names_match_date.startswith('['):\n",
    "                        print(i, 'MULTIPLE REMAINING', ORIGINAL_JERSEY, filtered_names_match_date, correct_names, names_transfermarkt_search)\n",
    "                        multi_cases += 1\n",
    "                    else:\n",
    "                        print(i, 'ONE REMAINING', ORIGINAL_JERSEY, filtered_names_match_date, correct_names, names_transfermarkt_search)\n",
    "                        match_date_filter_said_0_guys.at[i, 'Status New'] = 'Done'\n",
    "                        match_date_filter_said_0_guys.at[i, 'Name(s) Found'] = filtered_names_match_date\n",
    "                        success_status_counter += 1\n",
    "\n",
    "                else:\n",
    "                    print(i, 'MULTIPLE REMAINING', ORIGINAL_JERSEY, filtered_names_match_date, correct_names, names_transfermarkt_search)\n",
    "                    multi_cases += 1\n",
    "print(done_cases, cases_2011_12, success_status_counter, multi_cases, zero_cases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_date_filter_said_0_guys.at[432, 'Name(s) Found'] = 'Mamy Gervais'\n",
    "\n",
    "\n",
    "\n",
    "match_date_filter_said_0_guys.at[402, 'Name(s) Found'] = 'Shane Long'\n",
    "match_date_filter_said_0_guys.at[402, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[406, 'Name(s) Found'] = 'Shane Long'\n",
    "match_date_filter_said_0_guys.at[406, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[411, 'Name(s) Found'] = 'Shane Long'\n",
    "match_date_filter_said_0_guys.at[411, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[412, 'Name(s) Found'] = 'Shane Long'\n",
    "match_date_filter_said_0_guys.at[412, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[413, 'Name(s) Found'] = 'Daryl Murphy'\n",
    "match_date_filter_said_0_guys.at[413, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[403, 'Name(s) Found'] = 'Daryl Murphy'\n",
    "match_date_filter_said_0_guys.at[403, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[392, 'Name(s) Found'] = 'Jordan Morris'\n",
    "match_date_filter_said_0_guys.at[392, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[296, 'Name(s) Found'] = 'Jermaine Johnson'\n",
    "match_date_filter_said_0_guys.at[296, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[299, 'Name(s) Found'] = 'Pa Modou'\n",
    "match_date_filter_said_0_guys.at[299, 'Status New'] = 'Done'\n",
    "\n",
    "\n",
    "match_date_filter_said_0_guys.at[225, 'Status New'] = 'Done'\n",
    "match_date_filter_said_0_guys.at[225, 'Name(s) Found'] = 'Jermaine Beckford'\n",
    "\n",
    "match_date_filter_said_0_guys.at[293, 'Status New'] = 'Done'\n",
    "match_date_filter_said_0_guys.at[293, 'Name(s) Found'] = 'Jermaine Beckford'\n",
    "\n",
    "match_date_filter_said_0_guys.at[292, 'Status New'] = 'Done'\n",
    "match_date_filter_said_0_guys.at[292, 'Name(s) Found'] = 'Jermaine Beckford'\n",
    "\n",
    "\n",
    "match_date_filter_said_0_guys.at[291, 'Status New'] = 'Done'\n",
    "match_date_filter_said_0_guys.at[291, 'Name(s) Found'] = 'Hérculez Gómez'\n",
    "\n",
    "match_date_filter_said_0_guys.at[295, 'Status New'] = 'Done'\n",
    "match_date_filter_said_0_guys.at[295, 'Name(s) Found'] = 'Hérculez Gómez'\n",
    "\n",
    "\n",
    "#293 is Jermaine Beckford, Done\n",
    "#292 is Jermaine Beckford, Done\n",
    "#295 is Hérculez Gómez, Done\n",
    "#291 is Hérculez Gómez, Done\n",
    "\n",
    "\n",
    "match_date_filter_said_0_guys.at[0, 'Status New'] = 'Done'\n",
    "match_date_filter_said_0_guys.at[0, 'Name(s) Found'] = 'Arda Turan'\n",
    "\n",
    "match_date_filter_said_0_guys.at[15, 'Status New'] = 'Done'\n",
    "match_date_filter_said_0_guys.at[15, 'Name(s) Found'] = 'Abdullah Al-Ammar'\n",
    "\n",
    "match_date_filter_said_0_guys.at[14, 'Name(s) Found'] = 'Tuan Tai Phan'\n",
    "\n",
    "match_date_filter_said_0_guys.at[43, 'Name(s) Found'] = 'Ablie Jallow'\n",
    "match_date_filter_said_0_guys.at[43, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[47, 'Name(s) Found'] = 'Ablie Jallow'\n",
    "match_date_filter_said_0_guys.at[47, 'Status New'] = 'Done'\n",
    "\n",
    "\n",
    "match_date_filter_said_0_guys.at[44, 'Name(s) Found'] = 'Musa Barrow'\n",
    "match_date_filter_said_0_guys.at[44, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[48, 'Name(s) Found'] = 'Musa Barrow'\n",
    "match_date_filter_said_0_guys.at[48, 'Status New'] = 'Done'\n",
    "\n",
    "\n",
    "match_date_filter_said_0_guys.at[45, 'Name(s) Found'] = 'Demba Kamara'\n",
    "match_date_filter_said_0_guys.at[45, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[46, 'Name(s) Found'] = 'Bubacarr Sanneh'\n",
    "match_date_filter_said_0_guys.at[46, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[55, 'Name(s) Found'] = 'Adama Traoré'\n",
    "match_date_filter_said_0_guys.at[55, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[56, 'Name(s) Found'] = 'Adama Traoré'\n",
    "match_date_filter_said_0_guys.at[56, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[199, 'Name(s) Found'] = 'Adama Traoré'\n",
    "match_date_filter_said_0_guys.at[199, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[69, 'Name(s) Found'] = 'Musa Barrow'\n",
    "match_date_filter_said_0_guys.at[69, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[119, 'Name(s) Found'] = 'Mamy Gervais'\n",
    "match_date_filter_said_0_guys.at[119, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[68, 'Name(s) Found'] = 'Bubacarr Sanneh'\n",
    "match_date_filter_said_0_guys.at[68, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[70, 'Name(s) Found'] = 'Nito'\n",
    "match_date_filter_said_0_guys.at[70, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[71, 'Name(s) Found'] = 'Raúl Fernández'\n",
    "match_date_filter_said_0_guys.at[71, 'Status New'] = 'Done'\n",
    "match_date_filter_said_0_guys.at[71, 'Nationality'] = 'Peru'\n",
    "match_date_filter_said_0_guys.at[71, 'Team Country Code'] = 'PE'\n",
    "\n",
    "match_date_filter_said_0_guys.at[94, 'Name(s) Found'] = 'Daryl Murphy'\n",
    "match_date_filter_said_0_guys.at[94, 'Status New'] = 'Done'\n",
    "\n",
    "\n",
    "\n",
    "match_date_filter_said_0_guys.at[117, 'Name(s) Found'] = 'Mohamed Abuaagla'\n",
    "match_date_filter_said_0_guys.at[117, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[116, 'Name(s) Found'] = 'Machawe Dlamini'\n",
    "match_date_filter_said_0_guys.at[116, 'Status New'] = 'Done'\n",
    "\n",
    "#43 Ablie Jallow\n",
    "#44 Musa Barrow\n",
    "#45 is Demba Kamara\n",
    "#46 Bubacarr Sanneh\n",
    "#47 Ablie Jallow\n",
    "#48 Musa Barrow\n",
    "\n",
    "match_date_filter_said_0_guys.at[122, 'Name(s) Found'] = 'Kevin Long'\n",
    "match_date_filter_said_0_guys.at[122, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[124, 'Name(s) Found'] = 'Kevin Long'\n",
    "match_date_filter_said_0_guys.at[124, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[126, 'Name(s) Found'] = 'Kevin Long'\n",
    "match_date_filter_said_0_guys.at[126, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[123, 'Name(s) Found'] = 'Shane Long'\n",
    "match_date_filter_said_0_guys.at[123, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[125, 'Name(s) Found'] = 'Aiden O\\'Brien'\n",
    "match_date_filter_said_0_guys.at[125, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[127, 'Name(s) Found'] = 'Aiden O\\'Brien'\n",
    "match_date_filter_said_0_guys.at[127, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[128, 'Name(s) Found'] = 'Arda Turan'\n",
    "match_date_filter_said_0_guys.at[128, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[153, 'Name(s) Found'] = 'Jordan Morris'\n",
    "match_date_filter_said_0_guys.at[153, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[178, 'Name(s) Found'] = 'Bùi Tấn Trường'\n",
    "match_date_filter_said_0_guys.at[178, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[172, 'Name(s) Found'] = 'Bùi Tấn Trường'\n",
    "match_date_filter_said_0_guys.at[174, 'Name(s) Found'] = 'Bùi Tấn Trường'\n",
    "match_date_filter_said_0_guys.at[175, 'Name(s) Found'] = 'Bùi Tấn Trường'\n",
    "\n",
    "match_date_filter_said_0_guys.at[177, 'Name(s) Found'] = 'Khalid Ali Al-Hajjaji'\n",
    "\n",
    "match_date_filter_said_0_guys.at[162, 'Name(s) Found'] = 'Arda Turan'\n",
    "match_date_filter_said_0_guys.at[162, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[189, 'Name(s) Found'] = 'Angel Cardozo Lucena'\n",
    "\n",
    "match_date_filter_said_0_guys.at[198, 'Name(s) Found'] = 'Arda Turan'\n",
    "match_date_filter_said_0_guys.at[198, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[182, 'Name(s) Found'] = 'Momodou Ceesay'\n",
    "match_date_filter_said_0_guys.at[182, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[196, 'Name(s) Found'] = 'Momodou Ceesay'\n",
    "match_date_filter_said_0_guys.at[196, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[181, 'Name(s) Found'] = 'Sanna Nyassi'\n",
    "match_date_filter_said_0_guys.at[181, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[180, 'Name(s) Found'] = 'Pa Modou'\n",
    "match_date_filter_said_0_guys.at[180, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[183, 'Name(s) Found'] = 'José María Giménez'\n",
    "match_date_filter_said_0_guys.at[183, 'Status New'] = 'Done'\n",
    "match_date_filter_said_0_guys.at[183, 'Nationality'] = 'Uruguay'\n",
    "match_date_filter_said_0_guys.at[183, 'Team Country Code'] = 'UY'\n",
    "\n",
    "match_date_filter_said_0_guys.at[195, 'Name(s) Found'] = 'Bubacarr Sanneh'\n",
    "match_date_filter_said_0_guys.at[195, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[202, 'Name(s) Found'] = 'Bùi Tấn Trường'\n",
    "match_date_filter_said_0_guys.at[206, 'Name(s) Found'] = 'Bùi Tấn Trường'\n",
    "match_date_filter_said_0_guys.at[206, 'Name(s) Found'] = 'Mohamed Fasal'\n",
    "match_date_filter_said_0_guys.at[200, 'Name(s) Found'] = 'Abdoulaye Diallo'\n",
    "\n",
    "match_date_filter_said_0_guys.at[223, 'Name(s) Found'] = 'Jermaine Johnson'\n",
    "match_date_filter_said_0_guys.at[224, 'Name(s) Found'] = 'Ryan Johnson'\n",
    "match_date_filter_said_0_guys.at[227, 'Name(s) Found'] = 'Eddie Johnson'\n",
    "match_date_filter_said_0_guys.at[230, 'Name(s) Found'] = 'Eddie Johnson'\n",
    "match_date_filter_said_0_guys.at[236, 'Name(s) Found'] = 'Eddie Johnson'\n",
    "\n",
    "match_date_filter_said_0_guys.at[241, 'Name(s) Found'] = 'Jermaine Johnson'\n",
    "\n",
    "match_date_filter_said_0_guys.at[302, 'Name(s) Found'] = 'Pa Modou'\n",
    "match_date_filter_said_0_guys.at[302, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[305, 'Name(s) Found'] = 'Pa Modou'\n",
    "match_date_filter_said_0_guys.at[305, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[309, 'Name(s) Found'] = 'Pa Modou'\n",
    "match_date_filter_said_0_guys.at[309, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[304, 'Name(s) Found'] = 'Ablie Jallow'\n",
    "match_date_filter_said_0_guys.at[304, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[306, 'Name(s) Found'] = 'Ablie Jallow'\n",
    "match_date_filter_said_0_guys.at[306, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[317, 'Name(s) Found'] = 'Adama Traoré'\n",
    "match_date_filter_said_0_guys.at[317, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[300, 'Name(s) Found'] = 'Musa Barrow'\n",
    "match_date_filter_said_0_guys.at[300, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[303, 'Name(s) Found'] = 'Musa Barrow'\n",
    "match_date_filter_said_0_guys.at[303, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[307, 'Name(s) Found'] = 'Musa Barrow'\n",
    "match_date_filter_said_0_guys.at[307, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[308, 'Name(s) Found'] = 'Modou Barrow'\n",
    "match_date_filter_said_0_guys.at[308, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[311, 'Name(s) Found'] = 'Musa Barrow'\n",
    "match_date_filter_said_0_guys.at[311, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[310, 'Name(s) Found'] = 'Modou Barrow'\n",
    "match_date_filter_said_0_guys.at[310, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[319, 'Name(s) Found'] = 'Mamy Gervais'\n",
    "match_date_filter_said_0_guys.at[319, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[312, 'Name(s) Found'] = 'Ablie Jallow'\n",
    "match_date_filter_said_0_guys.at[312, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[313, 'Name(s) Found'] = 'Modou Barrow'\n",
    "match_date_filter_said_0_guys.at[313, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[315, 'Name(s) Found'] = 'Pa Modou'\n",
    "match_date_filter_said_0_guys.at[315, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[318, 'Name(s) Found'] = 'Mohamed Abuaagla'\n",
    "match_date_filter_said_0_guys.at[318, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[316, 'Name(s) Found'] = 'Momodou Ceesay'\n",
    "match_date_filter_said_0_guys.at[316, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[324, 'Name(s) Found'] = 'Arda Turan'\n",
    "match_date_filter_said_0_guys.at[324, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[330, 'Name(s) Found'] = 'Arda Turan'\n",
    "match_date_filter_said_0_guys.at[330, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[332, 'Name(s) Found'] = 'Arda Turan'\n",
    "match_date_filter_said_0_guys.at[332, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[335, 'Name(s) Found'] = 'Arda Turan'\n",
    "match_date_filter_said_0_guys.at[335, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[325, 'Name(s) Found'] = 'Kevin Long'\n",
    "match_date_filter_said_0_guys.at[325, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[326, 'Name(s) Found'] = 'Shane Long'\n",
    "match_date_filter_said_0_guys.at[326, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[327, 'Name(s) Found'] = 'Kevin Long'\n",
    "match_date_filter_said_0_guys.at[327, 'Status New'] = 'Done'\n",
    "match_date_filter_said_0_guys.at[328, 'Name(s) Found'] = 'Kevin Long'\n",
    "match_date_filter_said_0_guys.at[328, 'Status New'] = 'Done'\n",
    "\n",
    "match_date_filter_said_0_guys.at[336, 'Name(s) Found'] = 'Alexander González'\n",
    "match_date_filter_said_0_guys.at[336, 'Status New'] = 'Done'\n",
    "match_date_filter_said_0_guys.at[337, 'Name(s) Found'] = 'Alexander González'\n",
    "match_date_filter_said_0_guys.at[337, 'Status New'] = 'Done'\n",
    "match_date_filter_said_0_guys.at[338, 'Name(s) Found'] = 'Alexander González'\n",
    "match_date_filter_said_0_guys.at[338, 'Status New'] = 'Done'\n",
    "match_date_filter_said_0_guys.at[339, 'Name(s) Found'] = 'Alexander González'\n",
    "match_date_filter_said_0_guys.at[339, 'Status New'] = 'Done'\n",
    "\n",
    "\n",
    "match_date_filter_said_0_guys.at[340, 'Name(s) Found'] = 'James Rodriguez'\n",
    "match_date_filter_said_0_guys.at[340, 'Nationality'] = 'Colombia'\n",
    "match_date_filter_said_0_guys.at[340, 'Team Country Code'] = 'CO'\n",
    "\n",
    "match_date_filter_said_0_guys.at[350, 'Name(s) Found'] = 'Mohamed Fasal'\n",
    "match_date_filter_said_0_guys.at[354, 'Name(s) Found'] = 'Khalid Ali Al Hajjaji'\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, 438):\n",
    "    if match_date_filter_said_0_guys.at[i, 'ORIGINAL JERSEY'] == 'Luis Puma Rodríguez':\n",
    "        match_date_filter_said_0_guys.at[i, 'Status New'] = 'Done'\n",
    "        match_date_filter_said_0_guys.at[i, 'Name(s) Found'] = 'Puma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leagues_value_large[leagues_value_large['Name'] == 'Mamy Gervais']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#247 Wilson Palacios Honduras HN\n",
    "match_date_filter_said_0_guys.at[247, 'New Found Name'] = 'Wilson Palacios'\n",
    "match_date_filter_said_0_guys.at[247, 'Nationality'] = 'Honduras'\n",
    "match_date_filter_said_0_guys.at[247, 'Team Country Code'] = 'HN'\n",
    "#248 Isidro Gutierrez\n",
    "match_date_filter_said_0_guys.at[248, 'New Found Name'] = 'Isidro Gutierrez'\n",
    "match_date_filter_said_0_guys.at[248, 'Nationality'] = 'El Salvador'\n",
    "match_date_filter_said_0_guys.at[248, 'Team Country Code'] = 'SV'\n",
    "#250, Mexico, MX, Francisco Rodriguez\n",
    "match_date_filter_said_0_guys.at[250, 'New Found Name'] = 'Francisco Rodríguez'\n",
    "match_date_filter_said_0_guys.at[250, 'Nationality'] = 'El Salvador'\n",
    "match_date_filter_said_0_guys.at[250, 'Team Country Code'] = 'SV'\n",
    "#251 Mexico MX Jorge Torres Nilo\n",
    "match_date_filter_said_0_guys.at[251, 'New Found Name'] = 'Jorge Torres Nilo'\n",
    "match_date_filter_said_0_guys.at[251, 'Nationality'] = 'Mexico'\n",
    "match_date_filter_said_0_guys.at[251, 'Team Country Code'] = 'MX'\n",
    "#252 El Salvador SV Ramón Sánchez\n",
    "match_date_filter_said_0_guys.at[252, 'New Found Name'] = 'Ramón Sánchez'\n",
    "match_date_filter_said_0_guys.at[252, 'Nationality'] = 'El Salvador'\n",
    "match_date_filter_said_0_guys.at[252, 'Team Country Code'] = 'SV'\n",
    "#258 Gregory Richardson Guyana GY\n",
    "match_date_filter_said_0_guys.at[250, 'New Found Name'] = 'Gregory Richardson'\n",
    "match_date_filter_said_0_guys.at[250, 'Nationality'] = 'Guyana'\n",
    "match_date_filter_said_0_guys.at[250, 'Team Country Code'] = 'GY'\n",
    "#280, Mexico, MX, Francisco Rodriguez\n",
    "match_date_filter_said_0_guys.at[280, 'New Found Name'] = 'Francisco Rodríguez'\n",
    "match_date_filter_said_0_guys.at[280, 'Nationality'] = 'El Salvador'\n",
    "match_date_filter_said_0_guys.at[280, 'Team Country Code'] = 'SV'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(match_date_filter_said_0_guys)):\n",
    "    if match_date_filter_said_0_guys.at[i, 'Status New'] != 'Done':\n",
    "        if match_date_filter_said_0_guys.at[i, 'ORIGINAL JERSEY'] == 'Vargas':\n",
    "            match_date_filter_said_0_guys.at[i, 'New Found Name'] = 'Christian Vargas'\n",
    "            match_date_filter_said_0_guys.at[i, 'Status New'] = 'Done'\n",
    "        elif match_date_filter_said_0_guys.at[i, 'ORIGINAL JERSEY'] == 'Cummings':\n",
    "            match_date_filter_said_0_guys.at[i, 'New Found Name'] = 'Shaun Cummings'\n",
    "            match_date_filter_said_0_guys.at[i, 'Status New'] = 'Done'\n",
    "        elif match_date_filter_said_0_guys.at[i, 'ORIGINAL JERSEY'] == 'Saucedo':\n",
    "            match_date_filter_said_0_guys.at[i, 'New Found Name'] = 'Mauricio Saucedo'\n",
    "            match_date_filter_said_0_guys.at[i, 'Status New'] = 'Done'\n",
    "        elif match_date_filter_said_0_guys.at[i, 'ORIGINAL JERSEY'] == 'Johnson' and match_date_filter_said_0_guys.at[i, 'Nationality'] == 'USA':\n",
    "            match_date_filter_said_0_guys.at[i, 'New Found Name'] = 'Fabian Johnson'\n",
    "            match_date_filter_said_0_guys.at[i, 'Status New'] = 'Done'\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with \"large dataset said 0\" people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_strings_equal_with_rotation(str1, str2):\n",
    "    # Split strings into tokens, convert to lowercase, and remove hyphens\n",
    "    tokens1 = [token.lower().replace('-', '') for token in str1.split()]\n",
    "    tokens2 = [token.lower().replace('-', '') for token in str2.split()]\n",
    "\n",
    "    # Rotate tokens of the second string\n",
    "    rotated_tokens2 = tokens2[-1:] + tokens2[:-1]\n",
    "\n",
    "    # Compare token lists\n",
    "    return tokens1 == rotated_tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_dataset_said_0 = load_csv_dataset('CSVs of edge cases - for F/\"other\" group/updated_large_dataset_said_0.csv')\n",
    "large_dataset_said_0['Status New'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0.2                                             394\n",
       "Unnamed: 0.1                                           53952\n",
       "Unnamed: 0                                                52\n",
       "Match ID                                                 557\n",
       "Competition                           Euros 2016, Qualifiers\n",
       "Match                                Liechtenstein vs Sweden\n",
       "Date                                                09.10.15\n",
       "Nationality                                    Liechtenstein\n",
       "Team Country Code                                         LI\n",
       "Season                                                  2015\n",
       "Status                                               SUCCESS\n",
       "Name                      ['Martin Büchel', 'Marcel Büchel']\n",
       "Market Value                                             0.0\n",
       "Lookup Still Required?                                 False\n",
       "Lookup Return Case                      Large Dataset said 0\n",
       "Impute Required?                                       False\n",
       "Name(s) Found                                  Martin Büchel\n",
       "ORIGINAL JERSEY                                   M-n Buchel\n",
       "Match Case                                          multiple\n",
       "Status New                                                  \n",
       "Name: 394, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Glenson Prince', 'Hubert Prince', 'Joslyn Prince', 'Joel Prince']\n",
      "['Glenson Prince', 'Hubert Prince', 'Joslyn Prince', 'Joel Prince']\n",
      "['Glenson Prince', 'Hubert Prince', 'Joslyn Prince', 'Joel Prince']\n",
      "['Glenson Prince', 'Hubert Prince', 'Joslyn Prince', 'Joel Prince']\n",
      "480 Prince ['Glenson Prince', 'Hubert Prince', 'Joslyn Prince', 'Joel Prince'] Dominica vs Panama 28-Mar-21\n",
      "reverse loop. there was no national team history time data '28/03/21' does not match format '%m/%d/%y'\n",
      "reverse loop. there was no national team history time data '28/03/21' does not match format '%m/%d/%y'\n",
      "reverse loop. there was no national team history time data '28/03/21' does not match format '%m/%d/%y'\n",
      "480 WORKING Prince Glenson Prince\n",
      "['Dilan De Silva', 'Dillon De Silva']\n",
      "['Dilan De Silva', 'Dillon De Silva']\n",
      "481 D De Silva ['Dilan De Silva', 'Dillon De Silva'] Sri Lanka vs Lebanon 15-10-2019\n",
      "reverse loop. there was no national team history time data '15/10/19' does not match format '%m/%d/%y'\n",
      "reverse loop. there was no national team history time data '15/10/19' does not match format '%m/%d/%y'\n",
      "481 not working nobody D De Silva \n",
      "['Dilan De Silva', 'Dillon De Silva']\n",
      "['Dilan De Silva', 'Dillon De Silva']\n",
      "482 D De Silva ['Dilan De Silva', 'Dillon De Silva'] Sri Lanka vs North Korea 10-09-2019\n",
      "482 not working nobody D De Silva \n",
      "483 WORKING An Tae-Song Tae-song An\n",
      "Tae-song An\n",
      "['Faisal Ajab', 'Faisal Al Enezi']\n",
      "['Faisal Ajab', 'Faisal Al Enezi']\n",
      "484 Faisal Al Harbi ['Faisal Ajab', 'Faisal Al Enezi'] Kuwait vs Australia 10-9-2019\n",
      "484 WORKING Faisal Al Harbi Faisal Ajab\n",
      "485 WORKING Sos Souhana Souhana Sos\n",
      "Souhana Sos\n",
      "['Faisal Ajab', 'Faisal Al Enezi']\n",
      "['Faisal Ajab', 'Faisal Al Enezi']\n",
      "486 Faisal Al Harbi ['Faisal Ajab', 'Faisal Al Enezi'] Jordan vs Kuwait 10-10-2019\n",
      "486 not working nobody Faisal Al Harbi \n",
      "487 WORKING Sos Souhana Souhana Sos\n",
      "Souhana Sos\n",
      "['Sokheang Chea', 'Meng Chheng', 'Pok Chanthan']\n",
      "['Sokheang Chea', 'Meng Chheng', 'Pok Chanthan']\n",
      "['Sokheang Chea', 'Meng Chheng', 'Pok Chanthan']\n",
      "488 Sieng Chanthea ['Sokheang Chea', 'Meng Chheng', 'Pok Chanthan'] Bahrain vs Cambodia 03-06-2021\n",
      "488 WORKING Sieng Chanthea Meng Chheng\n",
      "['Duncan Ochieng', 'David Ochieng']\n",
      "['Duncan Ochieng', 'David Ochieng']\n",
      "489 D Ochieng ['Duncan Ochieng', 'David Ochieng'] Kenya vs Rwanda 15-11-2021\n",
      "reverse loop. there was no national team history time data '15/11/21' does not match format '%m/%d/%y'\n",
      "489 WORKING D Ochieng David Ochieng\n",
      "['Ishmeal Koroma', 'Ibrahim Koroma']\n",
      "['Ishmeal Koroma', 'Ibrahim Koroma']\n",
      "490 I Koroma ['Ishmeal Koroma', 'Ibrahim Koroma'] Sierra Leone vs Liberia 08-09-2019\n",
      "490 WORKING I Koroma Ishmeal Koroma\n",
      "['Mohamed Kamara', 'Musa Kamara']\n",
      "['Mohamed Kamara', 'Musa Kamara']\n",
      "491 M Kamara ['Mohamed Kamara', 'Musa Kamara'] Sierra Leone vs Liberia 08-09-2019\n",
      "491 WORKING M Kamara Musa Kamara\n",
      "['Ahmed Eid', 'Ahmed Ali']\n",
      "['Ahmed Eid', 'Ahmed Ali']\n",
      "492 Ahmed Zizo ['Ahmed Eid', 'Ahmed Ali'] Egypt vs Angola 01-09-2021\n",
      "492 WORKING Ahmed Zizo Ahmed Ali\n",
      "['Mohamed Ali Mohamed', 'Mohamed Abukar Mohamed']\n",
      "['Mohamed Ali Mohamed', 'Mohamed Abukar Mohamed']\n",
      "493 M Ali Mohamed ['Mohamed Ali Mohamed', 'Mohamed Abukar Mohamed'] Somalia vs Zimbabwe 05-09-2019\n",
      "493 not working nobody M Ali Mohamed \n",
      "['Mohamed Bourhan Mohamed', 'Moussa Helem Mohamed', 'Mohamed Fouad Mohamed']\n",
      "['Mohamed Bourhan Mohamed', 'Moussa Helem Mohamed', 'Mohamed Fouad Mohamed']\n",
      "['Mohamed Bourhan Mohamed', 'Moussa Helem Mohamed', 'Mohamed Fouad Mohamed']\n",
      "494 M Mohamed ['Mohamed Bourhan Mohamed', 'Moussa Helem Mohamed', 'Mohamed Fouad Mohamed'] Djibouti vs Burkina Faso 08-10-2021\n",
      "['Mohamed Bourhan Mohamed', 'Mohamed Fouad Mohamed'] <class 'list'> False\n",
      "494 not working list M Mohamed ['Mohamed Bourhan Mohamed', 'Mohamed Fouad Mohamed']\n",
      "['Issiaka Ouédraogo', 'Ismahila Ouédraogo']\n",
      "['Issiaka Ouédraogo', 'Ismahila Ouédraogo']\n",
      "495 I Ouédraogo ['Issiaka Ouédraogo', 'Ismahila Ouédraogo'] Djibouti vs Burkina Faso 08-10-2021\n",
      "495 WORKING I Ouédraogo Ismahila Ouédraogo\n",
      "['Mohamed Bourhan Mohamed', 'Moussa Helem Mohamed', 'Mohamed Fouad Mohamed']\n",
      "['Mohamed Bourhan Mohamed', 'Moussa Helem Mohamed', 'Mohamed Fouad Mohamed']\n",
      "['Mohamed Bourhan Mohamed', 'Moussa Helem Mohamed', 'Mohamed Fouad Mohamed']\n",
      "496 M Mohamed ['Mohamed Bourhan Mohamed', 'Moussa Helem Mohamed', 'Mohamed Fouad Mohamed'] Burkina Faso vs Djibouti 11-10-2021\n",
      "['Mohamed Bourhan Mohamed', 'Mohamed Fouad Mohamed'] <class 'list'> False\n",
      "496 not working list M Mohamed ['Mohamed Bourhan Mohamed', 'Mohamed Fouad Mohamed']\n",
      "['Jason Johnson', 'Jermaine Johnson']\n",
      "['Jason Johnson', 'Jermaine Johnson']\n",
      "497 J Johnson ['Jason Johnson', 'Jermaine Johnson'] Curaçao vs Jamaica 10-07-2017\n",
      "497 WORKING J Johnson Jermaine Johnson\n",
      "['Jan-Michael Williams', 'Jomal Williams', 'Jesse Williams']\n",
      "['Jan-Michael Williams', 'Jomal Williams', 'Jesse Williams']\n",
      "['Jan-Michael Williams', 'Jomal Williams', 'Jesse Williams']\n",
      "498 J Williams ['Jan-Michael Williams', 'Jomal Williams', 'Jesse Williams'] Panama vs Trinidad and Tobago 19-06-2019\n",
      "reverse loop. there was no national team history time data '19/06/19' does not match format '%m/%d/%y'\n",
      "reverse loop. there was no national team history time data '19/06/19' does not match format '%m/%d/%y'\n",
      "498 WORKING J Williams Jomal Williams\n",
      "['Jan-Michael Williams', 'Jomal Williams', 'Jesse Williams']\n",
      "['Jan-Michael Williams', 'Jomal Williams', 'Jesse Williams']\n",
      "['Jan-Michael Williams', 'Jomal Williams', 'Jesse Williams']\n",
      "499 J Williams ['Jan-Michael Williams', 'Jomal Williams', 'Jesse Williams'] Trinidad and Tobago vs Guyana 27-06-2019\n",
      "reverse loop. there was no national team history time data '27/06/19' does not match format '%m/%d/%y'\n",
      "reverse loop. there was no national team history time data '27/06/19' does not match format '%m/%d/%y'\n",
      "499 WORKING J Williams Jomal Williams\n",
      "0 14 6\n"
     ]
    }
   ],
   "source": [
    "success_status_counter = 0\n",
    "fail_counter = 0\n",
    "done_cases = 0\n",
    "for i in range(480, 500):\n",
    "    index = i\n",
    "    row = large_dataset_said_0.loc[index]\n",
    "    if row['Status New'] != 'Done':\n",
    "        match = row[\"Match\"]\n",
    "        date = row['Date']\n",
    "        nationality = row['Nationality']\n",
    "        this_country_code = row['Team Country Code']\n",
    "        if pd.isna(this_country_code):\n",
    "            country_name = 'Namibia'\n",
    "        else:\n",
    "            if this_country_code == 'BVI':\n",
    "                country_name = 'British Virgin Islands'\n",
    "            else:\n",
    "                country_name = countries_codes[countries_codes[' Code'] == this_country_code]['Country'].unique()[0]\n",
    "        dataset_nationality = leagues_value[leagues_value['Team 1 Code'] == this_country_code]['Name'].unique()\n",
    "        ORIGINAL_JERSEY = row['ORIGINAL JERSEY']\n",
    "        if ORIGINAL_JERSEY == 'Sos Suhana':\n",
    "            ORIGINAL_JERSEY = 'Sos Souhana'\n",
    "        elif i == 6:\n",
    "            ORIGINAL_JERSEY = 'Franklin Lucena'\n",
    "        elif i == 128:\n",
    "            ORIGINAL_JERSEY = 'Franklin Lucena'\n",
    "        elif i == 131:\n",
    "            ORIGINAL_JERSEY = 'Franklin Lucena'\n",
    "        elif i == 135:\n",
    "            ORIGINAL_JERSEY = 'Franklin Lucena'\n",
    "        elif i == 140:\n",
    "            ORIGINAL_JERSEY = 'Franklin Lucena'\n",
    "        elif i == 214:\n",
    "            ORIGINAL_JERSEY = 'Tyler Lee'\n",
    "        names_found = row['Name(s) Found']\n",
    "        if type(names_found) == str and names_found.startswith('['):\n",
    "            names_found = eval(names_found)\n",
    "        if type(names_found) == str:\n",
    "            if names_found == ORIGINAL_JERSEY or check_strings_equal_with_rotation(ORIGINAL_JERSEY, names_found) == True:\n",
    "                print(i, 'WORKING', ORIGINAL_JERSEY, names_found)\n",
    "                large_dataset_said_0.at[i, 'Status New'] = 'Done'\n",
    "                large_dataset_said_0.at[i, 'Name(s) Found'] = names_found\n",
    "                success_status_counter += 1\n",
    "            else:\n",
    "                print(i, 'WORKING', ORIGINAL_JERSEY, names_found)\n",
    "                large_dataset_said_0.at[i, 'Status New'] = 'Done'\n",
    "                large_dataset_said_0.at[i, 'Name(s) Found'] = names_found\n",
    "                success_status_counter += 1\n",
    "        else:\n",
    "            for name in names_found:\n",
    "                if name == ORIGINAL_JERSEY or check_strings_equal_with_rotation(ORIGINAL_JERSEY, name) == True:\n",
    "                    print(i, 'WORKING', ORIGINAL_JERSEY, name)\n",
    "                    names_found = name\n",
    "                    large_dataset_said_0.at[i, 'Status New'] = 'Done'\n",
    "                    large_dataset_said_0.at[i, 'Name(s) Found'] = names_found\n",
    "                    success_status_counter += 1\n",
    "                else:\n",
    "                    print(names_found)\n",
    "        if type(names_found) == list:\n",
    "            print(i, ORIGINAL_JERSEY, names_found, match, date)\n",
    "            filtered_names_match_date = filter_using_date_of_match(names_found, row['Nationality'], date, False)[1]\n",
    "            if type(filtered_names_match_date) == str:\n",
    "                if filtered_names_match_date == '':\n",
    "                    print(i, 'not working nobody', ORIGINAL_JERSEY, filtered_names_match_date)\n",
    "                    fail_counter += 1 \n",
    "                else:\n",
    "                    print(i, 'WORKING', ORIGINAL_JERSEY, filtered_names_match_date)\n",
    "                    large_dataset_said_0.at[i, 'Status New'] = 'Done'\n",
    "                    large_dataset_said_0.at[i, 'Name(s) Found'] = filtered_names_match_date\n",
    "                    success_status_counter += 1\n",
    "            else:\n",
    "                print(i, 'not working list', ORIGINAL_JERSEY, filtered_names_match_date)\n",
    "                large_dataset_said_0.at[i, 'Name(s) Found'] = filtered_names_match_date\n",
    "                large_dataset_said_0.at[i, 'Status New'] = 'Multi'\n",
    "                fail_counter += 1 \n",
    "    else:\n",
    "        done_cases += 1\n",
    "print(done_cases, success_status_counter, fail_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(large_dataset_said_0)):\n",
    "    if large_dataset_said_0.at[i, 'ORIGINAL JERSEY'] == 'Faisal Al Harbi' and large_dataset_said_0.at[i, 'Status New'] == '':\n",
    "        large_dataset_said_0.at[i, 'Name(s) Found'] = 'Faisal Zayed'\n",
    "        large_dataset_said_0.at[i, 'Status New'] = 'Done'\n",
    "    elif large_dataset_said_0.at[i, 'ORIGINAL JERSEY'] == 'H Nguyn' and large_dataset_said_0.at[i, 'Status New'] == '':\n",
    "        large_dataset_said_0.at[i, 'Name(s) Found'] = 'Huy Hung Nguyen'\n",
    "        large_dataset_said_0.at[i, 'Status New'] = 'Done'\n",
    "    elif large_dataset_said_0.at[i, 'ORIGINAL JERSEY'] == 'P Mkhonto' and large_dataset_said_0.at[i, 'Status New'] == '':\n",
    "        large_dataset_said_0.at[i, 'Name(s) Found'] = 'Philani Thabo Mkhontfo'\n",
    "        large_dataset_said_0.at[i, 'Status New'] = 'Done'\n",
    "    elif large_dataset_said_0.at[i, 'ORIGINAL JERSEY'] == 'Assadhulla Abdulla' and large_dataset_said_0.at[i, 'Status New'] == '':\n",
    "        large_dataset_said_0.at[i, 'Name(s) Found'] = 'Abdulla Assadhulla'\n",
    "        large_dataset_said_0.at[i, 'Status New'] = 'Done'\n",
    "    elif large_dataset_said_0.at[i, 'ORIGINAL JERSEY'] == 'Abu Agla Abdalla' and large_dataset_said_0.at[i, 'Status New'] == '':\n",
    "        large_dataset_said_0.at[i, 'Name(s) Found'] = 'Mohamed Abuaagla'\n",
    "        large_dataset_said_0.at[i, 'Status New'] = 'Done'\n",
    "    elif large_dataset_said_0.at[i, 'ORIGINAL JERSEY'] == 'D De Silva' and large_dataset_said_0.at[i, 'Status New'] == '':\n",
    "        large_dataset_said_0.at[i, 'Name(s) Found'] = 'Madushan de Silva'\n",
    "        large_dataset_said_0.at[i, 'Status New'] = 'Done'\n",
    "    elif large_dataset_said_0.at[i, 'ORIGINAL JERSEY'] == 'Thu Aung' and large_dataset_said_0.at[i, 'Status New'] == '':\n",
    "        large_dataset_said_0.at[i, 'Name(s) Found'] = 'Aung Thu'\n",
    "        large_dataset_said_0.at[i, 'Status New'] = 'Done'\n",
    "    elif large_dataset_said_0.at[i, 'ORIGINAL JERSEY'] == 'Ahmad Almahaijri' and large_dataset_said_0.at[i, 'Status New'] == '':\n",
    "        large_dataset_said_0.at[i, 'Name(s) Found'] = 'Ahmed Kano'\n",
    "        large_dataset_said_0.at[i, 'Status New'] = 'Done'\n",
    "    elif large_dataset_said_0.at[i, 'ORIGINAL JERSEY'] == 'F Camara' and large_dataset_said_0.at[i, 'Status New'] == '':\n",
    "        large_dataset_said_0.at[i, 'Name(s) Found'] = 'Fodé Camara'\n",
    "        large_dataset_said_0.at[i, 'Status New'] = 'Done'\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "large_dataset_said_0.at[80, 'Status New'] = 'Done'\n",
    "large_dataset_said_0.at[80, 'Name(s) Found'] = 'Lanfia Camara'\n",
    "\n",
    "\n",
    "large_dataset_said_0.at[92, 'Status New'] = 'Done'\n",
    "large_dataset_said_0.at[92, 'Name(s) Found'] = 'Ashley Torres'\n",
    "\n",
    "\n",
    "\n",
    "large_dataset_said_0.at[92, 'Status New'] = 'Done'\n",
    "large_dataset_said_0.at[92, 'Name(s) Found'] = 'Ashley Torres'\n",
    "\n",
    "large_dataset_said_0.at[111, 'Status New'] = ''\n",
    "large_dataset_said_0.at[111, 'Name(s) Found'] = ''\n",
    "large_dataset_said_0.at[111, 'Nationality'] = 'Somalia'\n",
    "large_dataset_said_0.at[111, 'Team Country Code'] = 'SO'\n",
    "\n",
    "large_dataset_said_0.at[127, 'Status New'] = 'Done'\n",
    "large_dataset_said_0.at[127, 'Name(s) Found'] = 'Luis Ramírez'\n",
    "large_dataset_said_0.at[127, 'Nationality'] = 'Peru'\n",
    "large_dataset_said_0.at[127, 'Team Country Code'] = 'PE'\n",
    "\n",
    "\n",
    "large_dataset_said_0.at[178, 'Status New'] = 'Done'\n",
    "large_dataset_said_0.at[178, 'Name(s) Found'] = 'Phinda Dlamini'\n",
    "\n",
    "large_dataset_said_0.at[198, 'Status New'] = 'Done'\n",
    "large_dataset_said_0.at[198, 'Name(s) Found'] = 'Veasna Soun'\n",
    "\n",
    "large_dataset_said_0.at[199, 'Status New'] = 'Done'\n",
    "large_dataset_said_0.at[199, 'Name(s) Found'] = 'Thet Naing'\n",
    "\n",
    "large_dataset_said_0.at[111, 'Status New'] = 'Done'\n",
    "large_dataset_said_0.at[493, 'Status New'] = 'Done'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# large_dataset_said_0.at[77, 'Status New'] = 'Done'\n",
    "# large_dataset_said_0.at[77, 'Name(s) Found'] = 'Fodé Camara'\n",
    "\n",
    "# large_dataset_said_0.at[78, 'Status New'] = 'Done'\n",
    "# large_dataset_said_0.at[78, 'Name(s) Found'] = 'Fodé Camara'\n",
    "\n",
    "# large_dataset_said_0.at[79, 'Status New'] = 'Done'\n",
    "# large_dataset_said_0.at[79, 'Name(s) Found'] = 'Fodé Camara'\n",
    "\n",
    "# large_dataset_said_0.at[79, 'Status New'] = 'Done'\n",
    "# large_dataset_said_0.at[79, 'Name(s) Found'] = 'Fodé Camara'\n",
    "\n",
    "# large_dataset_said_0.at[333, 'Status New'] = 'Done'\n",
    "# large_dataset_said_0.at[333, 'Name(s) Found'] = 'Jesús Sagredo'\n",
    "\n",
    "# large_dataset_said_0.at[334, 'Status New'] = 'Done'\n",
    "# large_dataset_said_0.at[334, 'Name(s) Found'] = 'Jesús Sagredo'\n",
    "\n",
    "# large_dataset_said_0.at[379, 'Status New'] = 'Done'\n",
    "# large_dataset_said_0.at[379, 'Name(s) Found'] = 'Fodé Camara'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_dataset_said_0.to_csv('lg_dataset_said_0_complete.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with lookup required dudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 2026):\n",
    "    names_found = []\n",
    "    num_identical_rows = 0\n",
    "    num_names_this = 0\n",
    "    index = i\n",
    "    row = lookup_required_dudes.loc[index]\n",
    "    # Extract relevant columns\n",
    "    match = row['Match']\n",
    "    date = row['Date']\n",
    "    ORIGINAL_JERSEY = row['ORIGINAL JERSEY']\n",
    "    nationality = row['Nationality']\n",
    "    this_country_code = row['Team Country Code']\n",
    "    if pd.isna(this_country_code):\n",
    "        country_name = 'Namibia'\n",
    "    else:\n",
    "        if this_country_code == 'BVI':\n",
    "            country_name = 'British Virgin Islands'\n",
    "        else:\n",
    "            country_name = countries_codes[countries_codes[' Code'] == this_country_code]['Country'].unique()[0]\n",
    "    dataset_nationality = leagues_value[leagues_value['Team 1 Code'] == this_country_code]['Name'].unique()\n",
    "\n",
    "    matches_try_1 = threshold_player_match(ORIGINAL_JERSEY, dataset_nationality)[0]\n",
    "\n",
    "    if len(matches_try_1) == 1:\n",
    "        if row['Season'] <= 2012:\n",
    "            0==0\n",
    "        elif matches_try_1 == ['No matches found even with the lowest threshold.']:\n",
    "            0==0\n",
    "        else:\n",
    "            print(i, f\"jersey is {ORIGINAL_JERSEY}, country is {nationality}. match is {row['Match']} on {row['Date']}. matches is {matches_try_1}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
