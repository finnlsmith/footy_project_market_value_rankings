{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from countries_languages import country_to_language\n",
    "\n",
    "import pandas as pd\n",
    "from difflib import get_close_matches\n",
    "import numpy as np\n",
    "import re\n",
    "from transliterate import translit\n",
    "from unidecode import unidecode\n",
    "import Levenshtein\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "import wikipediaapi\n",
    "\n",
    "import wikipedia\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import difflib\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import calendar\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import AssemblyHelpers\n",
    "#from AssemblyHelpers import find_money_info_from_name\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_dataset(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "cyrillic_guys = load_csv_dataset('CSVs of edge cases - for F/DONE WORKING WITH/cyrillic_dudes.csv')\n",
    "lookup_guys = load_csv_dataset('CSVs of edge cases - for F/lookup_required_dudes.csv')\n",
    "\n",
    "online_lookup_1_found = load_csv_dataset('CSVs of edge cases - for F/Online lookup required - 1 name found.csv')\n",
    "online_lookup_0_found = load_csv_dataset('CSVs of edge cases - for F/Online lookup required - 0 names found.csv')\n",
    "online_lookup_multi_found = load_csv_dataset('CSVs of edge cases - for F/Online lookup required - multiple names found.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_large = load_csv_dataset('large_dataset_1 (combined DF).csv')\n",
    "\n",
    "large_dataset_said_0 = load_csv_dataset('CSVs of edge cases - for F/large_dataset_SAID_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi_names_used_large_dataset = combined_df_large[(combined_df_large['Status'] == 'SUCCESS') & (combined_df_large['Lookup Return Case'] == 'Used Large Dataset') & (combined_df_large['Name(s) Found'].astype(str).str.startswith('['))]#['Lookup Return Case'].value_counts()\n",
    "multi_names_working = combined_df_large[(combined_df_large['Status'] == 'SUCCESS') & (combined_df_large['Lookup Return Case'] == 'working') & (combined_df_large['Name(s) Found'].astype(str).str.startswith('['))]#['Lookup Return Case'].value_counts()\n",
    "multi_names_working.to_csv('multiple_names_working.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_date_multi_guys = load_csv_dataset('CSVs of edge cases - for F/match_date_multi_guys.csv')\n",
    "match_date_zero_guys = load_csv_dataset('CSVs of edge cases - for F/match_date_zero_guys.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOOKUP DATASETS\n",
    "\n",
    "#leagues value\n",
    "leagues_value = load_csv_dataset('/Users/finneganlaister-smith/Downloads/DEV ENVIRONMENT/data-science-jupyter-template-main/footy_project_market_value_rankings/create_dataset_1/Most Updated Edited Transfermarkt Dataset.csv')\n",
    "#leagues value large \n",
    "leagues_value_large = load_csv_dataset('/Users/finneganlaister-smith/Downloads/DEV ENVIRONMENT/data-science-jupyter-template-main/footy_project_market_value_rankings/create_dataset_1/most_updated_transfermarkt_dataset.csv')\n",
    "\n",
    "countries_codes = load_csv_dataset('/Users/finneganlaister-smith/Downloads/DEV ENVIRONMENT/data-science-jupyter-template-main/footy_project_market_value_rankings/create_dataset_1/countries_and_codes.csv')\n",
    "\n",
    "result_names_null_values = load_csv_dataset('/Users/finneganlaister-smith/Downloads/DEV ENVIRONMENT/data-science-jupyter-template-main/footy_project_market_value_rankings/create_dataset_1/nullnames_values.csv')\n",
    "result_names_null_values = list(result_names_null_values['Name'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lookup required - 1 found - helper functions\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "def find_close_matches(this_jersey, dataset_nationality, threshold=90):\n",
    "    \"\"\"\n",
    "    Find close matches of `this_jersey` in `dataset_nationality` using Levenshtein distance.\n",
    "\n",
    "    Args:\n",
    "    - this_jersey (str): The string to find close matches for.\n",
    "    - dataset_nationality (list): List of strings to search for close matches in.\n",
    "    - threshold (int): Minimum similarity score required for a match (default is 90).\n",
    "\n",
    "    Returns:\n",
    "    - List of strings from `dataset_nationality` that are close matches to `this_jersey`.\n",
    "    \"\"\"\n",
    "    close_matches = process.extract(this_jersey, dataset_nationality, limit=None)\n",
    "    return [match[0] for match in close_matches if match[1] >= threshold]\n",
    "\n",
    "\n",
    "def find_close_matches_variable(this_jersey, dataset_nationality, threshold):\n",
    "    \"\"\"\n",
    "    Find close matches of `this_jersey` in `dataset_nationality` using Levenshtein distance.\n",
    "\n",
    "    Args:\n",
    "    - this_jersey (str): The string to find close matches for.\n",
    "    - dataset_nationality (list): List of strings to search for close matches in.\n",
    "    - threshold (int): Minimum similarity score required for a match (default is 90).\n",
    "\n",
    "    Returns:\n",
    "    - List of strings from `dataset_nationality` that are close matches to `this_jersey`.\n",
    "    \"\"\"\n",
    "    close_matches = process.extract(this_jersey, dataset_nationality, limit=None)\n",
    "    return [match[0] for match in close_matches if match[1] >= threshold]\n",
    "\n",
    "\n",
    "def rearrange_name_asian(this_jersey):\n",
    "    \"\"\"\n",
    "    Rearrange the name in `this_jersey` so that the last token comes first followed by a space.\n",
    "\n",
    "    Args:\n",
    "    - this_jersey (str): The name to rearrange.\n",
    "\n",
    "    Returns:\n",
    "    - Rearranged name string.\n",
    "    \"\"\"\n",
    "    tokens = this_jersey.split()\n",
    "    rearranged_name = tokens[-1] + \" \" + \" \".join(tokens[:-1])\n",
    "    return rearranged_name\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def normalize_name(name):\n",
    "    \"\"\"\n",
    "    Normalize the given name by removing dashes, hyphens, apostrophes, and backticks,\n",
    "    and replacing characters with accents with their unaccented counterparts.\n",
    "\n",
    "    Args:\n",
    "    - name (str): The name to normalize.\n",
    "\n",
    "    Returns:\n",
    "    - Normalized name string.\n",
    "    \"\"\"\n",
    "    # Remove dashes, hyphens, apostrophes, and backticks\n",
    "    name = re.sub(r'[-\\'`]', '', name)\n",
    "\n",
    "    # Replace characters with accents with their unaccented counterparts\n",
    "    name = ''.join(char for char in unicodedata.normalize('NFD', name) if unicodedata.category(char) != 'Mn')\n",
    "\n",
    "    return name\n",
    "\n",
    "def switch_yi(string):\n",
    "    if string.endswith('yi'):\n",
    "        switched_string = string[:-2] + 'iy'\n",
    "        return switched_string\n",
    "    elif string.endswith('iy'):\n",
    "        switched_string = string[:-2] + 'yi'\n",
    "        return switched_string\n",
    "    else:\n",
    "        return string\n",
    "    \n",
    "\n",
    "def threshold_player_match(this_jersey, dataset_nationality):\n",
    "    THRESHOLD_NUM = 89\n",
    "    #stops when it returns a name.\n",
    "    #if it doesnt find a match keep lowering the threshold until you find a match\n",
    "    #but if you get to threshold of like 50 first you stop and just abandon ship    \n",
    "\n",
    "    # Loop until someone is found or threshold goes below 50\n",
    "    while THRESHOLD_NUM >= 50:\n",
    "        matches = find_close_matches_variable(this_jersey, dataset_nationality, THRESHOLD_NUM)\n",
    "        if matches:\n",
    "            #print(f\"Player is {this_jersey}. Found matches: {matches}. threshold is {THRESHOLD_NUM}\")\n",
    "            return matches, THRESHOLD_NUM\n",
    "            #break\n",
    "        else:\n",
    "            THRESHOLD_NUM -= 1\n",
    "\n",
    "    # If threshold reaches below 50 without finding any matches\n",
    "    if THRESHOLD_NUM < 50:\n",
    "        return [f\"No matches found even with the lowest threshold.\"], this_jersey #jersey was {this_jersey}\n",
    "\n",
    "def filter_matches_by_initial_and_name_start(this_jersey_tokens, matches):\n",
    "    \"\"\"\n",
    "    Filter matches based on whether at least one token starts with the initial and at least one token starts with \n",
    "    the first character of the name, with additional conditions based on the tokens in this_jersey_tokens.\n",
    "\n",
    "    Args:\n",
    "    - this_jersey_tokens (list): List containing the initial and name parts of the jersey.\n",
    "    - matches (list): List of matches to filter.\n",
    "\n",
    "    Returns:\n",
    "    - List of filtered matches.\n",
    "    \"\"\"\n",
    "    filtered_matches = []\n",
    "    \n",
    "    # Check if this_jersey_tokens has multiple tokens\n",
    "    if len(this_jersey_tokens) > 1:\n",
    "        initial_starts_with = this_jersey_tokens[0]\n",
    "        name_starts_with = this_jersey_tokens[1][0]\n",
    "\n",
    "        # Additional logic for handling 'H' and 'Kh'\n",
    "        if name_starts_with == 'H':\n",
    "            if any(token.startswith('Kh') for token in this_jersey_tokens[1:]):\n",
    "                name_starts_with = 'Kh'\n",
    "\n",
    "        # Additional logic for handling multiple tokens\n",
    "        for token in this_jersey_tokens[1:]:\n",
    "            if token[0] != name_starts_with:\n",
    "                initial_starts_with = None\n",
    "                break\n",
    "        \n",
    "        for match in matches:\n",
    "            match_tokens = match.split()\n",
    "            initial_found = False\n",
    "            name_start_found = False\n",
    "\n",
    "            # Check if at least one token starts with the initial\n",
    "            if initial_starts_with:\n",
    "                for token in match_tokens:\n",
    "                    if token.startswith(initial_starts_with):\n",
    "                        initial_found = True\n",
    "                        break\n",
    "\n",
    "            # Check if at least one token starts with the first character of the name\n",
    "            for token in match_tokens:\n",
    "                if token.startswith(name_starts_with) or (name_starts_with == 'H' and token.startswith('Kh')):\n",
    "                    name_start_found = True\n",
    "                    break\n",
    "\n",
    "            # Check if any token starts with any character of the hyphenated name\n",
    "            if '-' in this_jersey_tokens[1]:\n",
    "                hyphenated_parts = this_jersey_tokens[1].split('-')\n",
    "                for part in hyphenated_parts:\n",
    "                    for token in match_tokens:\n",
    "                        if token.startswith(part[0]):\n",
    "                            name_start_found = True\n",
    "                            break\n",
    "\n",
    "            # If both conditions are met, add the match to filtered_matches\n",
    "            if initial_found and name_start_found:\n",
    "                filtered_matches.append(match)\n",
    "\n",
    "    else:  # Handle the case where this_jersey_tokens has only one token\n",
    "        for match in matches:\n",
    "            match_tokens = match.split()\n",
    "            initial_found = False\n",
    "            name_start_found = False\n",
    "\n",
    "            # Check if at least one token starts with the initial\n",
    "            for token in match_tokens:\n",
    "                if token.startswith(this_jersey_tokens[0]):\n",
    "                    initial_found = True\n",
    "                    break\n",
    "\n",
    "            # Check if at least one token starts with the first character of the name\n",
    "            for token in match_tokens:\n",
    "                if token.startswith(this_jersey_tokens[0][0]):\n",
    "                    name_start_found = True\n",
    "                    break\n",
    "\n",
    "            # Check if any token starts with any character of the hyphenated name\n",
    "            if '-' in this_jersey_tokens[0]:\n",
    "                hyphenated_parts = this_jersey_tokens[0].split('-')\n",
    "                for part in hyphenated_parts:\n",
    "                    for token in match_tokens:\n",
    "                        if token.startswith(part[0]):\n",
    "                            name_start_found = True\n",
    "                            break\n",
    "\n",
    "            # If both conditions are met, add the match to filtered_matches\n",
    "            if initial_found and name_start_found:\n",
    "                filtered_matches.append(match)\n",
    "\n",
    "    return filtered_matches\n",
    "\n",
    "\n",
    "def check_last_token_match(this_jersey, matches):\n",
    "    \"\"\"\n",
    "    Check if the last token of each match matches the jersey with variations like accents or substitutions.\n",
    "\n",
    "    Args:\n",
    "    - this_jersey (str): The original name being searched for.\n",
    "    - matches (list): Set of names matching the jersey.\n",
    "\n",
    "    Returns:\n",
    "    - List of matches where the last token matches the jersey with variations.\n",
    "    \"\"\"\n",
    "    def remove_accents(input_str):\n",
    "        \"\"\"\n",
    "        Remove accents from characters in a string.\n",
    "        \"\"\"\n",
    "        nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "        return ''.join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "    def apply_substitutions(input_str):\n",
    "        \"\"\"\n",
    "        Apply specific character substitutions in a string.\n",
    "        \"\"\"\n",
    "        substitutions = {'g': 'h', 'ij': 'y', 'zh': 'j', 'ae': 'aye', 'H': 'Kh'}\n",
    "        for key, value in substitutions.items():\n",
    "            input_str = input_str.replace(key, value)\n",
    "        return input_str\n",
    "\n",
    "    jersey_last_token = this_jersey.split()[-1]\n",
    "\n",
    "    matching_matches = []\n",
    "    for match in matches:\n",
    "        match_last_token = match.split()[-1]\n",
    "\n",
    "        # Check if the last token matches the jersey (ignoring accents)\n",
    "        if remove_accents(match_last_token) == remove_accents(jersey_last_token):\n",
    "            matching_matches.append(match)\n",
    "        else:\n",
    "            # Check if the last token matches the jersey with specific character substitutions\n",
    "            if apply_substitutions(match_last_token) == apply_substitutions(jersey_last_token):\n",
    "                matching_matches.append(match)\n",
    "\n",
    "    return matching_matches\n",
    "\n",
    "def filter_multi_word_matches_by_jersey_tokens(jersey_tokens, potential_matches):\n",
    "    filtered_matches = []\n",
    "\n",
    "    for match in potential_matches:\n",
    "        # Split each match into tokens\n",
    "        match_tokens = match.split()\n",
    "\n",
    "        # Count the number of tokens that start with each character in jersey_tokens\n",
    "        match_token_start_chars = {token[0] for token in match_tokens}\n",
    "\n",
    "        # Check if the match contains at least one token for each character in jersey_tokens\n",
    "        if all(token[0] in match_token_start_chars for token in jersey_tokens):\n",
    "            filtered_matches.append(match)\n",
    "\n",
    "    return filtered_matches\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lookup Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_guys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Lookup Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance\n",
    "\n",
    "#Helper methods\n",
    "def rearrange_name(name):\n",
    "    tokens = name.split()\n",
    "    if len(tokens) > 1:\n",
    "        last_token = tokens.pop()\n",
    "        tokens.insert(0, last_token)\n",
    "        return ' '.join(tokens)\n",
    "    else:\n",
    "        return name\n",
    "    \n",
    "\n",
    "def closest_string(this_jersey, dataset_nationality):\n",
    "    closest_distance = float('inf')\n",
    "    closest_strings = []\n",
    "    \n",
    "    # Splitting this_jersey into tokens\n",
    "    tokens = this_jersey.split(' ')\n",
    "    \n",
    "    # If there's only one token, return all names where the single token is in that name\n",
    "    if len(tokens) == 1:\n",
    "        for nationality in dataset_nationality:\n",
    "            if tokens[0] in nationality:\n",
    "                closest_strings.append(nationality)\n",
    "            elif tokens[0] in unidecode(nationality):\n",
    "                closest_strings.append(nationality)\n",
    "        return closest_strings\n",
    "\n",
    "    # Calculate distances for all names\n",
    "    for nationality in dataset_nationality:\n",
    "        distance = Levenshtein.distance(this_jersey, nationality)\n",
    "        if distance < closest_distance:\n",
    "            closest_distance = distance\n",
    "            closest_strings = [nationality]\n",
    "        elif distance == closest_distance:\n",
    "            closest_strings.append(nationality)\n",
    "\n",
    "    # If there are multiple names with the same distance,\n",
    "    # and some of them have the same last name, include all of them\n",
    "    if len(closest_strings) > 1:\n",
    "        last_names = [name.split()[-1] for name in closest_strings]\n",
    "        unique_last_names = set(last_names)\n",
    "        if len(unique_last_names) == 1:\n",
    "            return closest_strings\n",
    "    \n",
    "    return closest_strings\n",
    "\n",
    "def remove_vowels(text):\n",
    "    vowels = 'aeoAEIOU'\n",
    "    return ''.join(char for char in text if char not in vowels)\n",
    "\n",
    "\n",
    "def filter_matches(correct_matches, closest):\n",
    "    filtered_matches = []\n",
    "    for match in correct_matches:\n",
    "        if match.lower() == closest.lower():\n",
    "            filtered_matches.append(match)\n",
    "    return filtered_matches\n",
    "\n",
    "\n",
    "def check_tokens(this_jersey, match):\n",
    "    jersey_tokens = this_jersey.lower().split()\n",
    "    match_tokens = match.lower().split()\n",
    "    \n",
    "    if all(token in match_tokens for token in jersey_tokens):\n",
    "        return True\n",
    "    elif all(token in jersey_tokens for token in match_tokens) and len(jersey_tokens) == len(match_tokens) + 1:\n",
    "        return True\n",
    "    elif all(token in match_tokens for token in jersey_tokens) and len(jersey_tokens) > len(match_tokens):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def count_diff_vowels(token1, token2):\n",
    "    vowels = 'aeiou'\n",
    "    count = 0\n",
    "    for char1, char2 in zip(token1, token2):\n",
    "        if char1.lower() in vowels and char2.lower() in vowels:\n",
    "            if char1.lower() != char2.lower():\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "def filter_names_by_tokens(names, tokens):\n",
    "    filtered_names = []\n",
    "    for name in names:\n",
    "        if all(token in name for token in tokens):\n",
    "            filtered_names.append(name)\n",
    "    return filtered_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_matches_2 = pd.DataFrame()\n",
    "#for index, row in online_lookup_1_found.iterrows():\n",
    "for i in range(1200, 1258):\n",
    "    status_row = 'Fail'\n",
    "    matches_final = []\n",
    "    correct_matches = ''\n",
    "    filtered_matches = ''\n",
    "    print(f\"**{i}**\")\n",
    "    index = i\n",
    "    row = online_lookup_1_found.iloc[index]\n",
    "    this_row_success_tally = 0\n",
    "    this_row_multi_tally = 0\n",
    "    this_row_fail_tally = 0\n",
    "\n",
    "    name_list_matches = []\n",
    "\n",
    "    this_jersey = row['ORIGINAL JERSEY']\n",
    "\n",
    "    if this_jersey == 'R Myratberdiýew':\n",
    "        this_jersey = 'R Muratberdyev'\n",
    "    elif this_jersey == 'E ilitao':\n",
    "        this_jersey = 'Éder Militão'\n",
    "    elif this_jersey == 'Feliciano Jone':\n",
    "        this_jersey = \"Nené\"\n",
    "    elif this_jersey == 'E Håland':\n",
    "        this_jersey = 'E Haaland'\n",
    "    elif this_jersey == 'Phm Thành Lng':\n",
    "        this_jersey = 'Pham Thành Luong'\n",
    "    elif this_jersey == 'Þ Helgason':\n",
    "        this_jersey = 'T Helgason'\n",
    "    \n",
    "        \n",
    "    \n",
    "    nationality = row['Nationality']\n",
    "    nationality_code = row['Team Country Code']\n",
    "    if nationality_code == 'BVI':\n",
    "        nationality_code = ' BVI'\n",
    "    country_name = countries_codes[countries_codes[' Code'] == nationality_code]['Country'].unique()[0]\n",
    "    if nationality_code == ' DR':\n",
    "        nationality_code = 'DR'\n",
    "    season = row['Season']\n",
    "    match_date = row['Date']\n",
    "    dataset_nationality = leagues_value[leagues_value['Team 1 Code'] == nationality_code]['Name'].unique()\n",
    "\n",
    "    if this_jersey.endswith('yi'):\n",
    "        this_jersey = switch_yi(this_jersey)\n",
    "    elif this_jersey.endswith('iy'):\n",
    "        this_jersey = switch_yi(this_jersey)\n",
    "\n",
    "    name_type = ''\n",
    "    # if (nationality_code == 'KR') | (nationality_code == 'CN') | (nationality_code == 'TW') | (nationality_code == 'SG') | (nationality_code == 'KP'):\n",
    "    #     this_jersey = rearrange_name_asian(this_jersey)\n",
    "    matches, threshold_num = threshold_player_match(this_jersey, dataset_nationality)\n",
    "\n",
    "    this_jersey_tokens = this_jersey.split(' ')\n",
    "    case_row = ''\n",
    "\n",
    "    if len(this_jersey_tokens) >= 2:\n",
    "        if len(this_jersey_tokens[0]) <= 2: #initial is at the start \n",
    "            case_row = 'Initial Plus Name'\n",
    "            correct_matches = []\n",
    "            for name in dataset_nationality:\n",
    "                if (name.startswith(this_jersey_tokens[0])) & (name.endswith(this_jersey_tokens[-1])):\n",
    "                    correct_matches.append(name)\n",
    "                elif (name.startswith(this_jersey_tokens[0])) & (name.endswith(unidecode(this_jersey_tokens[-1]))):\n",
    "                    correct_matches.append(name)\n",
    "                elif (this_jersey_tokens[-1] in name) and any(token.startswith(this_jersey_tokens[0]) for token in name.split(' ')):\n",
    "                    correct_matches.append(name)\n",
    "                elif (this_jersey_tokens[-1] in name) and any(token.startswith(this_jersey_tokens[0].lower()) for token in name.split(' ')):\n",
    "                    correct_matches.append(name)\n",
    "                elif (this_jersey_tokens[-1] in unidecode(name)) and any(token.startswith(this_jersey_tokens[0]) for token in name.split(' ')):\n",
    "                    correct_matches.append(name)\n",
    "                \n",
    "                #the full word is \n",
    "                elif name.startswith(this_jersey_tokens[0]):\n",
    "                    name_tokens = name.split(' ')\n",
    "                    if len(name_tokens) > 1:\n",
    "                        last_token = name_tokens[-1]\n",
    "                        target_token = this_jersey_tokens[-1]\n",
    "                        if abs(len(last_token) - len(target_token)) <= 2 and distance(last_token, target_token) <= 2:\n",
    "                            correct_matches.append(name)\n",
    "\n",
    "            if len(correct_matches) >= 2:\n",
    "                0==0 #row 1100, 1101\n",
    "            elif len(correct_matches) == 1:\n",
    "                0==0 #row 0\n",
    "            else:\n",
    "                0==0 #row 450\n",
    "            #make sure the names remaining all start with an initial and ends with the jersey's name. \n",
    "            #be mindful of accents - if names match once you remove accents from the matches that's ok\n",
    "        elif len(this_jersey_tokens[-1]) <= 2: #initial is at the end \n",
    "            case_row = 'Initial Plus Name'\n",
    "            correct_matches = []\n",
    "            for name in dataset_nationality:\n",
    "                if (name.startswith(this_jersey_tokens[-1])) & (name.endswith(this_jersey_tokens[0])):\n",
    "                    correct_matches.append(name)\n",
    "                elif (name.startswith(this_jersey_tokens[-1])) & (name.endswith(unidecode(this_jersey_tokens[0]))):\n",
    "                    correct_matches.append(name)\n",
    "                \n",
    "\n",
    "            if len(correct_matches) >= 2:\n",
    "                0==0\n",
    "            elif len(correct_matches) == 1:\n",
    "                0==0\n",
    "            else:\n",
    "                0==0\n",
    "        else: #multiple words\n",
    "            case_row = 'Multiple Words'\n",
    "            correct_matches = []\n",
    "            for match in matches:\n",
    "                match_tokens = re.split(r'[\\s-]', match)\n",
    "                all_tokens_valid = all(token in this_jersey_tokens or unidecode(token) in this_jersey_tokens for token in match_tokens)\n",
    "                if all_tokens_valid:\n",
    "                    #print(f'{match}. All tokens present.')\n",
    "                    correct_matches.append(match)\n",
    "                else:\n",
    "                    extra_tokens = [token for token in match_tokens if token not in this_jersey_tokens and unidecode(token) not in this_jersey_tokens]\n",
    "                    if not extra_tokens:\n",
    "                        #print(f'{match}. All tokens present.')\n",
    "                        correct_matches.append(match)\n",
    "                    else:\n",
    "                        #print(f'{match}. Extra tokens: {extra_tokens}')\n",
    "                        correct_matches.append(match)\n",
    "    else: #single word\n",
    "        case_row = 'Single Word'\n",
    "        correct_matches = []\n",
    "        for match in matches:\n",
    "            match_tokens = match.split(' ')\n",
    "            last_token = match_tokens[-1]\n",
    "            if match == this_jersey:\n",
    "                correct_matches.append(match)\n",
    "            if match.endswith(this_jersey):\n",
    "                correct_matches.append(match)\n",
    "            elif unidecode(match).endswith(this_jersey):\n",
    "                correct_matches.append(match)\n",
    "            elif unidecode(match).startswith(this_jersey):\n",
    "                correct_matches.append(match)\n",
    "            elif match.startswith(this_jersey):\n",
    "                correct_matches.append(match)\n",
    "            elif len(last_token) == len(this_jersey) and sum(1 for a, b in zip(last_token, this_jersey) if a != b and a.lower() == 'z' and b.lower() == 's') == 1:\n",
    "                correct_matches.append(match) \n",
    "            elif len(last_token) == len(this_jersey) and sum(1 for a, b in zip(last_token, this_jersey) if a != b and a.lower() == 's' and b.lower() == 'z') == 1:\n",
    "                correct_matches.append(match)\n",
    "\n",
    "        if correct_matches == []:\n",
    "            for match in matches:\n",
    "                if this_jersey in match:\n",
    "                    correct_matches.append(match)\n",
    "            \n",
    "\n",
    "    if correct_matches == []: #Vietnam case\n",
    "        if nationality == 'Vietnam':\n",
    "            matches_copy = matches[:]\n",
    "            for match in matches_copy:\n",
    "                matches.remove(match)  # Remove the original match from matches\n",
    "                rearranged_match = rearrange_name_asian(match)  # Rearrange the name\n",
    "                #print(rearranged_match)\n",
    "                matches.append(rearranged_match)  # Add the rearranged name back to matches\n",
    "                if remove_vowels(rearranged_match) == this_jersey:\n",
    "                    \n",
    "                    correct_matches.append(match)\n",
    "    else:\n",
    "        if nationality == 'Vietnam':\n",
    "            matches_copy = matches[:]\n",
    "            for match in matches_copy:\n",
    "                \n",
    "                matches.remove(match)  # Remove the original match from matches\n",
    "                rearranged_match = rearrange_name_asian(match)  # Rearrange the name\n",
    "                #print(rearranged_match)\n",
    "                matches.append(rearranged_match)  # Add the rearranged name back to matches\n",
    "                if remove_vowels(rearranged_match) == this_jersey:\n",
    "                    \n",
    "                    correct_matches.append(match)\n",
    "\n",
    "    #print(f'player is {this_jersey}. after 1st round matches are {correct_matches}')\n",
    "    if len(correct_matches) >= 2: \n",
    "        #print('multiple matches', this_jersey, correct_matches)\n",
    "        if len(closest_string(this_jersey, dataset_nationality)) == 1: #was type == str\n",
    "            if closest_string(this_jersey, dataset_nationality) in correct_matches:\n",
    "                filtered_matches = filter_matches(correct_matches, closest_string(this_jersey, dataset_nationality))\n",
    "                second_list = [match for match in correct_matches if check_tokens(this_jersey, match)]\n",
    "                filtered_matches = list(set(filtered_matches + second_list))\n",
    "                if filtered_matches == []:\n",
    "                    print(\"x no good matches:\", filtered_matches)  \n",
    "                else:\n",
    "                    #print(\"✔ Filtered matches:\", filtered_matches) #Success\n",
    "                    status_row = 'Success'\n",
    "                    matches_final = filtered_matches\n",
    "            else:\n",
    "                filtered_matches = [match for match in correct_matches if check_tokens(this_jersey, match)]\n",
    "                if filtered_matches == []:\n",
    "                    for name in correct_matches:\n",
    "                        if (this_jersey_tokens[-1] in unidecode(name)) and any(token.startswith(this_jersey_tokens[0]) for token in name.split(' ')):\n",
    "                            filtered_matches.append(name)\n",
    "                        elif (unidecode(this_jersey_tokens[-1]) in unidecode(name)) and any(token.startswith(this_jersey_tokens[0]) for token in name.split(' ')):\n",
    "                            filtered_matches.append(name)\n",
    "                    if filtered_matches == []:\n",
    "                        for match in correct_matches:\n",
    "                            match_tokens = re.split(r'[\\s-]', match)\n",
    "                            if match_tokens[-1].lower() == this_jersey_tokens[-1].lower() and match_tokens[0][0].lower() == this_jersey_tokens[0][0].lower():\n",
    "                                filtered_matches.append(match)\n",
    "                        \n",
    "                        if filtered_matches == []:\n",
    "                            0==0\n",
    "                            if len(this_jersey_tokens) >= 2:\n",
    "                                \n",
    "                                token_counts = {token[0]: 0 for token in this_jersey_tokens}\n",
    "        \n",
    "                                # Iterate through each match in correct_matches\n",
    "                                for match in correct_matches:\n",
    "                                    # Create a copy of token_counts for each match\n",
    "                                    match_token_counts = token_counts.copy()\n",
    "                                    \n",
    "                                    # Iterate through each token in the match\n",
    "                                    for token in match:\n",
    "                                        # Check if the token's first letter is in match_token_counts\n",
    "                                        if token[0] in match_token_counts:\n",
    "                                            # Increment the count for that letter\n",
    "                                            match_token_counts[token[0]] += 1\n",
    "                                    \n",
    "                                    # Check if all letters in this match have at least one token\n",
    "                                    if all(count > 0 for count in match_token_counts.values()):\n",
    "                                        # Do something if all letters in this match have at least one token\n",
    "                                        #print(match, \"All letters in this match have at least one token.\")\n",
    "                                        filtered_matches.append(match)\n",
    "                                        if match == 'Ngoc Hai Que':\n",
    "                                            status_row = 'Success'\n",
    "                                            matches_final = [match]\n",
    "\n",
    "                                if filtered_matches == []:\n",
    "                                    0==0\n",
    "                                    #print(\"x No good matches\", filtered_matches)\n",
    "                        else:\n",
    "                            #print(\"✔ Matches to keep:\", filtered_matches) #Success\n",
    "                            status_row = 'Success'\n",
    "                            matches_final = filtered_matches\n",
    "                    else:\n",
    "                        #print(\"✔ Matches to keep:\", filtered_matches) #Success\n",
    "                        status_row = 'Success'\n",
    "                        matches_final = filtered_matches\n",
    "                else:\n",
    "                    #print(\"✔ Matches to keep:\", filtered_matches) #Success\n",
    "                    status_row = 'Success'\n",
    "                    matches_final = filtered_matches\n",
    "        else: #returned multiple people \n",
    "            last_name = this_jersey.split()[-1]\n",
    "            matching_names = [name for name in dataset_nationality if name.split()[-1] == last_name]\n",
    "            if matching_names:\n",
    "                matches_final = closest_string(this_jersey.split(' ')[-1], dataset_nationality)\n",
    "            else:\n",
    "                matches_final = closest_string(this_jersey, dataset_nationality)\n",
    "            if this_jersey == 'd Leon':\n",
    "                matches_final = correct_matches\n",
    "            status_row = 'Success'\n",
    "    elif len(correct_matches) == 1:\n",
    "        #print('✔', this_jersey, correct_matches) #Success\n",
    "        matches_final = correct_matches\n",
    "        status_row = 'Success'\n",
    "    elif len(correct_matches) == 0:\n",
    "        0==0\n",
    "        #print('no matches', this_jersey, matches)\n",
    "    if correct_matches == []:\n",
    "        if case_row == 'Single Word':\n",
    "            0==0\n",
    "        elif case_row == 'Multiple Words':\n",
    "            print('here')\n",
    "        elif case_row == 'Initial Plus Name':\n",
    "            0==0\n",
    "        else:\n",
    "            print('where')\n",
    "\n",
    "    if status_row == 'Success':\n",
    "        if len(matches_final) == 1:\n",
    "            if case_row == 'Multiple Words':\n",
    "                candidate = matches_final[0]\n",
    "                tokens_final_match = re.split(r'[\\s-]', candidate)\n",
    "                matching_tokens_count = sum(1 for token in tokens_final_match if token in this_jersey_tokens)\n",
    "                if matching_tokens_count / len(tokens_final_match) < 0.5:\n",
    "                    status_row = 'Lookup Still Required'\n",
    "                    #print('X - reversed success', this_jersey, matches_final)\n",
    "                    matches_final = []\n",
    "                    this_jersey_tokens_new = re.split(r'[\\s-]', this_jersey)\n",
    "                    matching_tokens_count_new = sum(1 for token in tokens_final_match if token in this_jersey_tokens_new)\n",
    "                    if matching_tokens_count_new / len(tokens_final_match) < 0.5:\n",
    "                        0==0\n",
    "                    else:\n",
    "                        status_row = 'Success'\n",
    "                        matches_final = [candidate]\n",
    "                if len(tokens_final_match) == 2:\n",
    "                    if matching_tokens_count == 1:\n",
    "                        status_row = 'Lookup Still Required'\n",
    "                        matches_final = [this_jersey, candidate]\n",
    "        else:\n",
    "            if len(this_jersey_tokens) > 1:\n",
    "                if case_row == 'Initial Plus Name':\n",
    "                    # Iterate over each item in matches_final\n",
    "                    \n",
    "                    for item in matches_final:\n",
    "                        # Split the item into words\n",
    "                        words = item.split()\n",
    "                        # Check if at least one word, except the last one, starts with this_jersey_tokens[0]\n",
    "                        if any(word.startswith(this_jersey_tokens[0]) for word in words[:-1]):\n",
    "                            # If at least one word, except the last one, starts with this_jersey_tokens[0], do nothing\n",
    "                            pass\n",
    "                        else:\n",
    "                            # If no word, except the last one, starts with this_jersey_tokens[0], remove the item from matches_final\n",
    "                            matches_final.remove(item)\n",
    "                                                                                                            # for match in matches_final:\n",
    "                                                                                                            #     last_token_word = match.split(' ')[-1]\n",
    "                                                                                                            #     print(last_token_word)\n",
    "                                                                                                            #     if last_token_word == this_jersey_tokens[-1]:\n",
    "                                                                                                            #         print('der')\n",
    "                                                                                                            #get the last token of the word \n",
    "                                                                                                            #if the last token not the same as the last token \n",
    "\n",
    "    if status_row == 'Lookup Still Required':\n",
    "        if len(correct_matches) == 1:\n",
    "            if correct_matches[0] == row['Name(s) Found']:\n",
    "                matches_final = correct_matches[0]\n",
    "                # Extracting last tokens\n",
    "                this_jersey_last_token = this_jersey.split()[-1]\n",
    "                correct_matches_last_token = correct_matches[0].split()[-1]\n",
    "                row_name_found_last_token = row['Name(s) Found'].split()[-1]\n",
    "                # Check if both correct_matches_last_token and row_name_found_last_token are different from this_jersey_last_token\n",
    "                if correct_matches_last_token != this_jersey_last_token and row_name_found_last_token != this_jersey_last_token:\n",
    "                    matches_final = [this_jersey, correct_matches[0]]                \n",
    "            elif row['Name(s) Found'] == closest_string(this_jersey, dataset_nationality)[0]:\n",
    "                matches_final = row['Name(s) Found']\n",
    "            \n",
    "            # Extracting last tokens\n",
    "            this_jersey_tokens = this_jersey.split()[-1]\n",
    "            if len(matches_final) == 1:\n",
    "                matches_final_tokens = matches_final[0].split()[-1]\n",
    "            correct_matches_tokens = correct_matches[0].split()[-1]\n",
    "\n",
    "            # Check conditions\n",
    "            if this_jersey_tokens != matches_final_tokens and ((this_jersey_tokens == correct_matches_tokens) or \n",
    "            (count_diff_vowels(this_jersey_tokens, correct_matches_tokens) == 1)):\n",
    "                #check if the last token of this_jersey is different from the last token of matches_final, AND its the same as the last token of correct matches\n",
    "                matches_final = [correct_matches[0], filtered_matches , row['Name(s) Found'], this_jersey]\n",
    "\n",
    "\n",
    "    if len(matches_final) > len(correct_matches) and \\\n",
    "    not any(item == this_jersey for item in matches_final) and \\\n",
    "    not any(item == this_jersey for item in correct_matches):\n",
    "        filtered_matches_final = filter_names_by_tokens(matches_final, this_jersey)\n",
    "        filtered_correct_matches = filter_names_by_tokens(correct_matches, this_jersey)\n",
    "        #print(\"matches_final is longer than correct_matches and neither list has any items that are exact matches with this_jersey\")\n",
    "        #print(filtered_correct_matches, filtered_matches_final)\n",
    "        matches_final = list(set(filtered_correct_matches + filtered_matches_final))\n",
    "        status_row = 'Lookup Still Required'\n",
    "\n",
    "    if status_row != 'Success':\n",
    "        for index, string in enumerate(matches_final):\n",
    "            if string == this_jersey:\n",
    "                status_row = 'Success'\n",
    "                matches_final = [string]  # Set matches_final to only contain the matching string\n",
    "                break\n",
    "\n",
    "    if status_row == 'Fail':\n",
    "        if this_jersey == row['Name(s) Found']:\n",
    "            matches_final = row['Name(s) Found']\n",
    "            status_row = 'Success'\n",
    "\n",
    "    print(f\"jersey is {this_jersey}. STATUS = {status_row}.\")\n",
    "    if status_row == 'Lookup Still Required':\n",
    "        print(f\"FIRST ROUND WAS {correct_matches}, 2nd was {filtered_matches}. FINAL ROUND WAS {matches_final}. Row said {row['Name(s) Found']}\")\n",
    "    elif status_row == 'Success':\n",
    "        print('FINAL MATCHES WERE', matches_final)\n",
    "    else:\n",
    "        print(f\"FIRST ROUND WAS {correct_matches}, 2nd was {filtered_matches}. FINAL ROUND WAS {matches_final}. Row said {row['Name(s) Found']}\")\n",
    "\n",
    "#1200 -end\n",
    "        #1210 - in case where it fails. if the name in the row equals the name on the jersey go with that name \n",
    "\n",
    "\n",
    "\n",
    "#ONCE READY\n",
    "\n",
    "#change sim sang min row name to sim sang min cause thats correct \n",
    "\n",
    "    \n",
    "    #IF ITS FAIL LOOK UP THE JERSEY (ONLINE)\n",
    "    #IF ITS LOOKUP STILL REQUIRED LOOK UP THE FINAL MATCHES (ONLINE), and the thing in the row['Name(s) Found']\n",
    "    #IF ITS SUCCESS \n",
    "            #IF MORE THAN 1 NAME FILTER USING AMTCH DATE\n",
    "                #if that leaves nobody u have to look up the OG jersey\n",
    "        #ONCE U HAVE 1 NAME FIND IN DB \n",
    "\n",
    "#provided you have a name \n",
    "    #look up that name + that season \n",
    "    #if there are no rows, you have to look them up online \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#done cases \n",
    "\n",
    "#work on row 35\n",
    "    #case is initial + name \n",
    "    #the match did start with Initial, and it ended with name but the real name has an extra U that the jersey was missing \n",
    "                \n",
    "    #so if it starts with the initial and ends with a name 1 character differnt from the last name, that's ok too. \n",
    "\n",
    "#row 53 \n",
    "    #if the name is only off between an S and a Z it should still be good \n",
    "\n",
    "#row 60 - single name \n",
    "    #if the MATCH was the closest name in the list. or maybe if the MATCH was the only word in dataset_nationality that contained the jersey_name\n",
    "\n",
    "#row 62 - single name\n",
    "    #REMEMBER TO ACCOUNT FOR ACCENTS \n",
    "\n",
    "#row 97 - if the jersey is S Cruz keep any matches ending in Santa Cruz\n",
    "\n",
    "#222 - s and z thing should work \n",
    "\n",
    "\n",
    "#D Santos - 245\n",
    "    #allow the token in match to start with the lower case version of the initial token   \n",
    "\n",
    "#286 - d Leon\n",
    "   #286 - the d Leon thing. i no longer get all the de Leon people  - fixed w ad hoc work \n",
    "\n",
    "#293 - Mohamed Eisa\n",
    "    #if the last name is the same\n",
    "    #and the first name starts with the same 2 letters\n",
    "    #that should count \n",
    "\n",
    "#296 was no longer working but should be. the Mo kisa one \n",
    "\n",
    "#305   \n",
    "    #this one should not work \n",
    "            \n",
    "#309\n",
    "    #SAME AS 305. SHOULD WORK BUT THEN TELL YOU IT'S NOT A GREAT MATCH AND TO LOOK IT UP AFTER \n",
    "\n",
    "#340 / 326\n",
    "    #should return both people\n",
    "\n",
    "    #374 - filter these names only one should remain       \n",
    "\n",
    "\n",
    "#389\n",
    "    #should return both of those people.\n",
    "\n",
    "\n",
    "#474 and 470. IF THE NAME IN THE NAME(S) FOUND COLUMN IS THE SAME AS CORRECT MATCHES OR FILTERED MATCHES U GOTTA GO WITH THAT NAME \n",
    "\n",
    "    #478 save the names from close matches and filtered matches when it fails so you can try them later.      \n",
    "\n",
    "    #589 should return names to search\n",
    "\n",
    "#500-600\n",
    "    #598 and #599 should return names to search\n",
    "        #if the name is 2 names and you can only match 1 token \n",
    "            #and the non-matching tokens arent the same name\n",
    "            #you should look up both names then. the jersey name and the one it found \n",
    "\n",
    "#611 should not work.\n",
    "\n",
    "        #677 should only retunr 1 guy\n",
    "\n",
    "#759 should not work.\n",
    "\n",
    "        #752/738 thats the wrong person \n",
    "\n",
    "        #851 one of the matches is actually the right player \n",
    "\n",
    "        #879 feels match-able  - ad hoc fix\n",
    "\n",
    "        #890 should be fixed by #374\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####LOGIC######\n",
    "            \n",
    "                                                            #FIRST FILTER - DONE\n",
    "                                                                #if the name is initial + name - DONE\n",
    "                                                                    \n",
    "                                                                #if its multiple words in the name - DONE\n",
    "                                                                    \n",
    "                                                                #IF JUST A NAME - DONE\n",
    "\n",
    "                                                                #multi-word names. that viet case - DONE\n",
    "\n",
    "#SECOND FILTER \n",
    "    #check if the name it found is a better match than waht's in the name column \n",
    "        #if they're the same or if it is a better match move ahead with the new name \n",
    "\n",
    "        #if the row is a better match, but theyre both shite, what do you do? \n",
    "#correct_matches#match_tokens#this_jersey_tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_lookup_multi_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#online lookup required - multiple names found \n",
    "\n",
    "\n",
    "#if multiple names were found\n",
    "\n",
    "    #if the Jersey has an initial \n",
    "        #keep all matches that start with the initial \n",
    "\n",
    "        #except if the initial is a lowercase d. then, make sure there is a short word (3 characters or less) starting with d followed by the Name \n",
    "        #so if its d Silva make sure theres a word startign with d then Silva soemwhere in the name. \n",
    "\n",
    "    #sadiqov case - what if the names are the same???\n",
    "        #in this case you should just look up each name and use like whichever has the record from the saeason of the game \n",
    "\n",
    "    #do the match date lookup thing with each of the matches to check who played in that game. \n",
    "        #if you still have more than 1 name\n",
    "\n",
    "            #look at the success dataset to find the record of each remaining guy\n",
    "                #if its ivanovic and you find both B and D Ivanovic played according to transfermarkt\n",
    "                \n",
    "                #check if there's a record of those guys in that match / date (in our dataset)\n",
    "                    \n",
    "                    #if all but one guy is in the success dataset, the guy who is not there is the name you need to use.\n",
    "\n",
    "                    #if none remain after checking the dataset just remove this row. but print that out. that would be weird\n",
    "\n",
    "                    #if multiple remain after checking the success dataset(there were 3 names remaining, only found 1 record)\n",
    "                        #print this out this is a special case.\n",
    "\n",
    "        #if you are just left with one name using the match date lookup \n",
    "\n",
    "            #that's a success - find that guy in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_lookup_0_found\n",
    "\n",
    "\n",
    "#look the guy up with no nationality in the offline DB\n",
    "\n",
    "#look him up online with the season in question "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CYRILLIC GUYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper methods\n",
    "\n",
    "def query_google_1(input_string):\n",
    "    # Replace spaces with plus signs to format the query string\n",
    "    query = '+'.join(input_string.split())\n",
    "\n",
    "    # URL for Google search\n",
    "    url = f\"https://www.google.com/search?q={query}\"\n",
    "\n",
    "    # Send GET request to Google\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Return the parsed HTML content\n",
    "        return soup\n",
    "    else:\n",
    "        # If request fails, print error message\n",
    "        print(\"Failed to fetch results from Google.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def find_name_in_language_translate(name_1, language):\n",
    "    status = 'Fail'\n",
    "    name = \"\"\n",
    "    input_string = f\"translate Russian to {language}:{name_1}\"\n",
    "    try:\n",
    "        resulting_html = query_google_1(input_string)\n",
    "    except Exception as e:\n",
    "        print(\"Couldn't get the right HTML:\", e)\n",
    "        return name, status\n",
    "    \n",
    "    try:\n",
    "        body_jsmodel = resulting_html.find('body', attrs={'jsmodel': 'hspDDf '})\n",
    "        potential_div = body_jsmodel.find_all('div')[7]\n",
    "        #set_of_divs = potential_div.find_all('div') \n",
    "    except Exception as e:\n",
    "        print(\"Could not find the correct divs:\", e)\n",
    "        return name, status\n",
    "\n",
    "    try:\n",
    "        # Find the div with the specified class name\n",
    "        div_with_class = potential_div.find('div', class_='Va3FIb')\n",
    "        # Or if you want to find all divs with the specified class name\n",
    "        divs_with_class = potential_div.find_all('div', class_='Va3FIb')\n",
    "\n",
    "        # Check if the div is found\n",
    "        if div_with_class:\n",
    "            status = 'Success'\n",
    "            #print(div_with_class)\n",
    "            name = (divs_with_class[0].text)\n",
    "        else:\n",
    "            print(\"Div with class 'Va3FIb' not found.\")\n",
    "            name = \"\"\n",
    "    except Exception as e:\n",
    "        print(\"Could not find the correct divs:\", e)\n",
    "    \n",
    "    return name, status\n",
    "\n",
    "def find_transfermarkt_links(html_content):\n",
    "    # Regular expression patterns for transfermarkt URLs\n",
    "    transfermarkt_patterns = [\n",
    "        r\"transfermarkt\\.de\",\n",
    "        r\"transfermarkt\\.us\",\n",
    "        r\"transfermarkt\\.world\"\n",
    "    ]\n",
    "    transfermarkt_links = []\n",
    "\n",
    "    # Find all 'a' tags in the HTML content\n",
    "    links = html_content.find_all('a', href=True)\n",
    "\n",
    "    # Filter links that match transfermarkt patterns\n",
    "    for link in links:\n",
    "        for pattern in transfermarkt_patterns:\n",
    "            if re.search(pattern, link['href']):\n",
    "                transfermarkt_links.append(link['href'])\n",
    "                break\n",
    "\n",
    "    return transfermarkt_links\n",
    "\n",
    "\n",
    "def extract_name_from_link(link):\n",
    "        \n",
    "    name_in_link = link.split('/')[3]\n",
    "    name_in_link_tokens = name_in_link.split('-')\n",
    "    capitalized_tokens = []\n",
    "\n",
    "    for token in name_in_link_tokens:\n",
    "        capitalized_tokens.append(token.capitalize())\n",
    "\n",
    "    combined_string = ' '.join(capitalized_tokens)\n",
    "\n",
    "    return combined_string\n",
    "\n",
    "\n",
    "# def add_spaces_to_middle_caps(string):\n",
    "#     # Split the string into tokens\n",
    "#     tokens = string.split()\n",
    "    \n",
    "#     # Check if there's only one token\n",
    "#     if len(tokens) == 1:\n",
    "#         token = tokens[0]\n",
    "        \n",
    "#         # Check if there are capital letters in the middle of the token\n",
    "#         if any(c.isupper() for c in token[1:-1]):\n",
    "#             # Add spaces before each capital letter in the middle\n",
    "#             modified_token = ''\n",
    "#             for char in token:\n",
    "#                 if char.isupper() and token.index(char) != 0 and token.index(char) != len(token) - 1:\n",
    "#                     modified_token += ' ' + char\n",
    "#                 else:\n",
    "#                     modified_token += char\n",
    "#             return modified_token\n",
    "#     return string\n",
    "def add_spaces_to_middle_caps(string):\n",
    "    # Split the string into tokens\n",
    "    tokens = string.split()\n",
    "    \n",
    "    # Check if there's only one token\n",
    "    if len(tokens) == 1:\n",
    "        token = tokens[0]\n",
    "        \n",
    "        # Check if there are capital letters in the middle of the token\n",
    "        if any(c.isupper() for c in token[1:-1]):\n",
    "            # Add spaces before each capital letter in the middle\n",
    "            modified_token = ''\n",
    "            for index, char in enumerate(token):\n",
    "                if char.isupper() and index != 0 and index != len(token) - 1:\n",
    "                    modified_token += ' ' + char\n",
    "                else:\n",
    "                    modified_token += char\n",
    "            return modified_token\n",
    "    return string\n",
    "\n",
    "from translate import Translator\n",
    "\n",
    "def translate_cyrillic_to_english(text):\n",
    "    translator = Translator(to_lang=\"en\", from_lang=\"ru\")\n",
    "    translation = translator.translate(text)\n",
    "    return translation\n",
    "\n",
    "\n",
    "\n",
    "def process_google_query(input_text):\n",
    "    html_result = query_google_1(input_text)\n",
    "    if html_result:\n",
    "        transfermarkt_links = find_transfermarkt_links(html_result)\n",
    "        if transfermarkt_links:\n",
    "            names_links = []\n",
    "            for link in transfermarkt_links:\n",
    "                if link.startswith('/url?q='):\n",
    "                    link = link.split('/url?q=')[1]\n",
    "                name_from_link = extract_name_from_link(link)\n",
    "                names_links.append(name_from_link)\n",
    "            if len(names_links) == 1:\n",
    "                name = names_links[0]\n",
    "                return name\n",
    "            elif len(names_links) >= 2:\n",
    "                name_list_matches = list(set(names_links))\n",
    "                return name_list_matches\n",
    "            else:\n",
    "                return 0  # Nobody found\n",
    "        else:\n",
    "            return 0  # No links found\n",
    "    else:\n",
    "        return 0  # No HTML result\n",
    "\n",
    "def vet_tokens_names(name, player):\n",
    "    player_original = player.copy()\n",
    "    original_name_tokens = name.split(' ')\n",
    "    print(original_name_tokens)\n",
    "    for player_individual in player:\n",
    "        #print(player)\n",
    "        this_player_tokens = player_individual.split(' ')\n",
    "        \n",
    "        for token in original_name_tokens:\n",
    "            if token == '':\n",
    "                0==0\n",
    "            else:\n",
    "                first_letter = token[0]\n",
    "                #print(initial_token)\n",
    "                token_found = False\n",
    "                for token_player in this_player_tokens:\n",
    "                    if token_player.startswith(first_letter):\n",
    "                        \n",
    "                        if levenshtein_distance(token_player, token) >= len(token_player):\n",
    "                            #print(f'did not match {token_player} with {token}')\n",
    "                            0==0\n",
    "                        else:\n",
    "                            #print(f'matched {token_player} with {token}')\n",
    "                            #print(token_player)\n",
    "                            token_found = True\n",
    "                            break\n",
    "                if not token_found:\n",
    "                    #print(player)\n",
    "                    if player_individual in player:\n",
    "                        player.remove(player_individual)\n",
    "    #print('banana', player)\n",
    "    if player  == []:\n",
    "        player = player_original\n",
    "        #print('razz', player)\n",
    "        for player_individual in player:\n",
    "            this_player_tokens = re.split(r'\\s+|-|–', player_individual)\n",
    "            #print('apple', this_player_tokens)\n",
    "            for token in original_name_tokens:\n",
    "                if token == '':\n",
    "                    0==0\n",
    "                else:\n",
    "                    first_letter = token[0]\n",
    "                    token_found = False\n",
    "                    for token_player in this_player_tokens:\n",
    "                        if token_player.startswith(first_letter):\n",
    "                            #print('pine', token_player)\n",
    "                            if levenshtein_distance(token_player, token) >= len(token_player):\n",
    "                                #print(f'did not match {token_player} with {token}')\n",
    "                                0==0\n",
    "                            else:\n",
    "                                #print(f'matched {token_player} with {token}')\n",
    "                                #print(token_player)\n",
    "                                token_found = True\n",
    "                                break\n",
    "                if not token_found:\n",
    "                    player.remove(player_individual)\n",
    "                    break\n",
    "\n",
    "    print(player)\n",
    "    return player\n",
    "\n",
    "def remove_duplicates(input_list):\n",
    "    \"\"\"\n",
    "    Removes duplicates from a list of strings.\n",
    "\n",
    "    Args:\n",
    "    input_list (list): List of strings with possible duplicates.\n",
    "\n",
    "    Returns:\n",
    "    list: List with duplicates removed.\n",
    "    \"\"\"\n",
    "    # Use set to keep track of unique strings\n",
    "    unique_strings = set()\n",
    "\n",
    "    # List to store the result\n",
    "    result = []\n",
    "\n",
    "    for string in input_list:\n",
    "        # If the string is not already in the set, add it to the result list\n",
    "        if string not in unique_strings:\n",
    "            result.append(string)\n",
    "            unique_strings.add(string)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding extra country names \n",
    "country_to_language.update({\n",
    "    \"Andorra\": \"Catalan\",\n",
    "    \"Montenegro\": \"Montenegrin\",\n",
    "    \"Bosnia\": \"Bosnian\",\n",
    "    \"Scotland\": \"English\",\n",
    "    \"Cyprus\": \"Greek\",\n",
    "    \"Wales\": \"Welsh\",\n",
    "    \"North Macedonia\": \"Macedonian\",\n",
    "    \"N. Macedonia\": \"Macedonian\",\n",
    "    \"Gibraltar\": \"English\",\n",
    "    \"England\": \"English\",\n",
    "    \"San Marino\": \"Italian\",\n",
    "    \"Northern Ireland\": \"English\",\n",
    "    \"North. Ireland\": \"English\",\n",
    "    \"Faroe Islands\": \"Faroese\",\n",
    "    \"Liechtenstein\": \"German\",\n",
    "    \"Czech\": \"Czech\",\n",
    "    \"Korea Republic\": \"Korean\",\n",
    "    \"Ivory Coast\": \"French\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyrillic_names_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cyrillic try 2 \n",
    "cyrillic_df = pd.DataFrame()\n",
    "\n",
    "for i in range(2800, 2844): \n",
    "    print(f'***{i}***')\n",
    "    player = []\n",
    "    part_3_match = ''\n",
    "    changed_name = ''\n",
    "    name_found_in_dict = False\n",
    "    correct_matches = []\n",
    "    match_case = 'None'\n",
    "    final_match_status = 'Fail'\n",
    "    main_word = ''\n",
    "    status = 'Fail'\n",
    "    index = i#15\n",
    "    row = cyrillic_guys.iloc[index]\n",
    "    this_country_code = row['Team Country Code']\n",
    "    jersey = row['ORIGINAL JERSEY']\n",
    "    season = row['Season']\n",
    "    jersey = ''.join(filter(str.isalpha, jersey))\n",
    "    country_name = countries_codes[countries_codes[' Code'] == this_country_code]['Country'].unique()[0]\n",
    "\n",
    "    dataset_nationality = leagues_value[leagues_value['Team 1 Code'] == this_country_code]['Name'].unique()\n",
    "    dataset_nationality_unidecoded = {unidecode(name) for name in dataset_nationality}\n",
    "\n",
    "    if i == 530:\n",
    "        jersey = 'Кашаро Л'\n",
    "    elif i == 1496:\n",
    "        jersey = 'John Mintoff'\n",
    "    elif i == 2416:\n",
    "        jersey = 'Jackson Porozo'\n",
    "    elif i == 2427:\n",
    "        jersey = 'Jackson Porozo'\n",
    "    elif i == 2476:\n",
    "        jersey = 'Jackson Porozo'\n",
    "\n",
    "    if AssemblyHelpers.is_cyrillic(jersey):\n",
    "        #print('jersey is cyrillic')\n",
    "\n",
    "        key = f\"{jersey}:{country_name}\"\n",
    "\n",
    "        if key in cyrillic_names_dict.keys():\n",
    "            name = cyrillic_names_dict[key][1]\n",
    "            name_found_in_dict = True\n",
    "            status = 'Success'\n",
    "            print(f'{key} was in dict. got {name}')\n",
    "        else:\n",
    "            name, status = find_name_in_language_translate(jersey, 'English')\n",
    "\n",
    "            html_result = query_google_1(f\"{jersey} {country_name} transfermarkt\")\n",
    "            if name == 'Next >':\n",
    "                name = ''\n",
    "                if html_result:\n",
    "                    #print('result')\n",
    "                    transfermarkt_links = find_transfermarkt_links(html_result)\n",
    "                    names_links = []\n",
    "                    correct_names = []\n",
    "                    #print(\"Transfermarkt Links:\")\n",
    "                    for link in transfermarkt_links:\n",
    "                        if link.startswith('/url?q='):\n",
    "                            link = link.split('/url?q=')[1]\n",
    "                        name_from_link = extract_name_from_link(link)\n",
    "                        names_links.append(name_from_link)\n",
    "                        #print(name_from_link)\n",
    "                    if len(names_links) == 1:\n",
    "                        name = names_links[0]\n",
    "                        #return name\n",
    "                    elif len(names_links) >= 2:\n",
    "                        #multiple names found for this name\n",
    "                        for name_link in names_links:\n",
    "                            if name_link in dataset_nationality:\n",
    "                                #print(name_link)\n",
    "                                correct_names.append(name_link)\n",
    "                            elif name_link in dataset_nationality_unidecoded:\n",
    "                                #print(closest_string(name_link, dataset_nationality))\n",
    "                                correct_names.append(name_link)\n",
    "                        correct_names = list(set(correct_names))\n",
    "                        if len(correct_names) == 1:\n",
    "                            name = correct_names[0]\n",
    "                            status = 'Success'\n",
    "                            #return name\n",
    "                        elif len(correct_names) >= 2:\n",
    "                            print('multiple matches here', correct_names)\n",
    "                            #return correct_names\n",
    "                        else:\n",
    "                            print('no matches', jersey)\n",
    "                            status = 'Fail'\n",
    "                            #return 0\n",
    "                    else:\n",
    "                        0==0#nobody found \n",
    "                        #return 0\n",
    "                else:\n",
    "                    print('nO HTML')\n",
    "                    0==0#no html\n",
    "\n",
    "            if name == 'Tip: Search for English results only. You can specify your search language in Preferences':\n",
    "                name = jersey\n",
    "                #break\n",
    "            if 'Did you mean: translate Russian to English:' in name:\n",
    "                name = name.split('Did you mean: translate Russian to English:')[1]\n",
    "            elif 'Russian to English:' in name:\n",
    "                name = name.split('Russian to English:')[1]\n",
    "            elif 'to English:' in name:\n",
    "                name = name.split('to English:')[1]\n",
    "    \n",
    "            if ((AssemblyHelpers.is_cyrillic(name)) | (name == '')):\n",
    "                #status = 'Fail'\n",
    "                name = translate_cyrillic_to_english(jersey)\n",
    "                if ((AssemblyHelpers.is_cyrillic(name)) | (name == '')):\n",
    "                    0==0 #it didnt work\n",
    "                else:\n",
    "                    status = 'Success'\n",
    "                    #print('used cyrillic translator, got', name)\n",
    "    \n",
    "            if name[-1].isupper(): # Separate capital letters from the rest of the string\n",
    "                for i in range(len(name)-1, -1, -1):\n",
    "                    if name[i].islower():\n",
    "                        main_word = name[:i+1]\n",
    "                        initial_letters = name[i+1:]\n",
    "                        break\n",
    "                name = initial_letters + ' ' + main_word\n",
    "    \n",
    "            if name[0] == ' ':\n",
    "                name = name.lstrip()\n",
    "\n",
    "\n",
    "            if ((len(name.split(' ')) == 1) & (any(c.isupper() for c in name[1:-1]))):\n",
    "               # print(name, 'yoooe')\n",
    "                name = add_spaces_to_middle_caps(name)\n",
    "            elif any(c.isupper() for c in name[1:-1]):\n",
    "                name = add_spaces_to_middle_caps(name)\n",
    "    else:\n",
    "        #print('jersey not cyrillic ')\n",
    "        key = f\"{jersey}:{country_name}\"\n",
    "\n",
    "        if key in cyrillic_names_dict.keys():\n",
    "            name = cyrillic_names_dict[key][1]\n",
    "            name_found_in_dict = True\n",
    "            status = 'Success'\n",
    "            print(f'{key} was in dict. got {name}')\n",
    "        else:\n",
    "            name = jersey\n",
    "            status = 'Success'\n",
    "    #print('gasp')\n",
    "    #print('b4 checking for cyrillic name, name is ', name)\n",
    "    if ((AssemblyHelpers.is_cyrillic(name)) | (name == '- none -')):\n",
    "        print(name, 'MATCH WAS CYRILLIC')\n",
    "        #print(status)\n",
    "        name = process_google_query(jersey)\n",
    "        \n",
    "        #if this one yields no results try this one below.\n",
    "\n",
    "        if name == 0: \n",
    "    \n",
    "            name = process_google_query(f\"{jersey} {country_name} transfermarkt\")\n",
    "\n",
    "    \n",
    "    if name == '':\n",
    "        status = 'Fail'\n",
    "    elif name[0] == ' ':\n",
    "        name = name.lstrip()\n",
    "\n",
    "    #print(f'b4 checking for status, name is {name} and status is {status}')\n",
    "    if status == 'Success':\n",
    "\n",
    "        if name_found_in_dict == True:\n",
    "            player = cyrillic_names_dict[key][0]\n",
    "        else:\n",
    "            #print(f'looking for exact. name is {name}, player is {player}')\n",
    "            if type(name) == list:\n",
    "                player = name\n",
    "            else:\n",
    "                #look for exact matches\n",
    "                player = [nationality for nationality in dataset_nationality if name in nationality or name in unidecode(nationality)]\n",
    "\n",
    "                if player != []:\n",
    "                    player = vet_tokens_names(name, player) #use tokens to vet names \n",
    "                    #print('gasp1')\n",
    "\n",
    "                else:\n",
    "                    #print(f'no exact matches. name is {name}, player is {player}')\n",
    "                    player = closest_string(unidecode(name), dataset_nationality_unidecoded)\n",
    "                    #print('gasp2')\n",
    "\n",
    "                    if player != []:\n",
    "                        #print('hir')\n",
    "                        #print('there were inexact matches', player)\n",
    "                        player = vet_tokens_names(name, player)\n",
    "                        #print('gasp30')\n",
    "                    \n",
    "                    else:\n",
    "                        #print(f'no inexact matches either. name is {name}, player is {player}')\n",
    "                        if 'ch' in name:\n",
    "                            changed_name = name.replace('ch', 'ć')\n",
    "                        if 'kh' in name:\n",
    "                            changed_name = name.replace('kh', 'j')\n",
    "                        elif 'sell' in name:\n",
    "                            changed_name = name.replace('sell', 'zzell')\n",
    "                        elif (('nyan' in name) & (name.endswith('nyan'))):\n",
    "                            changed_name = name.replace('nyan', 'nian')\n",
    "                        elif 'es' in name:\n",
    "                            changed_name = name.replace('es', 'ez')\n",
    "                        elif 'iy' in name:\n",
    "                            changed_name = name.replace('iy', 'ij')\n",
    "                        elif name.endswith('shai'):\n",
    "                            changed_name = name.replace('shai', 'shaj')\n",
    "                        elif 'ck' in name:\n",
    "                            changed_name = name.replace('ck', 'k')\n",
    "                        elif 'Qu' in name:\n",
    "                            changed_name = name.replace('Qu', 'Kv')\n",
    "                        elif 'ny' in name:\n",
    "                            changed_name = name.replace('ny', 'nj')\n",
    "                        elif 'Jak' in name:\n",
    "                            changed_name = name.replace('Jak', 'Xhak')\n",
    "                        elif 'li' in name:\n",
    "                            changed_name = name.replace('li', 'lli')\n",
    "                        elif 'Ji' in name:\n",
    "                            changed_name = name.replace('Ji', 'Dji')\n",
    "                            if 'sity' in name:\n",
    "                                changed_name = changed_name.replace('sity', 'siti')\n",
    "                        elif \"Ga\" in name:\n",
    "                            changed_name = name.replace('Ga', 'Ha')\n",
    "                        elif 'yevich' in name:\n",
    "                            changed_name = name.replace('yevich', 'jević')\n",
    "                        if changed_name != '':\n",
    "                            player = closest_string(unidecode(changed_name), dataset_nationality_unidecoded)\n",
    "                            #print('gasp4', player)\n",
    "                            if player != []:\n",
    "                                name = changed_name\n",
    "                                0==0\n",
    "                                #player = vet_tokens_names(name, player)\n",
    "                                #print('gasp5')\n",
    "                            else:\n",
    "                                player = ''\n",
    "    print(f'b4 checking for finals, name is {name}, player is {player} and status is {status}')\n",
    "    if len(player) == 1:\n",
    "        player_real = threshold_player_match(player[0], dataset_nationality)[0]\n",
    "    elif player == []:\n",
    "        player_real = 'No Matches'\n",
    "    else:\n",
    "        player_real = 'Multiple matches'\n",
    "    if type(name) == list:\n",
    "        name = [word for word in name if word in dataset_nationality or unidecode(word) in dataset_nationality]\n",
    "    else:\n",
    "        part_3_match = threshold_player_match(name, dataset_nationality)[0]\n",
    "        if 'No matches found even with the lowest threshold.' in part_3_match:\n",
    "            0==0\n",
    "        else:\n",
    "            #print('koolio')\n",
    "            part_3_match = vet_tokens_names(name, part_3_match)\n",
    "\n",
    "            if part_3_match == []:\n",
    "                print('vetting tokens removed everybody from part 3 match')\n",
    "            else:\n",
    "                0==0\n",
    "                ##player = part_3_match\n",
    "                #player_real = part_3_match\n",
    "\n",
    "    # print('Part 1 gave:', name)\n",
    "    # print('Part 2 gave', player, player_real)\n",
    "    # print('Part 3 gave', part_3_match)\n",
    "\n",
    "    final_match = ''\n",
    "    if (('No matches found even with the lowest threshold.' in part_3_match) | (part_3_match == [])):\n",
    "        print(f'X - THRESHOLD MATCH DID NOT WORK. jersey was {jersey}, name was {name}, player was {player} and best match was {player_real}')\n",
    "    elif player_real == 'No Matches':\n",
    "        if len(part_3_match) == 1: #if the threshold match returns 1 guy \n",
    "            #find an example - ROW 10 SIKERO \n",
    "            print(f'X - PLAYER MATCHING DID NOT WORK. jersey was {jersey}, name was {name} and best match was a single, {part_3_match}')\n",
    "        elif len(part_3_match) >= 2: #if the threshold match returns 2+ guy \n",
    "            print(f'X - PLAYER MATCHING DID NOT WORK. jersey was {jersey}, name was {name} and best match was a MULTI, {part_3_match}')\n",
    "    else:\n",
    "        #print(f'derp {name} {player} {part_3_match}')\n",
    "\n",
    "        if len(player) == 1: #part 2 returns 1 player\n",
    "            if len(part_3_match) == 1: #part 3 returns 1 player\n",
    "                if ((player_real == part_3_match) | (player_real == part_3_match[0])):\n",
    "                    final_match = part_3_match\n",
    "                else:\n",
    "                    print('parts 2 and 3 each returned 1 name but they arent the same', player_real, part_3_match)\n",
    "            elif len(part_3_match) >= 2: #part 3 returns 2+ players\n",
    "                bad_list = []\n",
    "                #print('gasp1', part_3_match)\n",
    "                for name_candidate in part_3_match:\n",
    "                    \n",
    "                    if name_candidate == player_real[0] or unidecode(name) == player_real[0]:\n",
    "                        0==0\n",
    "                    else:\n",
    "                        bad_list.append(name_candidate)\n",
    "                        #part_3_match.remove(name_candidate)\n",
    "                #print('gasp2', bad_list)\n",
    "                for bad_name in bad_list:\n",
    "                    part_3_match.remove(bad_name)\n",
    "                #print('gasp3', part_3_match)\n",
    "                if part_3_match == []:\n",
    "                    print('part 2 returned a single, part 3 returned a list, no overlap.')\n",
    "                else:\n",
    "                    final_match = part_3_match\n",
    "                    \n",
    "        elif len(player) >= 2: #part 2 returns 2+ players\n",
    "            if len(part_3_match) == 1: #part 3 returns 1 player\n",
    "                bad_list = []\n",
    "                if type(player) != str:\n",
    "                    for name_candidate in player:\n",
    "                        #print(name)\n",
    "                        if name_candidate != part_3_match[0]:\n",
    "                            #print('pikel')\n",
    "                            bad_list.append(name_candidate)\n",
    "                    for name_candidate_bad in bad_list:\n",
    "                        player.remove(name_candidate_bad)\n",
    "                else:\n",
    "                    if player_real == part_3_match[0]:\n",
    "                        final_match = part_3_match\n",
    "                \n",
    "                if player == []:\n",
    "                    print('part 2 returned a list, part 3 returned a single, no overlap.')\n",
    "                else:\n",
    "                    player_real = threshold_player_match(player[0], dataset_nationality)\n",
    "                    final_match = player_real\n",
    "            elif len(part_3_match) >= 2:\n",
    "                #print('both attempts returned lists', player, part_3_match)\n",
    "                #SEE ROW 2. \n",
    "\n",
    "                #MAKE A COMBINED SET ALL THE ONES IN BOTH LISTS. \n",
    "                final_match = [string for string in part_3_match if string in player or unidecode(string) in player]\n",
    "                \n",
    "                #DO A MATCH DATE FILTER with result list. \n",
    "            else:\n",
    "                0==0\n",
    "    if player == []:\n",
    "        if threshold_player_match(name, dataset_nationality)[0] == part_3_match:\n",
    "            final_match = part_3_match\n",
    "            #print('ceeeeeeele')\n",
    "\n",
    "    if player == '':\n",
    "        if threshold_player_match(name, dataset_nationality)[0] == part_3_match:\n",
    "            final_match = part_3_match\n",
    "\n",
    "    if ((player == '') & (part_3_match == [])):\n",
    "        0==0\n",
    "    else:\n",
    "        if type(player) == str:\n",
    "            player = [player]\n",
    "        elif type(part_3_match) == str:\n",
    "            part_3_match = [part_3_match]\n",
    "            #print(part 3 match was a string)\n",
    "        final_list_to_check = player + part_3_match\n",
    "        #final_list_to_check = list(set(final_list_to_check))\n",
    "        if type(name) == list:\n",
    "            final_list_to_check = final_list_to_check + name\n",
    "            final_match = [word for word in final_list_to_check if unidecode(word) in dataset_nationality_unidecoded]\n",
    "            final_match = list(set(final_match))\n",
    "        else:\n",
    "            final_match = [word for word in final_list_to_check if unidecode(name) in unidecode(word)]\n",
    "\n",
    "    if final_match == []:\n",
    "        if len(part_3_match) == 1:\n",
    "            part_3_match = part_3_match[0]\n",
    "        if name == '' or name == []:\n",
    "            0==0\n",
    "        else:\n",
    "            if ((threshold_player_match(name, dataset_nationality)[0] == part_3_match) | (part_3_match in threshold_player_match(name, dataset_nationality)[0])):\n",
    "                final_match = part_3_match\n",
    "\n",
    "    if ((final_match != '') & (final_match != [])):\n",
    "        print(f\"jersey said {jersey}. name said {name}\")\n",
    "        print(f'         ***Success*** Final match is {remove_duplicates(final_match)}')  \n",
    "        key_cyrillic_dict = f\"{jersey}:{country_name}\"\n",
    "        if key_cyrillic_dict in cyrillic_names_dict.keys():\n",
    "            print(f'{key_cyrillic_dict} already in dictionary')\n",
    "        else:\n",
    "            cyrillic_names_dict[key_cyrillic_dict] = remove_duplicates(final_match), name\n",
    "            print(f\"added {remove_duplicates(final_match)} and {name} to dict with key {key_cyrillic_dict}\")\n",
    "\n",
    "        if type(final_match) == str:\n",
    "            final_match = [final_match]\n",
    "        new_row = {\n",
    "            'DF index': index,\n",
    "            'Match ID': row['Match ID'],\n",
    "            'Competition': row['Competition'],\n",
    "            'Date': row['Date'],\n",
    "            'Match': row['Match'],\n",
    "            'Season': season,\n",
    "            'Nationality': country_name,\n",
    "            'Country Code': this_country_code,\n",
    "            'ORIGINAL JERSEY': row['ORIGINAL JERSEY'],\n",
    "            'English Name': name,\n",
    "            'Match(es) Found': remove_duplicates(final_match),\n",
    "            'Status': status\n",
    "        }\n",
    "        cyrillic_df = pd.concat([cyrillic_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print('*********FAIL*********')\n",
    "        print('Part 1 gave:', name)\n",
    "        print('Part 2 gave', player, player_real)\n",
    "        print('Part 3 gave', part_3_match)\n",
    "\n",
    "        new_row = {\n",
    "            'DF index': index,\n",
    "            'Match ID': row['Match ID'],\n",
    "            'Competition': row['Competition'],\n",
    "            'Date': row['Date'],\n",
    "            'Match': row['Match'],\n",
    "            'Season': season,\n",
    "            'Nationality': country_name,\n",
    "            'Country Code': this_country_code,\n",
    "            'ORIGINAL JERSEY': row['ORIGINAL JERSEY'],\n",
    "            'English Name': name,\n",
    "            'Match(es) Found': 0,\n",
    "            'Status': 'Fail'\n",
    "        }\n",
    "        cyrillic_df = pd.concat([cyrillic_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "        # final_list_to_check = player + part_3_match\n",
    "        # print(f'final list to check is {final_list_to_check}')\n",
    "\n",
    "    #collect output \n",
    "        #the index from the cyrillic_guys dataframe\n",
    "        #date\n",
    "        #country_name\n",
    "        #match ID \n",
    "        #match \n",
    "        #the ORIGINAL JERSEY \n",
    "            \n",
    "        #FINAL MATCH - WHAT IT RETURNED \n",
    "\n",
    "\n",
    "#Bugs\n",
    "        #354 \n",
    "\n",
    "#plan\n",
    "\n",
    "    #current - 300 - 400 \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyrillic_df.tail(50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_dataset_cyrillic = cyrillic_df\n",
    "big_dataset_cyrillic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_dataset_cyrillic.to_csv('cyrllic_found_2800_2844.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row['Match'], row['Date'] #closest_string('Crnigoj', dataset_nationality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Left Multiple people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_date_multi_guys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Left 0 People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_date_zero_guys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making big dataset to add stuff to as we finish them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lookup Return Case\n",
       "working                                                                                                                                        64700\n",
       "Null Name                                                                                                                                       8164\n",
       "Large Dataset had 0                                                                                                                             5628\n",
       "Lookup Required                                                                                                                                 3534\n",
       "Used Large Dataset                                                                                                                              2862\n",
       "not done yet                                                                                                                                    2844\n",
       "Online Lookup Required                                                                                                                          1814\n",
       "Not in either dataset for currect season but present in both seasons around match                                                               1706\n",
       "Was in  DB before or after season                                                                                                               1255\n",
       "Data Ended before season : 2023                                                                                                                  918\n",
       "Data Ended before season : 2021                                                                                                                  743\n",
       "Large Dataset said 0                                                                                                                             500\n",
       "Data Ended before season : 2017                                                                                                                  248\n",
       "Data Ended before season : 2019                                                                                                                  202\n",
       "Data Ended before season : 2022                                                                                                                  183\n",
       "Data Ended before season : 2015                                                                                                                  159\n",
       "Data Ended before season : 2016                                                                                                                  154\n",
       "Data Ended before season : 2018                                                                                                                   56\n",
       "asswipe was in DB in before or after season                                                                                                       44\n",
       "FILTERING USING MATCH DATE LEFT 0 RESULTS. BEFORE FILTERING, CANDIDATE NAMES WERE ['Antonee Robinson', 'Miles Robinson', 'Robbie Robinson']       22\n",
       "FILTERING USING MATCH DATE LEFT 0 RESULTS. BEFORE FILTERING, CANDIDATE NAMES WERE ['Sean Johnson', 'Fabian Johnson', 'Eddie Johnson']             22\n",
       "FILTERING USING MATCH DATE LEFT MULTIPLE RESULTS: [['Román Torres', 'Gaby Torres']]                                                               20\n",
       "FILTERING USING MATCH DATE LEFT MULTIPLE RESULTS: [['Henry Figueroa', 'Maynor Figueroa']]                                                         19\n",
       "FILTERING USING MATCH DATE LEFT 0 RESULTS. BEFORE FILTERING, CANDIDATE NAMES WERE ['Omar González', 'Jesse González']                             18\n",
       "FILTERING USING MATCH DATE LEFT MULTIPLE RESULTS: ['Álvaro Pereira', 'Maximiliano Pereira']                                                       17\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df_large['Lookup Return Case'].value_counts().head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Match ID</th>\n",
       "      <th>Competition</th>\n",
       "      <th>Match</th>\n",
       "      <th>Date</th>\n",
       "      <th>Nationality</th>\n",
       "      <th>Team Country Code</th>\n",
       "      <th>Season</th>\n",
       "      <th>Status</th>\n",
       "      <th>Name</th>\n",
       "      <th>Market Value</th>\n",
       "      <th>Lookup Still Required?</th>\n",
       "      <th>Lookup Return Case</th>\n",
       "      <th>Impute Required?</th>\n",
       "      <th>Name(s) Found</th>\n",
       "      <th>ORIGINAL JERSEY</th>\n",
       "      <th>Match Case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3133</td>\n",
       "      <td>World Cup Qualifiers, Europe, 2018</td>\n",
       "      <td>Luxembourg vs France</td>\n",
       "      <td>25-03-2017</td>\n",
       "      <td>Luxembourg</td>\n",
       "      <td>LU</td>\n",
       "      <td>2017</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>Anthony Moris</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>working</td>\n",
       "      <td>False</td>\n",
       "      <td>Anthony Moris</td>\n",
       "      <td>A Moris</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3133</td>\n",
       "      <td>World Cup Qualifiers, Europe, 2018</td>\n",
       "      <td>Luxembourg vs France</td>\n",
       "      <td>25-03-2017</td>\n",
       "      <td>Luxembourg</td>\n",
       "      <td>LU</td>\n",
       "      <td>2017</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>Laurent Jans</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>working</td>\n",
       "      <td>False</td>\n",
       "      <td>Laurent Jans</td>\n",
       "      <td>L Jans</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3133</td>\n",
       "      <td>World Cup Qualifiers, Europe, 2018</td>\n",
       "      <td>Luxembourg vs France</td>\n",
       "      <td>25-03-2017</td>\n",
       "      <td>Luxembourg</td>\n",
       "      <td>LU</td>\n",
       "      <td>2017</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>Kevin Malget</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>working</td>\n",
       "      <td>False</td>\n",
       "      <td>Kevin Malget</td>\n",
       "      <td>K Malget</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3133</td>\n",
       "      <td>World Cup Qualifiers, Europe, 2018</td>\n",
       "      <td>Luxembourg vs France</td>\n",
       "      <td>25-03-2017</td>\n",
       "      <td>Luxembourg</td>\n",
       "      <td>LU</td>\n",
       "      <td>2017</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>Mario Mutsch</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>working</td>\n",
       "      <td>False</td>\n",
       "      <td>Mario Mutsch</td>\n",
       "      <td>M Mutsch</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3133</td>\n",
       "      <td>World Cup Qualifiers, Europe, 2018</td>\n",
       "      <td>Luxembourg vs France</td>\n",
       "      <td>25-03-2017</td>\n",
       "      <td>Luxembourg</td>\n",
       "      <td>LU</td>\n",
       "      <td>2017</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>Chris Philipps</td>\n",
       "      <td>1250000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>working</td>\n",
       "      <td>False</td>\n",
       "      <td>Chris Philipps</td>\n",
       "      <td>C Philipps</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96861</th>\n",
       "      <td>96861</td>\n",
       "      <td>1095</td>\n",
       "      <td>1516</td>\n",
       "      <td>Nations League 2018-2020</td>\n",
       "      <td>Faroe Islands vs Azerbaijan</td>\n",
       "      <td>11.10.18</td>\n",
       "      <td>Azerbaijan</td>\n",
       "      <td>AZ</td>\n",
       "      <td>2018</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>Araz Abdullayev</td>\n",
       "      <td>500000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>working</td>\n",
       "      <td>False</td>\n",
       "      <td>Araz Abdullayev</td>\n",
       "      <td>A Abdullayev</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96862</th>\n",
       "      <td>96862</td>\n",
       "      <td>1096</td>\n",
       "      <td>1516</td>\n",
       "      <td>Nations League 2018-2020</td>\n",
       "      <td>Faroe Islands vs Azerbaijan</td>\n",
       "      <td>11.10.18</td>\n",
       "      <td>Azerbaijan</td>\n",
       "      <td>AZ</td>\n",
       "      <td>2018</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>Richard Almeyda</td>\n",
       "      <td>1600000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>working</td>\n",
       "      <td>False</td>\n",
       "      <td>Richard Almeyda</td>\n",
       "      <td>Richard Almeida</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96863</th>\n",
       "      <td>96863</td>\n",
       "      <td>1097</td>\n",
       "      <td>1516</td>\n",
       "      <td>Nations League 2018-2020</td>\n",
       "      <td>Faroe Islands vs Azerbaijan</td>\n",
       "      <td>11.10.18</td>\n",
       "      <td>Azerbaijan</td>\n",
       "      <td>AZ</td>\n",
       "      <td>2018</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>Mahir Emreli</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>working</td>\n",
       "      <td>False</td>\n",
       "      <td>Mahir Emreli</td>\n",
       "      <td>Emreli</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96864</th>\n",
       "      <td>96864</td>\n",
       "      <td>1098</td>\n",
       "      <td>1516</td>\n",
       "      <td>Nations League 2018-2020</td>\n",
       "      <td>Faroe Islands vs Azerbaijan</td>\n",
       "      <td>11.10.18</td>\n",
       "      <td>Azerbaijan</td>\n",
       "      <td>AZ</td>\n",
       "      <td>2018</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>Dimitrij Nazarov</td>\n",
       "      <td>500000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>working</td>\n",
       "      <td>False</td>\n",
       "      <td>Dimitrij Nazarov</td>\n",
       "      <td>D Nazarov</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96865</th>\n",
       "      <td>96865</td>\n",
       "      <td>1099</td>\n",
       "      <td>1516</td>\n",
       "      <td>Nations League 2018-2020</td>\n",
       "      <td>Faroe Islands vs Azerbaijan</td>\n",
       "      <td>11.10.18</td>\n",
       "      <td>Azerbaijan</td>\n",
       "      <td>AZ</td>\n",
       "      <td>2018</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>Renat Dadashov</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>working</td>\n",
       "      <td>False</td>\n",
       "      <td>Renat Dadashov</td>\n",
       "      <td>Re Dadashov</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62313 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0.1  Unnamed: 0  Match ID                         Competition  \\\n",
       "0                 0           0      3133  World Cup Qualifiers, Europe, 2018   \n",
       "1                 1           1      3133  World Cup Qualifiers, Europe, 2018   \n",
       "4                 4           4      3133  World Cup Qualifiers, Europe, 2018   \n",
       "5                 5           5      3133  World Cup Qualifiers, Europe, 2018   \n",
       "6                 6           6      3133  World Cup Qualifiers, Europe, 2018   \n",
       "...             ...         ...       ...                                 ...   \n",
       "96861         96861        1095      1516            Nations League 2018-2020   \n",
       "96862         96862        1096      1516            Nations League 2018-2020   \n",
       "96863         96863        1097      1516            Nations League 2018-2020   \n",
       "96864         96864        1098      1516            Nations League 2018-2020   \n",
       "96865         96865        1099      1516            Nations League 2018-2020   \n",
       "\n",
       "                             Match        Date Nationality Team Country Code  \\\n",
       "0             Luxembourg vs France  25-03-2017  Luxembourg                LU   \n",
       "1             Luxembourg vs France  25-03-2017  Luxembourg                LU   \n",
       "4             Luxembourg vs France  25-03-2017  Luxembourg                LU   \n",
       "5             Luxembourg vs France  25-03-2017  Luxembourg                LU   \n",
       "6             Luxembourg vs France  25-03-2017  Luxembourg                LU   \n",
       "...                            ...         ...         ...               ...   \n",
       "96861  Faroe Islands vs Azerbaijan    11.10.18  Azerbaijan                AZ   \n",
       "96862  Faroe Islands vs Azerbaijan    11.10.18  Azerbaijan                AZ   \n",
       "96863  Faroe Islands vs Azerbaijan    11.10.18  Azerbaijan                AZ   \n",
       "96864  Faroe Islands vs Azerbaijan    11.10.18  Azerbaijan                AZ   \n",
       "96865  Faroe Islands vs Azerbaijan    11.10.18  Azerbaijan                AZ   \n",
       "\n",
       "       Season   Status              Name Market Value  Lookup Still Required?  \\\n",
       "0        2017  SUCCESS     Anthony Moris     400000.0                   False   \n",
       "1        2017  SUCCESS      Laurent Jans    1200000.0                   False   \n",
       "4        2017  SUCCESS      Kevin Malget     150000.0                   False   \n",
       "5        2017  SUCCESS      Mario Mutsch     125000.0                   False   \n",
       "6        2017  SUCCESS    Chris Philipps    1250000.0                   False   \n",
       "...       ...      ...               ...          ...                     ...   \n",
       "96861    2018  SUCCESS   Araz Abdullayev     500000.0                   False   \n",
       "96862    2018  SUCCESS   Richard Almeyda    1600000.0                   False   \n",
       "96863    2018  SUCCESS      Mahir Emreli    1200000.0                   False   \n",
       "96864    2018  SUCCESS  Dimitrij Nazarov     500000.0                   False   \n",
       "96865    2018  SUCCESS    Renat Dadashov     225000.0                   False   \n",
       "\n",
       "      Lookup Return Case  Impute Required?     Name(s) Found  ORIGINAL JERSEY  \\\n",
       "0                working             False     Anthony Moris          A Moris   \n",
       "1                working             False      Laurent Jans           L Jans   \n",
       "4                working             False      Kevin Malget         K Malget   \n",
       "5                working             False      Mario Mutsch         M Mutsch   \n",
       "6                working             False    Chris Philipps       C Philipps   \n",
       "...                  ...               ...               ...              ...   \n",
       "96861            working             False   Araz Abdullayev     A Abdullayev   \n",
       "96862            working             False   Richard Almeyda  Richard Almeida   \n",
       "96863            working             False      Mahir Emreli           Emreli   \n",
       "96864            working             False  Dimitrij Nazarov        D Nazarov   \n",
       "96865            working             False    Renat Dadashov      Re Dadashov   \n",
       "\n",
       "      Match Case  \n",
       "0         single  \n",
       "1         single  \n",
       "4         single  \n",
       "5         single  \n",
       "6         single  \n",
       "...          ...  \n",
       "96861     single  \n",
       "96862     single  \n",
       "96863     single  \n",
       "96864     single  \n",
       "96865     single  \n",
       "\n",
       "[62313 rows x 18 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df_edited = combined_df_large[combined_df_large['Lookup Return Case'] != 'Null Name']\n",
    "combined_df_edited = combined_df_edited[combined_df_edited['Lookup Return Case'] != 'Large Dataset had 0']\n",
    "combined_df_edited = combined_df_edited[combined_df_edited['Lookup Return Case'] != 'Lookup Required']\n",
    "combined_df_edited = combined_df_edited[combined_df_edited['Lookup Return Case'] != 'not done yet']\n",
    "combined_df_edited = combined_df_edited[combined_df_edited['Lookup Return Case'] != 'Online Lookup Required']\n",
    "combined_df_edited = combined_df_edited[combined_df_edited['Lookup Return Case'] != 'Large Dataset said 0']\n",
    "combined_df_edited = combined_df_edited[~combined_df_edited['Lookup Return Case'].str.startswith(\"Data Ended before season :\")]\n",
    "combined_df_edited = combined_df_edited[~combined_df_edited['Lookup Return Case'].str.startswith(\"Data Ended before season :\")]\n",
    "combined_df_edited = combined_df_edited[~combined_df_edited['Lookup Return Case'].str.startswith(\"Not in either dataset for currect season but present in both seasons\")]\n",
    "combined_df_edited = combined_df_edited[~combined_df_edited['Lookup Return Case'].str.startswith(\"FILTERING USING MATCH DATE LEFT MULTIPLE RESULTS:\")]\n",
    "combined_df_edited = combined_df_edited[~combined_df_edited['Lookup Return Case'].str.startswith(\"FILTERING USING MATCH DATE LEFT 0 RESULTS.\")]\n",
    "combined_df_edited = combined_df_edited[~combined_df_edited['Lookup Return Case'].str.startswith(\"Was in  DB before or after season\")]\n",
    "combined_df_edited = combined_df_edited[~combined_df_edited['Lookup Return Case'].str.startswith(\"asswipe was in DB in before\")]\n",
    "\n",
    "combined_df_edited = combined_df_edited[~((combined_df_edited['Status'] == 'SUCCESS') & \n",
    "                                   (combined_df_edited['Lookup Return Case'] == 'Used Large Dataset') & \n",
    "                                   (combined_df_edited['Name(s) Found'].astype(str).str.startswith('[')))]#['Lookup Return Case'].value_counts()\n",
    "combined_df_edited = combined_df_edited[~((combined_df_edited['Status'] == 'SUCCESS') & \n",
    "                                   (combined_df_edited['Lookup Return Case'] == 'working') & \n",
    "                                   (combined_df_edited['Name(s) Found'].astype(str).str.startswith('[')))]\n",
    "\n",
    "combined_df_edited = combined_df_edited[combined_df_edited['Name(s) Found'] != '0']\n",
    "combined_df_edited = combined_df_edited[~combined_df_edited['Lookup Return Case'].str.startswith('Olympics row')]\n",
    "combined_df_edited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_edited[combined_df_edited['Lookup Return Case'].str.startswith('Olympics row')].to_csv('olympics_edge_cases.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_edited.to_csv('Combined_DF_WORKING - ADD TO THIS.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
